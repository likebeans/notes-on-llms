---
title: Embedding æŠ€æœ¯è¯¦è§£
description: ä»åŸç†åˆ°å®è·µï¼ŒæŒæ¡æ–‡æœ¬å‘é‡åŒ–çš„æ ¸å¿ƒæŠ€æœ¯
---

# Embedding æŠ€æœ¯è¯¦è§£

> æ·±å…¥ç†è§£æ–‡æœ¬å‘é‡åŒ–çš„åŸç†ä¸å®è·µï¼Œé€‰æ‹©åˆé€‚çš„ Embedding æ¨¡å‹

## ğŸ¯ æ ¸å¿ƒæ¦‚å¿µ

### ä»€ä¹ˆæ˜¯ Embeddingï¼Ÿ

**Embeddingï¼ˆåµŒå…¥ï¼‰** æ˜¯å°†ç¦»æ•£çš„æ–‡æœ¬ç¬¦å·æ˜ å°„ä¸ºè¿ç»­çš„é«˜ç»´å‘é‡è¡¨ç¤ºçš„æŠ€æœ¯ï¼Œæ˜¯ç°ä»£ NLP å’Œ RAG ç³»ç»Ÿçš„åŸºç¡€ã€‚

```python
# æ–‡æœ¬ â†’ å‘é‡çš„æ˜ å°„è¿‡ç¨‹
"ä»Šå¤©å¤©æ°”çœŸå¥½" â†’ [0.12, -0.34, 0.56, ..., 0.78]  # 1536ç»´å‘é‡
"å¤©æ°”ä¸é”™"     â†’ [0.11, -0.32, 0.58, ..., 0.76]  # è¯­ä¹‰ç›¸ä¼¼ï¼Œå‘é‡æ¥è¿‘
"æˆ‘å–œæ¬¢ç¼–ç¨‹"   â†’ [-0.45, 0.67, -0.23, ..., 0.12]  # è¯­ä¹‰ä¸åŒï¼Œå‘é‡è·ç¦»è¿œ
```

### ä¸ºä»€ä¹ˆéœ€è¦ Embeddingï¼Ÿ

::: tip æ ¸å¿ƒä»·å€¼
**è¯­ä¹‰ç†è§£**ï¼šå°†äººç±»è¯­è¨€è½¬æ¢ä¸ºæœºå™¨å¯ç†è§£çš„æ•°å­¦è¡¨ç¤º  
**ç›¸ä¼¼åº¦è®¡ç®—**ï¼šé€šè¿‡å‘é‡è·ç¦»è¡¡é‡æ–‡æœ¬è¯­ä¹‰ç›¸ä¼¼æ€§  
**é«˜æ•ˆæ£€ç´¢**ï¼šåœ¨é«˜ç»´å‘é‡ç©ºé—´ä¸­è¿›è¡Œå¿«é€Ÿç›¸ä¼¼æ€§æœç´¢
:::

---

## ğŸ§® Embedding çš„ç†è®ºåŸºç¡€

> åŸºäº[ã€Šä»æ„ä¹‰åˆ°æœºåˆ¶ï¼šæ·±å…¥å‰–æEmbeddingæ¨¡å‹åŸç†åŠå…¶åœ¨RAGä¸­çš„ä½œç”¨ã€‹](https://dd-ff.blog.csdn.net/article/details/152809855)

### åˆ†å¸ƒå¼å‡è¯´ï¼ˆDistributional Hypothesisï¼‰

**æ ¸å¿ƒæ€æƒ³**ï¼š*"You shall know a word by the company it keeps"*ï¼ˆè§‚å…¶ä¼´è€ŒçŸ¥å…¶ä¹‰ï¼‰

- **åŸºæœ¬ä¸»å¼ **ï¼šå‡ºç°åœ¨ç›¸ä¼¼ä¸Šä¸‹æ–‡çš„å•è¯ï¼Œè¯­ä¹‰æ›´ç›¸è¿‘
- **å®é™…åº”ç”¨**ï¼šé€šè¿‡åˆ†æè¯æ±‡çš„å…±ç°æ¨¡å¼æ¥å­¦ä¹ è¯­ä¹‰è¡¨ç¤º

**ç¤ºä¾‹**ï¼š
```
ä¸Šä¸‹æ–‡1: "å›½ç‹ç»Ÿæ²»ç€ç‹å›½"
ä¸Šä¸‹æ–‡2: "å¥³ç‹ç»Ÿæ²»ç€ç‹å›½" 
â†’ "å›½ç‹"å’Œ"å¥³ç‹"åœ¨ç›¸ä¼¼ä¸Šä¸‹æ–‡ä¸­å‡ºç° â†’ è¯­ä¹‰ç›¸å…³
```

### å‘é‡ç©ºé—´æ¨¡å‹

å°†æ–‡æœ¬æ˜ å°„åˆ°é«˜ç»´å‘é‡ç©ºé—´ï¼Œè¯­ä¹‰å…³ç³»è½¬åŒ–ä¸ºå‡ ä½•è·ç¦»ï¼š

- **ä½™å¼¦ç›¸ä¼¼åº¦**ï¼šè¡¡é‡å‘é‡å¤¹è§’ï¼Œå€¼è¶Šæ¥è¿‘1è¶Šç›¸ä¼¼
- **æ¬§å‡ é‡Œå¾—è·ç¦»**ï¼šè¡¡é‡å‘é‡é—´çš„ç›´çº¿è·ç¦»
- **å‘é‡è¿ç®—**ï¼šæ”¯æŒè¯­ä¹‰æ¨ç†ï¼ˆå¦‚"å›½ç‹-ç”·äºº+å¥³äººâ‰ˆå¥³ç‹"ï¼‰

---

## ğŸ“ˆ Embedding æŠ€æœ¯æ¼”è¿›

### ç¬¬ä¸€ä»£ï¼šé™æ€è¯å‘é‡

#### Word2Vecï¼ˆ2013å¹´ï¼‰
- **CBOW**ï¼šé€šè¿‡ä¸Šä¸‹æ–‡é¢„æµ‹ä¸­å¿ƒè¯
- **Skip-gram**ï¼šé€šè¿‡ä¸­å¿ƒè¯é¢„æµ‹ä¸Šä¸‹æ–‡
- **ç‰¹ç‚¹**ï¼šæ¯ä¸ªè¯å¯¹åº”å›ºå®šå‘é‡ï¼Œæ— æ³•å¤„ç†ä¸€è¯å¤šä¹‰

#### GloVeï¼ˆ2014å¹´ï¼‰
- **åŸç†**ï¼šç»“åˆå…¨å±€ç»Ÿè®¡ä¿¡æ¯å’Œå±€éƒ¨ä¸Šä¸‹æ–‡
- **ä¼˜åŠ¿**ï¼šå¹³è¡¡Word2Vecçš„å±€éƒ¨æ€§å’ŒçŸ©é˜µåˆ†è§£çš„å…¨å±€æ€§

### ç¬¬äºŒä»£ï¼šåŠ¨æ€ä¸Šä¸‹æ–‡å‘é‡

#### BERTï¼ˆ2018å¹´ï¼‰
- **çªç ´**ï¼šåŒä¸€ä¸ªè¯åœ¨ä¸åŒä¸Šä¸‹æ–‡ä¸­æœ‰ä¸åŒå‘é‡è¡¨ç¤º
- **æ¶æ„**ï¼šåŸºäºTransformerçš„åŒå‘ç¼–ç å™¨
- **èƒ½åŠ›**ï¼šè§£å†³ä¸€è¯å¤šä¹‰é—®é¢˜

```python
# BERTçš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥èƒ½åŠ›ç¤ºä¾‹
"é“¶è¡Œå¡åœ¨é“¶è¡ŒåŠç†" 
# "é“¶è¡Œ"(é‡‘èæœºæ„) å’Œ "é“¶è¡Œ"(å¡ç‰‡) æœ‰ä¸åŒçš„å‘é‡è¡¨ç¤º
```

### ç¬¬ä¸‰ä»£ï¼šä¸“ç”¨ Embedding æ¨¡å‹

é’ˆå¯¹æ£€ç´¢ä»»åŠ¡ä¼˜åŒ–çš„ä¸“é—¨æ¨¡å‹ï¼š
- **sentence-transformers**ï¼šä¸“é—¨ç”¨äºå¥å­çº§å‘é‡åŒ–
- **BGEç³»åˆ—**ï¼šä¸­æ–‡ä¼˜åŒ–çš„åŒç¼–ç å™¨æ¨¡å‹
- **OpenAI text-embedding-3**ï¼šå¤šè¯­è¨€ã€é«˜ç»´åº¦

---

## ğŸ”§ ä¸»æµ Embedding æ¨¡å‹å¯¹æ¯”

### å•†ä¸šæ¨¡å‹

| æ¨¡å‹ | ç»´åº¦ | æœ€å¤§Token | ä¸­æ–‡æ”¯æŒ | æˆæœ¬ | ç‰¹ç‚¹ |
|------|------|-----------|----------|------|------|
| **text-embedding-3-large** | 3072 | 8191 | âœ… | é«˜ | ç²¾åº¦æœ€é«˜ï¼Œé€‚åˆé«˜è´¨é‡åœºæ™¯ |
| **text-embedding-3-small** | 1536 | 8191 | âœ… | ä½ | æ€§ä»·æ¯”ä¼˜ç§€ï¼Œé€šç”¨æ¨è |
| **text-embedding-ada-002** | 1536 | 8191 | âœ… | ä¸­ | æˆç†Ÿç¨³å®šï¼Œå¹¿æ³›ä½¿ç”¨ |

### å¼€æºæ¨¡å‹

| æ¨¡å‹ | ç»´åº¦ | ä¼˜åŠ¿ | é€‚ç”¨åœºæ™¯ |
|------|------|------|----------|
| **bge-large-zh-v1.5** | 1024 | ä¸­æ–‡ä¼˜åŒ–ã€å¼€æºå…è´¹ | ä¸­æ–‡RAGç³»ç»Ÿ |
| **bge-m3** | 1024 | å¤šè¯­è¨€ã€ç¨ å¯†+ç¨€ç– | è·¨è¯­è¨€æ£€ç´¢ |
| **m3e-base** | 768 | è½»é‡ã€å¿«é€Ÿ | èµ„æºå—é™ç¯å¢ƒ |
| **text2vec-large-chinese** | 1024 | ä¸­æ–‡ç‰¹åŒ– | ä¸­æ–‡è¯­ä¹‰æœç´¢ |

### é€‰æ‹©å»ºè®®

::: info æ¨¡å‹é€‰æ‹©æŒ‡å—
**è¿½æ±‚ç²¾åº¦**ï¼šOpenAI text-embedding-3-large  
**å¹³è¡¡æ€§ä»·æ¯”**ï¼šOpenAI text-embedding-3-small  
**çº¯ä¸­æ–‡åœºæ™¯**ï¼šbge-large-zh-v1.5  
**èµ„æºå—é™**ï¼šm3e-base  
**å¤šè¯­è¨€éœ€æ±‚**ï¼šbge-m3
:::

---

## ğŸ’» å®æˆ˜ä»£ç ç¤ºä¾‹

### OpenAI Embedding

```python
from openai import OpenAI
import numpy as np

client = OpenAI(api_key="your-api-key")

def get_embedding(text, model="text-embedding-3-small"):
    """è·å–æ–‡æœ¬çš„embeddingå‘é‡"""
    response = client.embeddings.create(
        model=model,
        input=text
    )
    return response.data[0].embedding

# ä½¿ç”¨ç¤ºä¾‹
text1 = "ä»€ä¹ˆæ˜¯æ£€ç´¢å¢å¼ºç”Ÿæˆï¼Ÿ"
text2 = "RAGæŠ€æœ¯çš„å·¥ä½œåŸç†"

embedding1 = get_embedding(text1)
embedding2 = get_embedding(text2)

# è®¡ç®—ç›¸ä¼¼åº¦
similarity = np.dot(embedding1, embedding2)
print(f"è¯­ä¹‰ç›¸ä¼¼åº¦: {similarity:.4f}")
```

### å¼€æºæ¨¡å‹ä½¿ç”¨

```python
from sentence_transformers import SentenceTransformer

# åŠ è½½BGEä¸­æ–‡æ¨¡å‹
model = SentenceTransformer('BAAI/bge-large-zh-v1.5')

# æ‰¹é‡ç¼–ç 
texts = [
    "æ£€ç´¢å¢å¼ºç”ŸæˆæŠ€æœ¯åŸç†",
    "RAGç³»ç»Ÿæ¶æ„è®¾è®¡",
    "å‘é‡æ•°æ®åº“é€‰å‹"
]

embeddings = model.encode(texts)
print(f"å‘é‡ç»´åº¦: {embeddings.shape}")

# è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
from sklearn.metrics.pairwise import cosine_similarity
sim_matrix = cosine_similarity(embeddings)
print("ç›¸ä¼¼åº¦çŸ©é˜µ:", sim_matrix)
```

### æ‰¹é‡å¤„ç†ä¼˜åŒ–

```python
import numpy as np
from typing import List
import time

class EmbeddingProcessor:
    def __init__(self, model_name="text-embedding-3-small"):
        self.model_name = model_name
        self.client = OpenAI()
    
    def batch_embed(self, texts: List[str], batch_size: int = 100):
        """æ‰¹é‡å¤„ç†embeddingï¼Œæé«˜æ•ˆç‡"""
        embeddings = []
        
        for i in range(0, len(texts), batch_size):
            batch = texts[i:i + batch_size]
            
            try:
                response = self.client.embeddings.create(
                    model=self.model_name,
                    input=batch
                )
                
                batch_embeddings = [item.embedding for item in response.data]
                embeddings.extend(batch_embeddings)
                
                # é¿å…APIé™æµ
                time.sleep(0.1)
                
            except Exception as e:
                print(f"æ‰¹æ¬¡ {i//batch_size + 1} å¤„ç†å¤±è´¥: {e}")
                continue
        
        return embeddings

# ä½¿ç”¨ç¤ºä¾‹
processor = EmbeddingProcessor()
large_text_list = ["æ–‡æœ¬1", "æ–‡æœ¬2", ...]  # å‡è®¾æœ‰å¾ˆå¤šæ–‡æœ¬
embeddings = processor.batch_embed(large_text_list)
```

---

## ğŸ¯ RAG ä¸­çš„ Embedding åº”ç”¨

### æ–‡æ¡£ç´¢å¼•æµç¨‹

```python
def build_document_index(documents: List[str]):
    """æ„å»ºæ–‡æ¡£å‘é‡ç´¢å¼•"""
    embeddings = []
    
    for doc in documents:
        # 1. æ–‡æ¡£åˆ‡åˆ†ï¼ˆè§chunkingç« èŠ‚ï¼‰
        chunks = chunk_document(doc)
        
        # 2. å‘é‡åŒ–
        doc_embeddings = []
        for chunk in chunks:
            embedding = get_embedding(chunk)
            doc_embeddings.append({
                'text': chunk,
                'vector': embedding,
                'metadata': {'source': doc, 'chunk_id': len(embeddings)}
            })
        
        embeddings.extend(doc_embeddings)
    
    return embeddings
```

### æ£€ç´¢åŒ¹é…

```python
def semantic_search(query: str, index: List[dict], top_k: int = 5):
    """è¯­ä¹‰æœç´¢"""
    query_embedding = get_embedding(query)
    
    # è®¡ç®—ç›¸ä¼¼åº¦
    similarities = []
    for item in index:
        similarity = cosine_similarity(query_embedding, item['vector'])
        similarities.append((similarity, item))
    
    # æ’åºè¿”å›top-k
    similarities.sort(key=lambda x: x[0], reverse=True)
    return [item[1] for item in similarities[:top_k]]
```

---

## âš ï¸ å®è·µä¸­çš„å¸¸è§é—®é¢˜

### é—®é¢˜1ï¼šæ¨¡å‹ä¸åŒ¹é…

**ç°è±¡**ï¼šæ£€ç´¢æ•ˆæœå·®ï¼Œç›¸ä¼¼æ–‡æœ¬åŒ¹é…åº¦ä½  
**åŸå› **ï¼šç´¢å¼•å’ŒæŸ¥è¯¢ä½¿ç”¨äº†ä¸åŒçš„embeddingæ¨¡å‹  
**è§£å†³**ï¼š
```python
# âŒ é”™è¯¯åšæ³•
index_embeddings = get_embedding(texts, model="text-embedding-ada-002")
query_embedding = get_embedding(query, model="text-embedding-3-small")

# âœ… æ­£ç¡®åšæ³•  
MODEL_NAME = "text-embedding-3-small"
index_embeddings = get_embedding(texts, model=MODEL_NAME)
query_embedding = get_embedding(query, model=MODEL_NAME)
```

### é—®é¢˜2ï¼šæ–‡æœ¬é•¿åº¦è¶…é™

**ç°è±¡**ï¼šé•¿æ–‡æœ¬è¢«æˆªæ–­ï¼Œä¿¡æ¯ä¸¢å¤±  
**è§£å†³æ–¹æ¡ˆ**ï¼š
```python
def safe_embedding(text: str, model: str, max_tokens: int = 8191):
    """å®‰å…¨çš„embeddingå¤„ç†ï¼Œé¿å…è¶…é•¿æˆªæ–­"""
    import tiktoken
    
    encoding = tiktoken.encoding_for_model(model)
    tokens = encoding.encode(text)
    
    if len(tokens) <= max_tokens:
        return get_embedding(text, model)
    else:
        # æˆªæ–­å¤„ç†
        truncated_tokens = tokens[:max_tokens]
        truncated_text = encoding.decode(truncated_tokens)
        return get_embedding(truncated_text, model)
```

### é—®é¢˜3ï¼šä¸­è‹±æ–‡æ··åˆå¤„ç†

**ç°è±¡**ï¼šä¸­è‹±æ–‡æ··åˆæ–‡æœ¬æ•ˆæœä¸ä½³  
**è§£å†³**ï¼šé€‰æ‹©å¤šè¯­è¨€æ¨¡å‹æˆ–åˆ†åˆ«å¤„ç†
```python
def multilingual_embedding(text: str):
    """å¤šè¯­è¨€æ–‡æœ¬å¤„ç†"""
    # æ£€æµ‹è¯­è¨€ç±»å‹
    if contains_chinese(text):
        if contains_english(text):
            # ä¸­è‹±æ··åˆï¼šä½¿ç”¨å¤šè¯­è¨€æ¨¡å‹
            return get_embedding(text, model="text-embedding-3-large")
        else:
            # çº¯ä¸­æ–‡ï¼šä½¿ç”¨ä¸­æ–‡ä¼˜åŒ–æ¨¡å‹
            return bge_model.encode(text)
    else:
        # çº¯è‹±æ–‡ï¼šä½¿ç”¨é€šç”¨æ¨¡å‹
        return get_embedding(text, model="text-embedding-3-small")
```

---

## ğŸ“Š æ€§èƒ½ä¼˜åŒ–å»ºè®®

### 1. ç¼“å­˜æœºåˆ¶

```python
import hashlib
import json
from functools import lru_cache

@lru_cache(maxsize=10000)
def cached_embedding(text: str, model: str):
    """å¸¦ç¼“å­˜çš„embeddingè®¡ç®—"""
    return get_embedding(text, model)

# æˆ–ä½¿ç”¨Redisç¼“å­˜
def redis_cached_embedding(text: str, model: str):
    import redis
    r = redis.Redis(host='localhost', port=6379, db=0)
    
    # ç”Ÿæˆç¼“å­˜key
    cache_key = f"emb:{model}:{hashlib.md5(text.encode()).hexdigest()}"
    
    # å°è¯•ä»ç¼“å­˜è·å–
    cached = r.get(cache_key)
    if cached:
        return json.loads(cached)
    
    # è®¡ç®—å¹¶ç¼“å­˜
    embedding = get_embedding(text, model)
    r.setex(cache_key, 86400, json.dumps(embedding))  # 24å°æ—¶è¿‡æœŸ
    return embedding
```

### 2. å¼‚æ­¥å¤„ç†

```python
import asyncio
import aiohttp

async def async_get_embedding(text: str, session: aiohttp.ClientSession):
    """å¼‚æ­¥embeddingè®¡ç®—"""
    # å®ç°å¼‚æ­¥APIè°ƒç”¨
    pass

async def batch_async_embedding(texts: List[str]):
    """å¼‚æ­¥æ‰¹é‡å¤„ç†"""
    async with aiohttp.ClientSession() as session:
        tasks = [async_get_embedding(text, session) for text in texts]
        return await asyncio.gather(*tasks)
```

---

## ğŸ”— ç›¸å…³é˜…è¯»

- [RAGèŒƒå¼æ¼”è¿›](/rag/paradigms) - äº†è§£RAGæŠ€æœ¯å‘å±•è„‰ç»œ
- [æ–‡æ¡£åˆ‡åˆ†ç­–ç•¥](/rag/chunking) - Embeddingå‰çš„æ–‡æœ¬é¢„å¤„ç†
- [å‘é‡æ•°æ®åº“é€‰å‹](/rag/vector-db) - Embeddingå­˜å‚¨ä¸æ£€ç´¢
- [æ£€ç´¢ç­–ç•¥ä¼˜åŒ–](/rag/retrieval) - åŸºäºå‘é‡çš„æ£€ç´¢æŠ€å·§

> **ç›¸å…³æ–‡ç« **ï¼š
> - [ä»æ„ä¹‰åˆ°æœºåˆ¶ï¼šæ·±å…¥å‰–æEmbeddingæ¨¡å‹åŸç†åŠå…¶åœ¨RAGä¸­çš„ä½œç”¨](https://dd-ff.blog.csdn.net/article/details/152809855)
> - [ä»æ½œåœ¨ç©ºé—´åˆ°å®é™…åº”ç”¨ï¼šEmbeddingæ¨¡å‹æ¶æ„ä¸è®­ç»ƒèŒƒå¼çš„ç»¼åˆè§£æ](https://dd-ff.blog.csdn.net/article/details/152815637)
> - [ä»æ–‡æœ¬åˆ°ä¸Šä¸‹æ–‡ï¼šæ·±å…¥è§£æTokenizerã€EmbeddingåŠé«˜çº§RAGæ¶æ„çš„åº•å±‚åŸç†](https://dd-ff.blog.csdn.net/article/details/152819135)

> **å¤–éƒ¨èµ„æº**ï¼š
> - [MTEBæ’è¡Œæ¦œ](https://huggingface.co/spaces/mteb/leaderboard) - Embeddingæ¨¡å‹æ€§èƒ½å¯¹æ¯”
> - [Sentence-Transformersæ–‡æ¡£](https://www.sbert.net/) - å¼€æºEmbeddingæ¡†æ¶
> - [OpenAI EmbeddingsæŒ‡å—](https://platform.openai.com/docs/guides/embeddings)

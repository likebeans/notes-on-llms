---
title: RLHF äººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ 
description: Reinforcement Learning from Human Feedback - è®©AIå­¦ä¼šäººç±»ä»·å€¼è§‚
---

# RLHF äººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ 

> ä»"èƒ½å›ç­”"åˆ°"ä¼šå›ç­”"çš„ä»·å€¼å¯¹é½

## ğŸ¯ æ ¸å¿ƒæ¦‚å¿µ

> æ¥æºï¼š[è¯­è¨€æ¨¡å‹å¯¹é½æŠ€æœ¯è®ºè¿°ï¼šä»åŸºäºPPOçš„RLHFåˆ°ç›´æ¥åå¥½ä¼˜åŒ–](https://dd-ff.blog.csdn.net/article/details/153269912)

### ä»€ä¹ˆæ˜¯RLHFï¼Ÿ

::: tip å®šä¹‰
**RLHFï¼ˆReinforcement Learning from Human Feedbackï¼‰** æ˜¯ä¸€ç§é€šè¿‡äººç±»åå¥½æ•°æ®è®­ç»ƒå¥–åŠ±æ¨¡å‹ï¼Œå†ç”¨å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–è¯­è¨€æ¨¡å‹çš„æŠ€æœ¯ï¼Œä½¿æ¨¡å‹è¾“å‡ºæ›´ç¬¦åˆäººç±»ä»·å€¼è§‚å’ŒæœŸæœ›ã€‚
:::

### ä¸ºä»€ä¹ˆéœ€è¦RLHFï¼Ÿ

| é˜¶æ®µ | é—®é¢˜ | RLHFè§£å†³æ–¹æ¡ˆ |
|------|------|-------------|
| **SFTå** | ä¼šå›ç­”ï¼Œä½†ä¸çŸ¥é“å“ªä¸ªæ›´å¥½ | å­¦ä¹ äººç±»åå¥½ |
| **å¤šä¸ªæ­£ç¡®ç­”æ¡ˆ** | æ— æ³•åŒºåˆ†ä¼˜åŠ£ | å¥–åŠ±æ¨¡å‹æ‰“åˆ† |
| **å®‰å…¨å¯¹é½** | å¯èƒ½ç”Ÿæˆæœ‰å®³å†…å®¹ | æƒ©ç½šä¸å½“è¾“å‡º |

---

## ğŸ”„ RLHFä¸‰é˜¶æ®µæµç¨‹

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         RLHF ä¸‰é˜¶æ®µæµç¨‹                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚    é˜¶æ®µä¸€        â”‚    é˜¶æ®µäºŒ        â”‚    é˜¶æ®µä¸‰                        â”‚
â”‚  ç›‘ç£å¾®è°ƒ(SFT)   â”‚  å¥–åŠ±æ¨¡å‹(RM)    â”‚  å¼ºåŒ–å­¦ä¹ (PPO)                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                 â”‚                 â”‚                                 â”‚
â”‚  æŒ‡ä»¤æ•°æ®é›†      â”‚   åå¥½æ•°æ®é›†     â”‚   SFTæ¨¡å‹ + RM                  â”‚
â”‚      â†“          â”‚       â†“         â”‚       â†“                         â”‚
â”‚  ç›‘ç£å­¦ä¹        â”‚   å¯¹æ¯”å­¦ä¹         â”‚   ç­–ç•¥ä¼˜åŒ–                       â”‚
â”‚      â†“          â”‚       â†“         â”‚       â†“                         â”‚
â”‚  SFTæ¨¡å‹        â”‚   å¥–åŠ±æ¨¡å‹        â”‚   å¯¹é½æ¨¡å‹                       â”‚
â”‚                 â”‚                 â”‚                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### é˜¶æ®µä¸€ï¼šç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰

```python
# ä½¿ç”¨é«˜è´¨é‡æŒ‡ä»¤æ•°æ®è¿›è¡Œç›‘ç£å¾®è°ƒ
# è¯¦è§ /training/sft é¡µé¢
```

### é˜¶æ®µäºŒï¼šå¥–åŠ±æ¨¡å‹è®­ç»ƒ

```python
from transformers import AutoModelForSequenceClassification
from trl import RewardTrainer, RewardConfig

# 1. å‡†å¤‡åå¥½æ•°æ®
# æ ¼å¼: {"prompt": "...", "chosen": "å¥½å›ç­”", "rejected": "å·®å›ç­”"}

# 2. åŠ è½½å¥–åŠ±æ¨¡å‹
reward_model = AutoModelForSequenceClassification.from_pretrained(
    "meta-llama/Llama-2-7b-hf",
    num_labels=1  # è¾“å‡ºå•ä¸€åˆ†æ•°
)

# 3. é…ç½®è®­ç»ƒ
reward_config = RewardConfig(
    output_dir="./reward_model",
    per_device_train_batch_size=4,
    num_train_epochs=1,
    learning_rate=1e-5,
)

# 4. è®­ç»ƒå¥–åŠ±æ¨¡å‹
trainer = RewardTrainer(
    model=reward_model,
    args=reward_config,
    train_dataset=preference_dataset,
    tokenizer=tokenizer,
)
trainer.train()
```

### é˜¶æ®µä¸‰ï¼šPPOå¼ºåŒ–å­¦ä¹ 

```python
from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead

# 1. åŠ è½½æ¨¡å‹
model = AutoModelForCausalLMWithValueHead.from_pretrained("sft_model")
ref_model = AutoModelForCausalLMWithValueHead.from_pretrained("sft_model")

# 2. PPOé…ç½®
ppo_config = PPOConfig(
    learning_rate=1e-5,
    batch_size=16,
    mini_batch_size=4,
    gradient_accumulation_steps=4,
    ppo_epochs=4,
    kl_penalty="kl",           # KLæ•£åº¦æƒ©ç½š
    init_kl_coef=0.2,          # KLç³»æ•°
    target_kl=6.0,             # ç›®æ ‡KLå€¼
)

# 3. åˆ›å»ºPPOè®­ç»ƒå™¨
ppo_trainer = PPOTrainer(
    model=model,
    ref_model=ref_model,
    config=ppo_config,
    tokenizer=tokenizer,
    dataset=dataset,
)

# 4. è®­ç»ƒå¾ªç¯
for batch in dataloader:
    # ç”Ÿæˆå“åº”
    query_tensors = batch["input_ids"]
    response_tensors = ppo_trainer.generate(query_tensors)
    
    # è®¡ç®—å¥–åŠ±
    rewards = reward_model(response_tensors)
    
    # PPOæ›´æ–°
    stats = ppo_trainer.step(query_tensors, response_tensors, rewards)
```

---

## ğŸ—ï¸ PPOå››æ¨¡å‹æ¶æ„

> æ¥æºï¼š[å¼ºåŒ–å­¦ä¹ å¯¹é½æŒ‡å—ï¼šPPOå’ŒDPOå®æ–½ä¸è¯„ä¼°](https://dd-ff.blog.csdn.net/article/details/153184150)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      PPO å››æ¨¡å‹æ¶æ„                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   ç­–ç•¥æ¨¡å‹       â”‚   ä»·å€¼æ¨¡å‹       â”‚   å¥–åŠ±æ¨¡å‹       â”‚  å‚è€ƒæ¨¡å‹  â”‚
â”‚   (Policy)      â”‚   (Value)       â”‚   (Reward)      â”‚  (Ref)    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   ç”Ÿæˆå“åº”      â”‚   é¢„æµ‹å›æŠ¥       â”‚   è¯„ä¼°è´¨é‡       â”‚  KLçº¦æŸ   â”‚
â”‚   å¾…è®­ç»ƒ        â”‚   å¾…è®­ç»ƒ         â”‚   å†»ç»“          â”‚  å†»ç»“     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### å„æ¨¡å‹ä½œç”¨

| æ¨¡å‹ | ä½œç”¨ | æ˜¯å¦è®­ç»ƒ |
|------|------|----------|
| **ç­–ç•¥æ¨¡å‹** | ç”Ÿæˆå“åº”ï¼Œæ˜¯æœ€ç»ˆè¦ä¼˜åŒ–çš„æ¨¡å‹ | âœ… è®­ç»ƒ |
| **ä»·å€¼æ¨¡å‹** | é¢„æµ‹æœªæ¥ç´¯ç§¯å¥–åŠ±ï¼Œè¾…åŠ©ç­–ç•¥ä¼˜åŒ– | âœ… è®­ç»ƒ |
| **å¥–åŠ±æ¨¡å‹** | å¯¹å“åº”è´¨é‡æ‰“åˆ† | âŒ å†»ç»“ |
| **å‚è€ƒæ¨¡å‹** | SFTæ¨¡å‹å‰¯æœ¬ï¼Œç”¨äºè®¡ç®—KLæ•£åº¦ | âŒ å†»ç»“ |

### KLæ•£åº¦çº¦æŸ

::: warning é‡è¦
KLæ•£åº¦çº¦æŸé˜²æ­¢ç­–ç•¥æ¨¡å‹åç¦»å‚è€ƒæ¨¡å‹å¤ªè¿œï¼Œé¿å…"å¥–åŠ±hacking"ï¼ˆæ¨¡å‹æ‰¾åˆ°å¥–åŠ±æ¨¡å‹æ¼æ´è€ŒéçœŸæ­£æ”¹è¿›ï¼‰ã€‚
:::

```python
# KLæ•£åº¦è®¡ç®—
kl_divergence = log(policy_prob / ref_prob)

# æœ€ç»ˆå¥–åŠ± = åŸå§‹å¥–åŠ± - KLæƒ©ç½š
final_reward = reward - kl_coef * kl_divergence
```

---

## âš™ï¸ å…³é”®è¶…å‚æ•°

| å‚æ•° | æ¨èå€¼ | è¯´æ˜ |
|------|--------|------|
| **learning_rate** | 1e-6 ~ 1e-5 | PPOå­¦ä¹ ç‡ï¼Œæ¯”SFTæ›´ä½ |
| **kl_coef** | 0.1 ~ 0.2 | KLæƒ©ç½šç³»æ•° |
| **target_kl** | 6.0 | ç›®æ ‡KLå€¼ï¼Œè¶…è¿‡åˆ™åœæ­¢ |
| **ppo_epochs** | 2-4 | æ¯æ‰¹æ•°æ®çš„PPOæ›´æ–°æ¬¡æ•° |
| **clip_range** | 0.2 | PPOè£å‰ªèŒƒå›´ |
| **vf_coef** | 0.1 | ä»·å€¼å‡½æ•°æŸå¤±ç³»æ•° |

---

## ğŸš€ ç®€åŒ–æ–¹æ¡ˆï¼šDPO

> è¯¦è§ [DPOç›´æ¥åå¥½ä¼˜åŒ–](/training/dpo) é¡µé¢

RLHFçš„ä¸»è¦æŒ‘æˆ˜ï¼š
- éœ€è¦è®­ç»ƒ4ä¸ªæ¨¡å‹
- è®­ç»ƒä¸ç¨³å®š
- è®¡ç®—æˆæœ¬é«˜

**DPOï¼ˆDirect Preference Optimizationï¼‰** é€šè¿‡ç›´æ¥ä¼˜åŒ–åå¥½æ•°æ®ï¼Œæ— éœ€å¥–åŠ±æ¨¡å‹ï¼š

```python
# DPOåªéœ€è¦ï¼šåå¥½æ•°æ® + ç­–ç•¥æ¨¡å‹ + å‚è€ƒæ¨¡å‹
# è¯¦è§ dpo.md
```

---

## ğŸ”— ç›¸å…³é˜…è¯»

- [è®­ç»ƒå¾®è°ƒæ¦‚è¿°](/training/) - äº†è§£å®Œæ•´è®­ç»ƒæµç¨‹
- [SFTç›‘ç£å¾®è°ƒ](/training/sft) - RLHFçš„å‰ç½®æ­¥éª¤
- [DPOç›´æ¥åå¥½ä¼˜åŒ–](/training/dpo) - ç®€åŒ–ç‰ˆRLHF

> **ç›¸å…³æ–‡ç« **ï¼š
> - [è¯­è¨€æ¨¡å‹å¯¹é½æŠ€æœ¯è®ºè¿°ï¼šä»PPOåˆ°DPO](https://dd-ff.blog.csdn.net/article/details/153269912)
> - [å¼ºåŒ–å­¦ä¹ å¯¹é½æŒ‡å—ï¼šPPOå’ŒDPOå®æ–½ä¸è¯„ä¼°](https://dd-ff.blog.csdn.net/article/details/153184150)
> - [verlä¸Rayå¤šèŠ‚ç‚¹RLç»ˆææŒ‡å—](https://dd-ff.blog.csdn.net/article/details/154654476)

> **å¤–éƒ¨èµ„æº**ï¼š
> - [InstructGPTè®ºæ–‡](https://arxiv.org/abs/2203.02155)
> - [Hugging Face TRL](https://huggingface.co/docs/trl/ppo_trainer)
> - [OpenAI RLHFåšå®¢](https://openai.com/research/learning-from-human-preferences)

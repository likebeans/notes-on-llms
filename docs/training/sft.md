---
title: SFT ç›‘ç£å¾®è°ƒ
description: Supervised Fine-Tuning - è®©æ¨¡å‹å­¦ä¼šéµå¾ªæŒ‡ä»¤
---

# SFT ç›‘ç£å¾®è°ƒ

> ä»çŸ¥è¯†å‚¨å¤‡åˆ°ä»»åŠ¡æ‰§è¡Œçš„å…³é”®ä¸€æ­¥

## ğŸ¯ æ ¸å¿ƒæ¦‚å¿µ

> æ¥æºï¼š[ä»"æ‰©å……ä¹¦åº“"åˆ°"æ•™æˆæŠ€èƒ½"](https://dd-ff.blog.csdn.net/article/details/152267590)

### ä»€ä¹ˆæ˜¯SFTï¼Ÿ

::: tip å®šä¹‰
**SFTï¼ˆSupervised Fine-Tuningï¼‰** æ˜¯åœ¨é¢„è®­ç»ƒæ¨¡å‹åŸºç¡€ä¸Šï¼Œä½¿ç”¨æ ‡æ³¨çš„æŒ‡ä»¤-å“åº”æ•°æ®è¿›è¡Œç›‘ç£å­¦ä¹ ï¼Œä½¿æ¨¡å‹å­¦ä¼šéµå¾ªäººç±»æŒ‡ä»¤ã€æ‰§è¡Œç‰¹å®šä»»åŠ¡çš„è®­ç»ƒæ–¹æ³•ã€‚
:::

### SFTçš„ä½œç”¨

| é˜¶æ®µ | æ¨¡å‹èƒ½åŠ› | è®­ç»ƒç›®æ ‡ |
|------|----------|----------|
| **é¢„è®­ç»ƒå** | çŸ¥è¯†å‚¨å¤‡ä¸°å¯Œï¼Œä½†ä¸ä¼šå¯¹è¯ | é¢„æµ‹ä¸‹ä¸€ä¸ªToken |
| **SFTå** | ç†è§£æŒ‡ä»¤ï¼ŒæŒ‰è¦æ±‚å›ç­” | ç”Ÿæˆç¬¦åˆæŒ‡ä»¤çš„å“åº” |

```
Base Model (ç»­å†™èƒ½åŠ›) â†’ SFT â†’ Instruct Model (æŒ‡ä»¤éµå¾ª)
```

---

## ğŸ“Š SFTå››ç§æ¨¡å¼

### æ¨¡å¼å¯¹æ¯”

| æ¨¡å¼ | æ•°æ®æ¥æº | ä¼˜ç‚¹ | ç¼ºç‚¹ |
|------|----------|------|------|
| **é€šç”¨SFT** | å¼€æºé€šç”¨æ•°æ®é›† | å¹¿æ³›èƒ½åŠ› | é¢†åŸŸè¡¨ç°ä¸€èˆ¬ |
| **é¢†åŸŸSFT** | é¢†åŸŸä¸“ç”¨æ•°æ® | ä¸“ä¸šæ€§å¼º | å¯èƒ½é—å¿˜é€šç”¨èƒ½åŠ› |
| **æ··åˆSFT** | é€šç”¨+é¢†åŸŸæ··åˆ | å¹³è¡¡ä¸¤è€… | æ•°æ®é…æ¯”éœ€è°ƒä¼˜ |
| **æŒç»­SFT** | å¢é‡é¢†åŸŸæ•°æ® | é€‚åº”æ–°ä»»åŠ¡ | éœ€è¦é˜²é—å¿˜ç­–ç•¥ |

### ç¾éš¾æ€§é—å¿˜é—®é¢˜

::: danger æ ¸å¿ƒæŒ‘æˆ˜
é¢†åŸŸSFTå¯èƒ½å¯¼è‡´æ¨¡å‹"å¿˜è®°"é¢„è®­ç»ƒé˜¶æ®µå­¦åˆ°çš„é€šç”¨çŸ¥è¯†ï¼Œè¿™è¢«ç§°ä¸º**ç¾éš¾æ€§é—å¿˜**ã€‚
:::

**è§£å†³æ–¹æ¡ˆ**ï¼š
- æ··å…¥5-10%é€šç”¨æ•°æ®
- ä½¿ç”¨æ­£åˆ™åŒ–æŠ€æœ¯ï¼ˆå¦‚EWCï¼‰
- é‡‡ç”¨LoRAç­‰å‚æ•°é«˜æ•ˆæ–¹æ³•

---

## ğŸ”§ SFTå®ç°

### ä½¿ç”¨Transformers

```python
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer
)
from datasets import load_dataset

# 1. åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
model_name = "meta-llama/Llama-2-7b-hf"
model = AutoModelForCausalLM.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token

# 2. å‡†å¤‡æ•°æ®é›†
def format_instruction(sample):
    """æ ¼å¼åŒ–ä¸ºæŒ‡ä»¤æ ¼å¼"""
    return f"""### æŒ‡ä»¤:
{sample['instruction']}

### è¾“å…¥:
{sample.get('input', '')}

### å›ç­”:
{sample['output']}"""

def tokenize(sample):
    text = format_instruction(sample)
    return tokenizer(
        text,
        truncation=True,
        max_length=2048,
        padding="max_length"
    )

dataset = load_dataset("json", data_files="train.json")
tokenized_dataset = dataset.map(tokenize, remove_columns=dataset["train"].column_names)

# 3. è®­ç»ƒé…ç½®
training_args = TrainingArguments(
    output_dir="./sft_output",
    num_train_epochs=3,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=8,
    learning_rate=2e-5,
    warmup_ratio=0.1,
    logging_steps=10,
    save_strategy="epoch",
    fp16=True,
)

# 4. å¼€å§‹è®­ç»ƒ
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"],
)
trainer.train()
```

### ä½¿ç”¨TRL SFTTrainer

```python
from trl import SFTTrainer, SFTConfig
from transformers import AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-7b-hf")
tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-hf")

# SFTé…ç½®
sft_config = SFTConfig(
    output_dir="./sft_output",
    max_seq_length=2048,
    num_train_epochs=3,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=8,
    learning_rate=2e-5,
    packing=True,  # æ ·æœ¬æ‰“åŒ…ï¼Œæå‡æ•ˆç‡
)

# åˆ›å»ºè®­ç»ƒå™¨
trainer = SFTTrainer(
    model=model,
    args=sft_config,
    train_dataset=dataset,
    tokenizer=tokenizer,
    formatting_func=format_instruction,
)

trainer.train()
```

---

## ğŸ“ æ¨¡æ¿è®¾è®¡

### å¸¸è§æ¨¡æ¿æ ¼å¼

#### Alpacaæ¨¡æ¿
```
Below is an instruction that describes a task. Write a response that appropriately completes the request.

### Instruction:
{instruction}

### Input:
{input}

### Response:
{output}
```

#### ChatMLæ¨¡æ¿
```
<|im_start|>system
{system_message}<|im_end|>
<|im_start|>user
{user_message}<|im_end|>
<|im_start|>assistant
{assistant_message}<|im_end|>
```

#### Llama2æ¨¡æ¿
```
<s>[INST] <<SYS>>
{system_message}
<</SYS>>

{user_message} [/INST] {assistant_message} </s>
```

### æŸå¤±æ©ç 

::: warning é‡è¦
SFTè®­ç»ƒæ—¶ï¼Œåªåº”è¯¥è®¡ç®—**å“åº”éƒ¨åˆ†**çš„æŸå¤±ï¼Œä¸åº”è¯¥è®¡ç®—æŒ‡ä»¤/è¾“å…¥éƒ¨åˆ†çš„æŸå¤±ã€‚
:::

```python
def create_labels_with_mask(input_ids, response_start_idx):
    """åˆ›å»ºå¸¦æ©ç çš„æ ‡ç­¾"""
    labels = input_ids.clone()
    # å°†æŒ‡ä»¤éƒ¨åˆ†çš„æ ‡ç­¾è®¾ä¸º-100ï¼ˆå¿½ç•¥ï¼‰
    labels[:response_start_idx] = -100
    return labels
```

---

## âš™ï¸ è¶…å‚æ•°è°ƒä¼˜

### å…³é”®è¶…å‚æ•°

| å‚æ•° | æ¨èå€¼ | è¯´æ˜ |
|------|--------|------|
| **learning_rate** | 1e-5 ~ 5e-5 | å­¦ä¹ ç‡ï¼Œå¤ªé«˜æ˜“è¿‡æ‹Ÿåˆ |
| **batch_size** | æ ¹æ®æ˜¾å­˜è°ƒæ•´ | æœ‰æ•ˆbatch=batchÃ—gradient_accumulation |
| **epochs** | 1-3 | SFTé€šå¸¸ä¸éœ€è¦å¤ªå¤šè½®æ¬¡ |
| **warmup_ratio** | 0.03-0.1 | é¢„çƒ­æ¯”ä¾‹ |
| **max_seq_length** | 2048-4096 | æœ€å¤§åºåˆ—é•¿åº¦ |

### å­¦ä¹ ç‡è°ƒåº¦

```python
from transformers import get_cosine_schedule_with_warmup

# ä½™å¼¦é€€ç«è°ƒåº¦å™¨
scheduler = get_cosine_schedule_with_warmup(
    optimizer,
    num_warmup_steps=100,
    num_training_steps=1000
)
```

---

## ğŸ“ˆ è¯„ä¼°æ–¹æ³•

### è‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡

```python
from evaluate import load

# åŠ è½½è¯„ä¼°æŒ‡æ ‡
bleu = load("bleu")
rouge = load("rouge")

def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)
    
    # BLEUåˆ†æ•°
    bleu_score = bleu.compute(
        predictions=decoded_preds, 
        references=[[l] for l in decoded_labels]
    )
    
    # ROUGEåˆ†æ•°
    rouge_score = rouge.compute(
        predictions=decoded_preds, 
        references=decoded_labels
    )
    
    return {
        "bleu": bleu_score["bleu"],
        "rouge-l": rouge_score["rougeL"]
    }
```

### äººå·¥è¯„ä¼°ç»´åº¦

| ç»´åº¦ | è¯„ä¼°å†…å®¹ |
|------|----------|
| **ç›¸å…³æ€§** | å›ç­”æ˜¯å¦åˆ‡é¢˜ |
| **å‡†ç¡®æ€§** | å†…å®¹æ˜¯å¦æ­£ç¡® |
| **æµç•…æ€§** | è¡¨è¾¾æ˜¯å¦è‡ªç„¶ |
| **å®Œæ•´æ€§** | æ˜¯å¦è¦†ç›–è¦ç‚¹ |
| **å®‰å…¨æ€§** | æ˜¯å¦æœ‰å®³å†…å®¹ |

---

## ğŸ”— ç›¸å…³é˜…è¯»

- [è®­ç»ƒå¾®è°ƒæ¦‚è¿°](/training/) - äº†è§£å®Œæ•´è®­ç»ƒæµç¨‹
- [æ•°æ®å¤„ç†](/training/data) - å‡†å¤‡é«˜è´¨é‡è®­ç»ƒæ•°æ®
- [LoRAé«˜æ•ˆå¾®è°ƒ](/training/lora) - ä½èµ„æºSFTæ–¹æ¡ˆ
- [RLHFå¯¹é½](/training/rlhf) - SFTä¹‹åçš„å¯¹é½

> **ç›¸å…³æ–‡ç« **ï¼š
> - [ä»"æ‰©å……ä¹¦åº“"åˆ°"æ•™æˆæŠ€èƒ½"](https://dd-ff.blog.csdn.net/article/details/152267590)
> - [æ·±å…¥æ¢ç§˜LLMçš„"æš—è¯­"ï¼šç‰¹æ®ŠTokenä¸æ¨¡æ¿](https://dd-ff.blog.csdn.net/article/details/152328698)
> - [Genesis-LLMå…¨æµç¨‹å¼€æºé¡¹ç›®è§£æ](https://dd-ff.blog.csdn.net/article/details/155355144)

> **å¤–éƒ¨èµ„æº**ï¼š
> - [Hugging Face TRL](https://huggingface.co/docs/trl/sft_trainer)
> - [LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory)

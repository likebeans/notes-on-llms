---
title: Agent è¯„ä¼°æ–¹æ³•
description: Agent æ€§èƒ½è¯„ä¼° - ä»æŒ‡æ ‡è®¾è®¡åˆ°è¯„ä¼°æ¡†æ¶
---

# Agent è¯„ä¼°æ–¹æ³•

> ç§‘å­¦è¯„ä¼°AIæ™ºèƒ½ä½“çš„èƒ½åŠ›ä¸å¯é æ€§

## ğŸ¯ æ ¸å¿ƒæ¦‚å¿µ

### ä¸ºä»€ä¹ˆéœ€è¦Agentè¯„ä¼°ï¼Ÿ

::: tip è¯„ä¼°çš„ä»·å€¼
Agentç³»ç»Ÿæ¯”ä¼ ç»ŸLLMåº”ç”¨æ›´å¤æ‚ï¼Œéœ€è¦è¯„ä¼°çš„ä¸ä»…æ˜¯å›ç­”è´¨é‡ï¼Œè¿˜åŒ…æ‹¬ï¼š
- **ä»»åŠ¡å®Œæˆèƒ½åŠ›**ï¼šèƒ½å¦è¾¾æˆç›®æ ‡
- **å·¥å…·ä½¿ç”¨æ•ˆç‡**ï¼šè°ƒç”¨æ˜¯å¦åˆç†
- **æ¨ç†è¿‡ç¨‹è´¨é‡**ï¼šæ€è€ƒé“¾æ˜¯å¦æ­£ç¡®
- **å®‰å…¨æ€§**ï¼šæ˜¯å¦äº§ç”Ÿæœ‰å®³è¡Œä¸º
:::

### Agentè¯„ä¼°çš„æŒ‘æˆ˜

| æŒ‘æˆ˜ | æè¿° |
|------|------|
| **å¼€æ”¾å¼ä»»åŠ¡** | æ²¡æœ‰å”¯ä¸€æ­£ç¡®ç­”æ¡ˆ |
| **å¤šæ­¥éª¤è¿‡ç¨‹** | éœ€è¦è¯„ä¼°ä¸­é—´æ­¥éª¤ |
| **å·¥å…·äº¤äº’** | å¤–éƒ¨APIè°ƒç”¨éš¾ä»¥å¤ç° |
| **éç¡®å®šæ€§** | ç›¸åŒè¾“å…¥å¯èƒ½äº§ç”Ÿä¸åŒè¾“å‡º |
| **æˆæœ¬é«˜æ˜‚** | å®Œæ•´è¯„ä¼°éœ€è¦å¤§é‡APIè°ƒç”¨ |

---

## ğŸ“Š è¯„ä¼°ç»´åº¦

### å››ç»´è¯„ä¼°æ¡†æ¶

| ç»´åº¦ | è¯„ä¼°å†…å®¹ | å…³é”®æŒ‡æ ‡ |
|------|----------|----------|
| **ä»»åŠ¡å®Œæˆ** | æœ€ç»ˆç»“æœæ˜¯å¦æ­£ç¡® | æˆåŠŸç‡ã€å‡†ç¡®ç‡ |
| **æ•ˆç‡** | èµ„æºæ¶ˆè€—æ˜¯å¦åˆç† | æ­¥éª¤æ•°ã€Tokenç”¨é‡ã€æ—¶é—´ |
| **è¿‡ç¨‹è´¨é‡** | æ¨ç†å’Œå·¥å…·ä½¿ç”¨ | å·¥å…·é€‰æ‹©å‡†ç¡®ç‡ã€æ¨ç†æ­£ç¡®ç‡ |
| **å®‰å…¨æ€§** | æ˜¯å¦äº§ç”Ÿæœ‰å®³è¡Œä¸º | è¶Šç•Œç‡ã€é”™è¯¯è°ƒç”¨ç‡ |

### ä»»åŠ¡å®Œæˆåº¦è¯„ä¼°

```python
class TaskCompletionEvaluator:
    """ä»»åŠ¡å®Œæˆåº¦è¯„ä¼°å™¨"""
    
    def evaluate(self, task: str, result: dict, ground_truth: dict) -> dict:
        """è¯„ä¼°ä»»åŠ¡å®Œæˆæƒ…å†µ"""
        scores = {}
        
        # 1. ç›®æ ‡è¾¾æˆåº¦ï¼ˆ0-1ï¼‰
        scores["goal_achievement"] = self._check_goal(result, ground_truth)
        
        # 2. ç­”æ¡ˆæ­£ç¡®æ€§
        scores["answer_correctness"] = self._check_answer(
            result.get("answer"), 
            ground_truth.get("answer")
        )
        
        # 3. å®Œæ•´æ€§
        scores["completeness"] = self._check_completeness(
            result.get("output"),
            ground_truth.get("required_elements", [])
        )
        
        # ç»¼åˆè¯„åˆ†
        scores["overall"] = sum(scores.values()) / len(scores)
        
        return scores
    
    def _check_goal(self, result: dict, ground_truth: dict) -> float:
        """æ£€æŸ¥ç›®æ ‡æ˜¯å¦è¾¾æˆ"""
        if result.get("status") == "completed":
            # ä½¿ç”¨LLMåˆ¤æ–­ç»“æœæ˜¯å¦æ»¡è¶³ç›®æ ‡
            prompt = f"""åˆ¤æ–­ä»¥ä¸‹ç»“æœæ˜¯å¦å®Œæˆäº†ä»»åŠ¡ç›®æ ‡ï¼š
            
ç›®æ ‡ï¼š{ground_truth.get('goal')}
ç»“æœï¼š{result.get('output')}

è¾“å‡º0åˆ°1ä¹‹é—´çš„åˆ†æ•°ï¼Œ1è¡¨ç¤ºå®Œå…¨å®Œæˆã€‚"""
            score = float(llm.generate(prompt))
            return min(max(score, 0), 1)
        return 0.0
```

### æ•ˆç‡è¯„ä¼°

```python
class EfficiencyEvaluator:
    """æ•ˆç‡è¯„ä¼°å™¨"""
    
    def evaluate(self, execution_trace: list) -> dict:
        """è¯„ä¼°æ‰§è¡Œæ•ˆç‡"""
        return {
            # æ­¥éª¤æ•ˆç‡
            "step_count": len(execution_trace),
            "redundant_steps": self._count_redundant_steps(execution_trace),
            
            # Tokenæ•ˆç‡
            "total_tokens": sum(step.get("tokens", 0) for step in execution_trace),
            "tokens_per_step": self._avg_tokens_per_step(execution_trace),
            
            # æ—¶é—´æ•ˆç‡
            "total_time": sum(step.get("duration", 0) for step in execution_trace),
            "tool_call_time": self._sum_tool_time(execution_trace),
            
            # å·¥å…·æ•ˆç‡
            "tool_calls": len([s for s in execution_trace if s.get("type") == "tool"]),
            "failed_tool_calls": len([
                s for s in execution_trace 
                if s.get("type") == "tool" and not s.get("success")
            ]),
        }
    
    def _count_redundant_steps(self, trace: list) -> int:
        """ç»Ÿè®¡å†—ä½™æ­¥éª¤ï¼ˆé‡å¤çš„å·¥å…·è°ƒç”¨ï¼‰"""
        seen = set()
        redundant = 0
        for step in trace:
            if step.get("type") == "tool":
                key = (step.get("tool"), str(step.get("args")))
                if key in seen:
                    redundant += 1
                seen.add(key)
        return redundant
```

### è¿‡ç¨‹è´¨é‡è¯„ä¼°

```python
class ProcessQualityEvaluator:
    """è¿‡ç¨‹è´¨é‡è¯„ä¼°å™¨"""
    
    def evaluate(self, execution_trace: list, task: str) -> dict:
        """è¯„ä¼°æ¨ç†å’Œå·¥å…·ä½¿ç”¨è´¨é‡"""
        scores = {}
        
        # 1. æ¨ç†è´¨é‡
        thoughts = [s for s in execution_trace if s.get("type") == "thought"]
        scores["reasoning_quality"] = self._evaluate_reasoning(thoughts, task)
        
        # 2. å·¥å…·é€‰æ‹©å‡†ç¡®æ€§
        tool_calls = [s for s in execution_trace if s.get("type") == "tool"]
        scores["tool_selection"] = self._evaluate_tool_selection(tool_calls, task)
        
        # 3. é”™è¯¯æ¢å¤èƒ½åŠ›
        scores["error_recovery"] = self._evaluate_error_recovery(execution_trace)
        
        return scores
    
    def _evaluate_reasoning(self, thoughts: list, task: str) -> float:
        """è¯„ä¼°æ¨ç†è´¨é‡"""
        if not thoughts:
            return 0.0
        
        prompt = f"""è¯„ä¼°ä»¥ä¸‹æ¨ç†è¿‡ç¨‹çš„è´¨é‡ï¼ˆ0-1åˆ†ï¼‰ï¼š

ä»»åŠ¡ï¼š{task}

æ¨ç†è¿‡ç¨‹ï¼š
{chr(10).join([t.get('content', '') for t in thoughts])}

è¯„ä¼°æ ‡å‡†ï¼š
- é€»è¾‘æ¸…æ™°æ€§
- ä¸ä»»åŠ¡ç›¸å…³æ€§
- æ¨ç†æ­¥éª¤åˆç†æ€§

è¾“å‡ºåˆ†æ•°ï¼š"""
        return float(llm.generate(prompt))
    
    def _evaluate_tool_selection(self, tool_calls: list, task: str) -> float:
        """è¯„ä¼°å·¥å…·é€‰æ‹©æ˜¯å¦åˆç†"""
        if not tool_calls:
            return 1.0  # æ²¡æœ‰å·¥å…·è°ƒç”¨ä¸æ‰£åˆ†
        
        correct = 0
        for call in tool_calls:
            if self._is_appropriate_tool(call, task):
                correct += 1
        
        return correct / len(tool_calls)
```

---

## ğŸ§ª è¯„ä¼°åŸºå‡†ï¼ˆBenchmarksï¼‰

### å¸¸ç”¨Agent Benchmarks

| Benchmark | è¯„ä¼°å†…å®¹ | ä»»åŠ¡ç±»å‹ |
|-----------|----------|----------|
| **AgentBench** | é€šç”¨Agentèƒ½åŠ› | æ“ä½œç³»ç»Ÿã€æ•°æ®åº“ã€ç½‘é¡µç­‰ |
| **WebArena** | ç½‘é¡µæ“ä½œèƒ½åŠ› | ç”µå•†ã€ç¤¾äº¤ã€åœ°å›¾ç­‰ç½‘ç«™ |
| **SWE-bench** | ä»£ç ä¿®å¤èƒ½åŠ› | GitHub Issueä¿®å¤ |
| **GAIA** | é€šç”¨åŠ©æ‰‹èƒ½åŠ› | é—®ç­”ã€æ–‡ä»¶å¤„ç†ã€ç½‘ç»œæœç´¢ |
| **ToolBench** | å·¥å…·ä½¿ç”¨èƒ½åŠ› | APIè°ƒç”¨ã€å·¥å…·ç»„åˆ |

### è‡ªå®šä¹‰è¯„ä¼°é›†æ„å»º

```python
class EvaluationDataset:
    """è¯„ä¼°æ•°æ®é›†"""
    
    def __init__(self):
        self.test_cases = []
    
    def add_case(
        self,
        task: str,
        expected_output: str,
        required_tools: list = None,
        max_steps: int = 10,
        difficulty: str = "medium"
    ):
        """æ·»åŠ æµ‹è¯•ç”¨ä¾‹"""
        self.test_cases.append({
            "task": task,
            "expected_output": expected_output,
            "required_tools": required_tools or [],
            "max_steps": max_steps,
            "difficulty": difficulty,
            "metadata": {
                "created_at": time.time(),
                "category": self._categorize(task)
            }
        })
    
    def run_evaluation(self, agent, evaluator) -> dict:
        """è¿è¡Œè¯„ä¼°"""
        results = []
        
        for case in self.test_cases:
            # æ‰§è¡ŒAgent
            trace = agent.run(case["task"])
            
            # è¯„ä¼°ç»“æœ
            score = evaluator.evaluate(
                task=case["task"],
                result=trace[-1] if trace else {},
                ground_truth={"answer": case["expected_output"]}
            )
            
            results.append({
                "case": case,
                "trace": trace,
                "score": score
            })
        
        # æ±‡æ€»ç»Ÿè®¡
        return {
            "cases": results,
            "summary": self._summarize(results)
        }
    
    def _summarize(self, results: list) -> dict:
        """æ±‡æ€»è¯„ä¼°ç»“æœ"""
        scores = [r["score"]["overall"] for r in results]
        return {
            "total_cases": len(results),
            "avg_score": sum(scores) / len(scores) if scores else 0,
            "success_rate": len([s for s in scores if s > 0.8]) / len(scores),
            "by_difficulty": self._group_by_difficulty(results)
        }
```

---

## ğŸ”„ åœ¨çº¿è¯„ä¼°ä¸ç›‘æ§

### ç”Ÿäº§ç¯å¢ƒç›‘æ§æŒ‡æ ‡

```python
class AgentMonitor:
    """Agentç”Ÿäº§ç›‘æ§"""
    
    def __init__(self):
        self.metrics = defaultdict(list)
    
    def record_execution(self, execution_data: dict):
        """è®°å½•æ‰§è¡Œæ•°æ®"""
        # å»¶è¿ŸæŒ‡æ ‡
        self.metrics["latency"].append(execution_data["duration"])
        
        # æˆåŠŸç‡
        self.metrics["success"].append(1 if execution_data["success"] else 0)
        
        # Tokenæ¶ˆè€—
        self.metrics["tokens"].append(execution_data["tokens"])
        
        # å·¥å…·è°ƒç”¨
        self.metrics["tool_calls"].append(len(execution_data["tools_used"]))
        
        # é”™è¯¯è¿½è¸ª
        if not execution_data["success"]:
            self.metrics["errors"].append({
                "type": execution_data.get("error_type"),
                "message": execution_data.get("error_message"),
                "timestamp": time.time()
            })
    
    def get_dashboard_data(self) -> dict:
        """è·å–ä»ªè¡¨ç›˜æ•°æ®"""
        return {
            "avg_latency": np.mean(self.metrics["latency"][-100:]),
            "p95_latency": np.percentile(self.metrics["latency"][-100:], 95),
            "success_rate": np.mean(self.metrics["success"][-100:]),
            "avg_tokens": np.mean(self.metrics["tokens"][-100:]),
            "error_rate": 1 - np.mean(self.metrics["success"][-100:]),
            "recent_errors": self.metrics["errors"][-10:]
        }
```

### A/Bæµ‹è¯•æ¡†æ¶

```python
class AgentABTest:
    """Agent A/Bæµ‹è¯•"""
    
    def __init__(self, agent_a, agent_b, split_ratio: float = 0.5):
        self.agent_a = agent_a
        self.agent_b = agent_b
        self.split_ratio = split_ratio
        self.results = {"a": [], "b": []}
    
    def run(self, task: str) -> dict:
        """æ‰§è¡ŒA/Bæµ‹è¯•"""
        import random
        
        # éšæœºåˆ†é…
        use_a = random.random() < self.split_ratio
        agent = self.agent_a if use_a else self.agent_b
        group = "a" if use_a else "b"
        
        # æ‰§è¡Œ
        start = time.time()
        result = agent.run(task)
        duration = time.time() - start
        
        # è®°å½•
        self.results[group].append({
            "task": task,
            "result": result,
            "duration": duration,
            "success": result.get("success", False)
        })
        
        return {"group": group, "result": result}
    
    def get_comparison(self) -> dict:
        """è·å–å¯¹æ¯”ç»“æœ"""
        def stats(results):
            if not results:
                return {}
            return {
                "count": len(results),
                "success_rate": sum(1 for r in results if r["success"]) / len(results),
                "avg_duration": sum(r["duration"] for r in results) / len(results)
            }
        
        return {
            "agent_a": stats(self.results["a"]),
            "agent_b": stats(self.results["b"]),
            "winner": self._determine_winner()
        }
```

---

## ğŸ“ˆ è¯„ä¼°æŠ¥å‘Šç”Ÿæˆ

```python
class EvaluationReporter:
    """è¯„ä¼°æŠ¥å‘Šç”Ÿæˆå™¨"""
    
    def generate_report(self, evaluation_results: dict) -> str:
        """ç”ŸæˆMarkdownæ ¼å¼æŠ¥å‘Š"""
        report = f"""# Agent è¯„ä¼°æŠ¥å‘Š

## æ¦‚è¿°
- è¯„ä¼°æ—¶é—´ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M')}
- æµ‹è¯•ç”¨ä¾‹æ•°ï¼š{evaluation_results['summary']['total_cases']}
- å¹³å‡å¾—åˆ†ï¼š{evaluation_results['summary']['avg_score']:.2%}
- æˆåŠŸç‡ï¼š{evaluation_results['summary']['success_rate']:.2%}

## è¯¦ç»†æŒ‡æ ‡

### ä»»åŠ¡å®Œæˆåº¦
| æŒ‡æ ‡ | æ•°å€¼ |
|------|------|
| ç›®æ ‡è¾¾æˆç‡ | {self._get_metric('goal_achievement'):.2%} |
| ç­”æ¡ˆæ­£ç¡®ç‡ | {self._get_metric('answer_correctness'):.2%} |

### æ•ˆç‡æŒ‡æ ‡
| æŒ‡æ ‡ | æ•°å€¼ |
|------|------|
| å¹³å‡æ­¥éª¤æ•° | {self._get_metric('step_count'):.1f} |
| å¹³å‡Tokenæ¶ˆè€— | {self._get_metric('total_tokens'):.0f} |
| å·¥å…·è°ƒç”¨æˆåŠŸç‡ | {self._get_metric('tool_success_rate'):.2%} |

### æŒ‰éš¾åº¦åˆ†å¸ƒ
{self._difficulty_table(evaluation_results)}

## å¤±è´¥æ¡ˆä¾‹åˆ†æ
{self._failure_analysis(evaluation_results)}
"""
        return report
```

---

## ğŸ”— ç›¸å…³é˜…è¯»

- [Agentæ¦‚è¿°](/agent/) - Agentæ•´ä½“æ¶æ„
- [å®‰å…¨ä¸æ²™ç®±](/agent/safety) - å®‰å…¨æ€§è¯„ä¼°
- [RAGè¯„ä¼°æ–¹æ³•](/rag/evaluation) - æ£€ç´¢ç”Ÿæˆè¯„ä¼°

> **ç›¸å…³æ–‡ç« **ï¼š
> - [12-Factor Agentæ–¹æ³•è®º](https://dd-ff.blog.csdn.net/article/details/154185674)
> - [æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿç»¼åˆè¯„ä¼°](https://dd-ff.blog.csdn.net/article/details/152823514)

> **å¤–éƒ¨èµ„æº**ï¼š
> - [AgentBench](https://github.com/THUDM/AgentBench)
> - [WebArena](https://webarena.dev/)
> - [SWE-bench](https://www.swebench.com/)

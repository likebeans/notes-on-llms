---
title: DPO ç›´æ¥åå¥½ä¼˜åŒ–
description: Direct Preference Optimization - æ— éœ€å¥–åŠ±æ¨¡å‹çš„ç®€åŒ–å¯¹é½
---

# DPO ç›´æ¥åå¥½ä¼˜åŒ–

> ç”¨ç›‘ç£å­¦ä¹ çš„æ–¹å¼åšå¼ºåŒ–å­¦ä¹ çš„äº‹

## ğŸ¯ æ ¸å¿ƒæ¦‚å¿µ

> æ¥æºï¼š[å¼ºåŒ–å­¦ä¹ å¯¹é½æŒ‡å—ï¼šPPOå’ŒDPOå®æ–½ä¸è¯„ä¼°](https://dd-ff.blog.csdn.net/article/details/153184150)

### ä»€ä¹ˆæ˜¯DPOï¼Ÿ

::: tip å®šä¹‰
**DPOï¼ˆDirect Preference Optimizationï¼‰** æ˜¯ä¸€ç§ç›´æ¥åœ¨åå¥½æ•°æ®ä¸Šä¼˜åŒ–è¯­è¨€æ¨¡å‹çš„æ–¹æ³•ï¼Œæ— éœ€è®­ç»ƒå¥–åŠ±æ¨¡å‹ï¼Œå°†RLHFç®€åŒ–ä¸ºç±»ä¼¼ç›‘ç£å­¦ä¹ çš„è¿‡ç¨‹ã€‚
:::

### DPO vs RLHF

| ç‰¹æ€§ | RLHF (PPO) | DPO |
|------|------------|-----|
| **æ¨¡å‹æ•°é‡** | 4ä¸ªï¼ˆç­–ç•¥+ä»·å€¼+å¥–åŠ±+å‚è€ƒï¼‰ | 2ä¸ªï¼ˆç­–ç•¥+å‚è€ƒï¼‰ |
| **è®­ç»ƒå¤æ‚åº¦** | é«˜ï¼ˆå¼ºåŒ–å­¦ä¹ ï¼‰ | ä½ï¼ˆç›‘ç£å­¦ä¹ ï¼‰ |
| **ç¨³å®šæ€§** | éœ€è¦ç²¾ç»†è°ƒå‚ | ç›¸å¯¹ç¨³å®š |
| **è®¡ç®—æˆæœ¬** | é«˜ | ä¸­ç­‰ |
| **æ•ˆæœ** | æœ€ä½³ | æ¥è¿‘RLHF |

---

## ğŸ”¬ DPOåŸç†

> æ¥æºï¼š[RLHFä¹‹PPOã€DPOè¯¦è§£](https://www.zhihu.com/tardis/zm/art/717010380) | [DPOåŸç†æ·±åº¦è§£æ](https://zhuanlan.zhihu.com/p/11913305485)

![PPO vs DPO](https://pic2.zhimg.com/v2-44f397a445692fe8631990b251d10bdf_r.jpg)
*PPO å’Œ DPO çš„åŒºåˆ«*

### æ ¸å¿ƒæ€æƒ³

DPOçš„å…³é”®æ´å¯Ÿï¼š**RLHF çš„ä¼˜åŒ–ç›®æ ‡å­˜åœ¨æ˜¾å¼è§£ï¼Œå¯ä»¥å°†å¥–åŠ±å‡½æ•°ä¸æœ€ä¼˜ç­–ç•¥å»ºç«‹è§£ææ˜ å°„**ã€‚

### ä» PPO åˆ° DPO çš„æ•°å­¦æ¨å¯¼

**Step 1ï¼šPPO çš„æœ€ä¼˜ç­–ç•¥å½¢å¼**

åœ¨ KL æ­£åˆ™åŒ–çº¦æŸä¸‹ï¼ŒPPO çš„æœ€ä¼˜ç­–ç•¥å¯ä»¥å†™ä¸ºï¼š

$$\pi^*(y|x) = \frac{1}{Z(x)} \pi_{ref}(y|x) \exp\left(\frac{1}{\beta} r(x,y)\right)$$

å…¶ä¸­ $Z(x) = \sum_y \pi_{ref}(y|x) \exp\left(\frac{1}{\beta} r(x,y)\right)$ æ˜¯å½’ä¸€åŒ–çš„åˆ†åŒºå‡½æ•°ã€‚

**Step 2ï¼šé‡å‚æ•°åŒ–å¥–åŠ±å‡½æ•°**

å°†ä¸Šå¼å¯¹æ•°åŒ–å¹¶é‡æ’ï¼Œå¯ä»¥å¾—åˆ°å¥–åŠ±å‡½æ•°çš„å½¢å¼ï¼š

$$r(x,y) = \beta \log \frac{\pi^*(y|x)}{\pi_{ref}(y|x)} + \beta \log Z(x)$$

**Step 3ï¼šä»£å…¥ Bradley-Terry åå¥½æ¨¡å‹**

åå¥½æ•°æ®éµå¾ª Bradley-Terry æ¨¡å‹ï¼Œä»£å…¥é‡å‚æ•°åŒ–åçš„ $r(x,y)$ å¹¶æ¶ˆå» $Z(x)$ï¼Œå¾—åˆ°ï¼š

$$p(y_w \succ y_l | x) = \sigma \left( \beta \log \frac{\pi_\theta(y_w|x)}{\pi_{ref}(y_w|x)} - \beta \log \frac{\pi_\theta(y_l|x)}{\pi_{ref}(y_l|x)} \right)$$

**Step 4ï¼šæœ€ç»ˆ DPO æŸå¤±å‡½æ•°**

$$\mathcal{L}_{DPO}(\pi_\theta; \pi_{ref}) = -\mathbb{E}_{(x, y_w, y_l) \sim D}\left[\log \sigma \left(\beta \log \frac{\pi_\theta(y_w|x)}{\pi_{ref}(y_w|x)} - \beta \log \frac{\pi_\theta(y_l|x)}{\pi_{ref}(y_l|x)}\right)\right]$$

å…¶ä¸­ï¼š
- $y_w$: åå¥½çš„ï¼ˆchosenï¼‰å“åº”
- $y_l$: ä¸åå¥½çš„ï¼ˆrejectedï¼‰å“åº”
- $\beta$: æ¸©åº¦å‚æ•°
- $\sigma$: sigmoidå‡½æ•°

**DPO æœ¬è´¨**ï¼šå°† RLHF å·§å¦™è½¬åŒ–ä¸ºç±»ä¼¼ SFT çš„ç›‘ç£å­¦ä¹ ï¼Œéšå¼å­¦ä¹ å¥–åŠ±å‡½æ•°ã€‚

### ç›´è§‚ç†è§£

```
DPOç›®æ ‡ï¼š
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  å¢åŠ  chosen å“åº”çš„æ¦‚ç‡              â”‚
  â”‚  é™ä½ rejected å“åº”çš„æ¦‚ç‡            â”‚
  â”‚  åŒæ—¶ä¸è¦åç¦»å‚è€ƒæ¨¡å‹å¤ªè¿œ             â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âš ï¸ DPO vs PPO æ·±åº¦åˆ†æ

> æ¥æºï¼š[DPO vs PPOï¼šæ·±åº¦è§£è¯»è°æ˜¯LLM Alignmentçš„æœªæ¥](https://zhuanlan.zhihu.com/p/11913305485)

è™½ç„¶ DPO çš„æ¨å¯¼çœ‹ä¼¼ä¸ PPO ç­‰ä»·ï¼Œä½†å®é™…å­˜åœ¨å‡ ä¸ªå…³é”®å·®å¼‚ï¼š

### 1. Distribution Shiftï¼ˆåˆ†å¸ƒåç§»ï¼‰

DPO å‡è®¾å‚è€ƒåˆ†å¸ƒ $\pi_{ref}$ èƒ½å‡†ç¡®æ•æ‰åå¥½æ•°æ®åˆ†å¸ƒï¼Œä½†å®é™…ä¸­å¸¸å­˜åœ¨åç§»ï¼š

| é—®é¢˜ | DPO | PPO |
|------|-----|-----|
| **OOD æ•°æ®å¤„ç†** | å¯èƒ½é”™è¯¯æé«˜ OOD æ ·æœ¬æ¦‚ç‡ | KL æ­£åˆ™åŒ–æŠ‘åˆ¶åç§» |
| **åˆ†å¸ƒå‡è®¾** | ä¾èµ– $\pi_{ref}$ å‡†ç¡®æ€§ | æ˜¾å¼çº¦æŸåç¦»ç¨‹åº¦ |

PPO é€šè¿‡æ˜¾å¼ KL æ­£åˆ™åŒ–é™åˆ¶ $\pi_\theta$ åç¦» $\pi_{ref}$ çš„ç¨‹åº¦ï¼š

$$\max_\pi \mathbb{E}_{x,y \sim \pi_\theta}\left[r(x,y) - \beta D_{KL}(\pi_\theta(y|x) || \pi_{ref}(y|x))\right]$$

### 2. Reward Hacking é£é™©

DPO é€šè¿‡éšå¼å»ºæ¨¡å¥–åŠ±å‡½æ•°ç»•è¿‡æ˜¾å¼å¥–åŠ±å»ºæ¨¡ï¼Œä½†è¿™å¯èƒ½å¼•å…¥é¢å¤–çš„ Reward Hacking é—®é¢˜ï¼š

- DPO çš„è§£é›† $\Pi_{DPO}$ **åŒ…å«** PPO çš„è§£é›† $\Pi_{PPO}$ï¼š$\Pi_{PPO} \subset \Pi_{DPO}$
- DPO å¯èƒ½æ‰¾åˆ°ç¬¦åˆåå¥½æ•°æ®ä½†åœ¨å®é™…åˆ†å¸ƒä¸Šæ— æ„ä¹‰çš„è§£
- PPO çš„æ˜¾å¼å¥–åŠ±å‡½æ•°å’Œ KL æ­£åˆ™åŒ–å¯å‡å°‘ Reward Hacking é£é™©

### 3. åˆ†åŒºå‡½æ•°ç¼ºå¤±

DPO åœ¨æ¨å¯¼ä¸­çœç•¥äº†åˆ†åŒºå‡½æ•° $Z(x)$ çš„æ˜¾å¼å½±å“ï¼š

**PPO**ï¼š$Z(x)$ çš„å½’ä¸€åŒ–ç¡®ä¿ $\pi^*(y|x)$ æ˜¯åˆæ³•æ¦‚ç‡åˆ†å¸ƒ

**DPO**ï¼šç›´æ¥æ¶ˆå» $Z(x)$ï¼Œå‡è®¾åˆ†å¸ƒè¶³å¤Ÿä¸€è‡´

å½“å‚è€ƒåˆ†å¸ƒ $\pi_{ref}(y|x)$ ä¸å¤Ÿå‡†ç¡®æ—¶ï¼Œè¿™ç§çœç•¥å¯èƒ½å¯¼è‡´å¯¹æŸäº›é€‰é¡¹èµ‹äºˆä¸åˆç†çš„é«˜æƒé‡ã€‚

::: tip æŠ«è¨åº—ç±»æ¯”
- **PPO** åƒä¸¥æ ¼çš„æœ‹å‹ï¼šåˆ†ææ¯ç§é€‰æ‹©çš„å¥½åï¼Œç»“åˆå†å²è®°å½•ï¼Œè®¡ç®—ç»¼åˆè¯„åˆ†ï¼ˆ$Z(x)$ å½’ä¸€åŒ–ï¼‰
- **DPO** åƒéšä¾¿çš„æœ‹å‹ï¼šç›´æ¥è¯´"A æ¯” B å¥½"ï¼Œä½†æ²¡è€ƒè™‘ä½ å¯¹ B çš„åå¥½å¯èƒ½åŸºäºä¼ªæ•°æ®
:::

### 4. Length Biasï¼ˆé•¿åº¦åå·®ï¼‰

DPO å¯èƒ½å­˜åœ¨å¯¹è¾ƒçŸ­åºåˆ—çš„éšæ€§åå¥½ï¼š

$$\log \frac{\pi_\theta(y_w|x)}{\pi_{ref}(y_w|x)} - \log \frac{\pi_\theta(y_l|x)}{\pi_{ref}(y_l|x)} \approx \text{Length}(y_w) - \text{Length}(y_l)$$

**è§£å†³æ–¹æ¡ˆ**ï¼šå¼•å…¥é•¿åº¦æ­£åˆ™åŒ–é¡¹

$$\mathcal{L}_{DPO}^{length} = \mathcal{L}_{DPO}(\pi_\theta) + \lambda \mathbb{E}_{(x, y_w, y_l) \sim D}\left[\text{Length}(y_w) - \text{Length}(y_l)\right]$$

### ç»“è®º

| ç»´åº¦ | DPO | PPO |
|------|-----|-----|
| **ç®€åŒ–ç¨‹åº¦** | âœ… æ— éœ€å¥–åŠ±æ¨¡å‹ | âŒ éœ€è¦ 4 ä¸ªæ¨¡å‹ |
| **åˆ†å¸ƒé²æ£’æ€§** | âŒ ä¾èµ– $\pi_{ref}$ | âœ… KL æ­£åˆ™åŒ– |
| **Reward Hacking** | âŒ é£é™©è¾ƒé«˜ | âœ… æ˜¾å¼çº¦æŸ |
| **é•¿åº¦åå·®** | âŒ éœ€é¢å¤–å¤„ç† | âœ… è‡ªç„¶å¹³è¡¡ |
| **å·¥ä¸šåº”ç”¨** | å­¦æœ¯å®éªŒä¸ºä¸» | ChatGPTã€Claude ç­‰ |

**ç»“è®º**ï¼šDPO ä¸èƒ½å®Œå…¨å–ä»£ PPOï¼Œè‡³å°‘ç›®å‰è¿˜ä¸èƒ½ã€‚

---

## ğŸ”§ DPOå®ç°

### ä½¿ç”¨TRLåº“

```python
from trl import DPOTrainer, DPOConfig
from transformers import AutoModelForCausalLM, AutoTokenizer
from datasets import load_dataset

# 1. åŠ è½½æ¨¡å‹
model = AutoModelForCausalLM.from_pretrained("sft_model")
ref_model = AutoModelForCausalLM.from_pretrained("sft_model")
tokenizer = AutoTokenizer.from_pretrained("sft_model")

# 2. å‡†å¤‡åå¥½æ•°æ®é›†
# æ ¼å¼: {"prompt": "...", "chosen": "å¥½å›ç­”", "rejected": "å·®å›ç­”"}
dataset = load_dataset("json", data_files="preference_data.json")

# 3. DPOé…ç½®
dpo_config = DPOConfig(
    output_dir="./dpo_output",
    beta=0.1,                          # æ¸©åº¦å‚æ•°
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    learning_rate=5e-7,                # DPOé€šå¸¸ç”¨è¾ƒä½å­¦ä¹ ç‡
    num_train_epochs=1,
    warmup_ratio=0.1,
    logging_steps=10,
    save_strategy="epoch",
    bf16=True,
)

# 4. åˆ›å»ºDPOè®­ç»ƒå™¨
trainer = DPOTrainer(
    model=model,
    ref_model=ref_model,
    args=dpo_config,
    train_dataset=dataset["train"],
    tokenizer=tokenizer,
)

# 5. å¼€å§‹è®­ç»ƒ
trainer.train()
```

### æ•°æ®æ ¼å¼

```json
{
  "prompt": "è¯·è§£é‡Šä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½",
  "chosen": "äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œè‡´åŠ›äºåˆ›å»ºèƒ½å¤Ÿæ¨¡æ‹Ÿäººç±»æ™ºèƒ½çš„ç³»ç»Ÿ...",
  "rejected": "AIå°±æ˜¯æœºå™¨äººå•Š"
}
```

### ç»“åˆLoRA

```python
from peft import LoraConfig, get_peft_model

# LoRAé…ç½®
lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=["q_proj", "v_proj", "k_proj", "o_proj"],
    lora_dropout=0.05,
)

# åº”ç”¨LoRA
model = get_peft_model(model, lora_config)

# DPOè®­ç»ƒï¼ˆä½¿ç”¨LoRAï¼‰
trainer = DPOTrainer(
    model=model,
    ref_model=None,  # ä½¿ç”¨LoRAæ—¶å¯ä»¥ä¸éœ€è¦æ˜¾å¼å‚è€ƒæ¨¡å‹
    args=dpo_config,
    train_dataset=dataset,
    tokenizer=tokenizer,
    peft_config=lora_config,
)
```

---

## âš™ï¸ å…³é”®è¶…å‚æ•°

| å‚æ•° | æ¨èå€¼ | è¯´æ˜ |
|------|--------|------|
| **beta** | 0.1 ~ 0.5 | æ¸©åº¦å‚æ•°ï¼Œæ§åˆ¶åç¦»å‚è€ƒæ¨¡å‹çš„ç¨‹åº¦ |
| **learning_rate** | 1e-7 ~ 5e-6 | å­¦ä¹ ç‡ï¼Œæ¯”SFTä½å¾ˆå¤š |
| **epochs** | 1-3 | è®­ç»ƒè½®æ¬¡ |
| **max_length** | 512-1024 | æœ€å¤§åºåˆ—é•¿åº¦ |
| **max_prompt_length** | 128-256 | æœ€å¤§æç¤ºé•¿åº¦ |

### Betaå‚æ•°å½±å“

| betaå€¼ | æ•ˆæœ |
|--------|------|
| å° (0.01-0.1) | æ›´å¼ºçš„åå¥½å­¦ä¹ ï¼Œå¯èƒ½åç¦»å‚è€ƒæ¨¡å‹è¾ƒè¿œ |
| ä¸­ (0.1-0.3) | å¹³è¡¡ï¼ˆæ¨èï¼‰ |
| å¤§ (0.5-1.0) | æ›´ä¿å®ˆï¼Œæ¥è¿‘å‚è€ƒæ¨¡å‹ |

---

## ğŸ“Š DPOå˜ä½“

![Iterative-DPOæµç¨‹](https://pic3.zhimg.com/v2-d5bf8d5dbb07200a39df63b5762b27f0_r.jpg)
*Iterative-DPO æµç¨‹*

### Iterative-DPOï¼ˆè¿­ä»£å¼DPOï¼‰

2024å¹´ Meta æå‡ºçš„æ”¹è¿›ç‰ˆï¼ˆ[Iterative Reasoning Preference Optimization](https://arxiv.org/pdf/2404.19733)ï¼‰ï¼Œä»‹äº Online å’Œ Offline ä¹‹é—´ï¼š

1. è®­ç»ƒ Reward Model
2. å°†æ•°æ®åˆ†æˆ m ä»½
3. å¯¹æ¯ä»½æ•°æ®ï¼šç”¨å½“å‰ LLM é‡‡æ · k ä¸ªå›ç­” â†’ RM æ‰“åˆ† â†’ é€‰æœ€é«˜/æœ€ä½æ„å»º pair å¯¹ â†’ è®­ç»ƒä¸€è½® DPO â†’ æ›´æ–° LLM
4. é‡å¤ç›´åˆ°æ‰€æœ‰æ•°æ®è®­ç»ƒå®Œæˆ

**ä¼˜åŠ¿**ï¼šæ¯è½®è®­ç»ƒååŸºäºæœ€æ–°æ¨¡å‹é‡æ–°é‡‡æ ·ï¼Œç¼“è§£ DPO çš„åˆ†å¸ƒåç§»é—®é¢˜ã€‚

### ORPO (Odds Ratio Preference Optimization)

æ— éœ€å‚è€ƒæ¨¡å‹çš„å¯¹é½æ–¹æ³•ï¼š

```python
from trl import ORPOTrainer, ORPOConfig

orpo_config = ORPOConfig(
    output_dir="./orpo_output",
    beta=0.1,
    # ... å…¶ä»–å‚æ•°
)

trainer = ORPOTrainer(
    model=model,
    # æ³¨æ„ï¼šæ— éœ€ref_model
    args=orpo_config,
    train_dataset=dataset,
    tokenizer=tokenizer,
)
```

### IPO (Identity Preference Optimization)

```python
# IPOä½¿ç”¨ä¸åŒçš„æŸå¤±å‡½æ•°
dpo_config = DPOConfig(
    loss_type="ipo",  # ä½¿ç”¨IPOæŸå¤±
    # ...
)
```

### æ–¹æ³•å¯¹æ¯”

| æ–¹æ³• | éœ€è¦å‚è€ƒæ¨¡å‹ | å¤æ‚åº¦ | æ•ˆæœ |
|------|-------------|--------|------|
| **DPO** | âœ… æ˜¯ | ä¸­ | å¾ˆå¥½ |
| **ORPO** | âŒ å¦ | ä½ | è‰¯å¥½ |
| **IPO** | âœ… æ˜¯ | ä¸­ | å¾ˆå¥½ |
| **KTO** | âŒ å¦ | ä½ | è‰¯å¥½ |

---

## ğŸ¯ æœ€ä½³å®è·µ

### æ•°æ®è´¨é‡

```python
def validate_preference_data(sample):
    """éªŒè¯åå¥½æ•°æ®è´¨é‡"""
    # 1. chosenå’Œrejectedä¸èƒ½ç›¸åŒ
    if sample["chosen"] == sample["rejected"]:
        return False
    
    # 2. å“åº”ä¸èƒ½è¿‡çŸ­
    if len(sample["chosen"]) < 50 or len(sample["rejected"]) < 20:
        return False
    
    # 3. å“åº”éœ€è¦æœ‰å®è´¨å·®å¼‚
    from difflib import SequenceMatcher
    similarity = SequenceMatcher(None, sample["chosen"], sample["rejected"]).ratio()
    if similarity > 0.9:
        return False
    
    return True
```

### è®­ç»ƒç›‘æ§

```python
# å…³æ³¨çš„å…³é”®æŒ‡æ ‡
# 1. rewards/chosen - chosenå“åº”çš„éšå¼å¥–åŠ±
# 2. rewards/rejected - rejectedå“åº”çš„éšå¼å¥–åŠ±
# 3. rewards/margins - ä¸¤è€…å·®è·ï¼ˆåº”è¯¥å¢åŠ ï¼‰
# 4. logps/chosen - chosençš„å¯¹æ•°æ¦‚ç‡
# 5. logps/rejected - rejectedçš„å¯¹æ•°æ¦‚ç‡
```

---

## ğŸ”— ç›¸å…³é˜…è¯»

- [è®­ç»ƒå¾®è°ƒæ¦‚è¿°](/llms/training/) - äº†è§£å®Œæ•´è®­ç»ƒæµç¨‹
- [RLHFå¯¹é½](/llms/training/rlhf) - ä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ å¯¹é½
- [SFTç›‘ç£å¾®è°ƒ](/llms/training/sft) - DPOçš„å‰ç½®æ­¥éª¤

> **ç›¸å…³æ–‡ç« **ï¼š
> - [å¼ºåŒ–å­¦ä¹ å¯¹é½æŒ‡å—ï¼šPPOå’ŒDPOå®æ–½ä¸è¯„ä¼°](https://dd-ff.blog.csdn.net/article/details/153184150)
> - [è¯­è¨€æ¨¡å‹å¯¹é½æŠ€æœ¯è®ºè¿°ï¼šä»PPOåˆ°DPO](https://dd-ff.blog.csdn.net/article/details/153269912)

> **å¤–éƒ¨èµ„æº**ï¼š
> - [DPOåŸå§‹è®ºæ–‡](https://arxiv.org/abs/2305.18290)
> - [Hugging Face TRL DPO](https://huggingface.co/docs/trl/dpo_trainer)
> - [ORPOè®ºæ–‡](https://arxiv.org/abs/2403.07691)

---
title: DPO ç›´æ¥åå¥½ä¼˜åŒ–
description: Direct Preference Optimization - æ— éœ€å¥–åŠ±æ¨¡å‹çš„ç®€åŒ–å¯¹é½
---

# DPO ç›´æ¥åå¥½ä¼˜åŒ–

> ç”¨ç›‘ç£å­¦ä¹ çš„æ–¹å¼åšå¼ºåŒ–å­¦ä¹ çš„äº‹

## ğŸ¯ æ ¸å¿ƒæ¦‚å¿µ

> æ¥æºï¼š[å¼ºåŒ–å­¦ä¹ å¯¹é½æŒ‡å—ï¼šPPOå’ŒDPOå®æ–½ä¸è¯„ä¼°](https://dd-ff.blog.csdn.net/article/details/153184150)

### ä»€ä¹ˆæ˜¯DPOï¼Ÿ

::: tip å®šä¹‰
**DPOï¼ˆDirect Preference Optimizationï¼‰** æ˜¯ä¸€ç§ç›´æ¥åœ¨åå¥½æ•°æ®ä¸Šä¼˜åŒ–è¯­è¨€æ¨¡å‹çš„æ–¹æ³•ï¼Œæ— éœ€è®­ç»ƒå¥–åŠ±æ¨¡å‹ï¼Œå°†RLHFç®€åŒ–ä¸ºç±»ä¼¼ç›‘ç£å­¦ä¹ çš„è¿‡ç¨‹ã€‚
:::

### DPO vs RLHF

| ç‰¹æ€§ | RLHF (PPO) | DPO |
|------|------------|-----|
| **æ¨¡å‹æ•°é‡** | 4ä¸ªï¼ˆç­–ç•¥+ä»·å€¼+å¥–åŠ±+å‚è€ƒï¼‰ | 2ä¸ªï¼ˆç­–ç•¥+å‚è€ƒï¼‰ |
| **è®­ç»ƒå¤æ‚åº¦** | é«˜ï¼ˆå¼ºåŒ–å­¦ä¹ ï¼‰ | ä½ï¼ˆç›‘ç£å­¦ä¹ ï¼‰ |
| **ç¨³å®šæ€§** | éœ€è¦ç²¾ç»†è°ƒå‚ | ç›¸å¯¹ç¨³å®š |
| **è®¡ç®—æˆæœ¬** | é«˜ | ä¸­ç­‰ |
| **æ•ˆæœ** | æœ€ä½³ | æ¥è¿‘RLHF |

---

## ğŸ”¬ DPOåŸç†

### æ ¸å¿ƒæ€æƒ³

DPOçš„å…³é”®æ´å¯Ÿï¼š**å¥–åŠ±å‡½æ•°å¯ä»¥ç”¨ç­–ç•¥æ¨¡å‹å’Œå‚è€ƒæ¨¡å‹çš„å¯¹æ•°æ¦‚ç‡å·®æ¥è¡¨ç¤º**

```
r(x, y) = Î² * log[Ï€(y|x) / Ï€_ref(y|x)] + Î² * log Z(x)
```

å› æ­¤å¯ä»¥è·³è¿‡å¥–åŠ±æ¨¡å‹è®­ç»ƒï¼Œç›´æ¥ä¼˜åŒ–åå¥½ï¼š

```
L_DPO = -log Ïƒ(Î² * [log Ï€(y_w|x)/Ï€_ref(y_w|x) - log Ï€(y_l|x)/Ï€_ref(y_l|x)])

å…¶ä¸­ï¼š
- y_w: åå¥½çš„ï¼ˆchosenï¼‰å“åº”
- y_l: ä¸åå¥½çš„ï¼ˆrejectedï¼‰å“åº”
- Î²: æ¸©åº¦å‚æ•°
- Ïƒ: sigmoidå‡½æ•°
```

### ç›´è§‚ç†è§£

```
DPOç›®æ ‡ï¼š
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  å¢åŠ  chosen å“åº”çš„æ¦‚ç‡              â”‚
  â”‚  é™ä½ rejected å“åº”çš„æ¦‚ç‡            â”‚
  â”‚  åŒæ—¶ä¸è¦åç¦»å‚è€ƒæ¨¡å‹å¤ªè¿œ             â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”§ DPOå®ç°

### ä½¿ç”¨TRLåº“

```python
from trl import DPOTrainer, DPOConfig
from transformers import AutoModelForCausalLM, AutoTokenizer
from datasets import load_dataset

# 1. åŠ è½½æ¨¡å‹
model = AutoModelForCausalLM.from_pretrained("sft_model")
ref_model = AutoModelForCausalLM.from_pretrained("sft_model")
tokenizer = AutoTokenizer.from_pretrained("sft_model")

# 2. å‡†å¤‡åå¥½æ•°æ®é›†
# æ ¼å¼: {"prompt": "...", "chosen": "å¥½å›ç­”", "rejected": "å·®å›ç­”"}
dataset = load_dataset("json", data_files="preference_data.json")

# 3. DPOé…ç½®
dpo_config = DPOConfig(
    output_dir="./dpo_output",
    beta=0.1,                          # æ¸©åº¦å‚æ•°
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    learning_rate=5e-7,                # DPOé€šå¸¸ç”¨è¾ƒä½å­¦ä¹ ç‡
    num_train_epochs=1,
    warmup_ratio=0.1,
    logging_steps=10,
    save_strategy="epoch",
    bf16=True,
)

# 4. åˆ›å»ºDPOè®­ç»ƒå™¨
trainer = DPOTrainer(
    model=model,
    ref_model=ref_model,
    args=dpo_config,
    train_dataset=dataset["train"],
    tokenizer=tokenizer,
)

# 5. å¼€å§‹è®­ç»ƒ
trainer.train()
```

### æ•°æ®æ ¼å¼

```json
{
  "prompt": "è¯·è§£é‡Šä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½",
  "chosen": "äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œè‡´åŠ›äºåˆ›å»ºèƒ½å¤Ÿæ¨¡æ‹Ÿäººç±»æ™ºèƒ½çš„ç³»ç»Ÿ...",
  "rejected": "AIå°±æ˜¯æœºå™¨äººå•Š"
}
```

### ç»“åˆLoRA

```python
from peft import LoraConfig, get_peft_model

# LoRAé…ç½®
lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=["q_proj", "v_proj", "k_proj", "o_proj"],
    lora_dropout=0.05,
)

# åº”ç”¨LoRA
model = get_peft_model(model, lora_config)

# DPOè®­ç»ƒï¼ˆä½¿ç”¨LoRAï¼‰
trainer = DPOTrainer(
    model=model,
    ref_model=None,  # ä½¿ç”¨LoRAæ—¶å¯ä»¥ä¸éœ€è¦æ˜¾å¼å‚è€ƒæ¨¡å‹
    args=dpo_config,
    train_dataset=dataset,
    tokenizer=tokenizer,
    peft_config=lora_config,
)
```

---

## âš™ï¸ å…³é”®è¶…å‚æ•°

| å‚æ•° | æ¨èå€¼ | è¯´æ˜ |
|------|--------|------|
| **beta** | 0.1 ~ 0.5 | æ¸©åº¦å‚æ•°ï¼Œæ§åˆ¶åç¦»å‚è€ƒæ¨¡å‹çš„ç¨‹åº¦ |
| **learning_rate** | 1e-7 ~ 5e-6 | å­¦ä¹ ç‡ï¼Œæ¯”SFTä½å¾ˆå¤š |
| **epochs** | 1-3 | è®­ç»ƒè½®æ¬¡ |
| **max_length** | 512-1024 | æœ€å¤§åºåˆ—é•¿åº¦ |
| **max_prompt_length** | 128-256 | æœ€å¤§æç¤ºé•¿åº¦ |

### Betaå‚æ•°å½±å“

| betaå€¼ | æ•ˆæœ |
|--------|------|
| å° (0.01-0.1) | æ›´å¼ºçš„åå¥½å­¦ä¹ ï¼Œå¯èƒ½åç¦»å‚è€ƒæ¨¡å‹è¾ƒè¿œ |
| ä¸­ (0.1-0.3) | å¹³è¡¡ï¼ˆæ¨èï¼‰ |
| å¤§ (0.5-1.0) | æ›´ä¿å®ˆï¼Œæ¥è¿‘å‚è€ƒæ¨¡å‹ |

---

## ğŸ“Š DPOå˜ä½“

### ORPO (Odds Ratio Preference Optimization)

æ— éœ€å‚è€ƒæ¨¡å‹çš„å¯¹é½æ–¹æ³•ï¼š

```python
from trl import ORPOTrainer, ORPOConfig

orpo_config = ORPOConfig(
    output_dir="./orpo_output",
    beta=0.1,
    # ... å…¶ä»–å‚æ•°
)

trainer = ORPOTrainer(
    model=model,
    # æ³¨æ„ï¼šæ— éœ€ref_model
    args=orpo_config,
    train_dataset=dataset,
    tokenizer=tokenizer,
)
```

### IPO (Identity Preference Optimization)

```python
# IPOä½¿ç”¨ä¸åŒçš„æŸå¤±å‡½æ•°
dpo_config = DPOConfig(
    loss_type="ipo",  # ä½¿ç”¨IPOæŸå¤±
    # ...
)
```

### æ–¹æ³•å¯¹æ¯”

| æ–¹æ³• | éœ€è¦å‚è€ƒæ¨¡å‹ | å¤æ‚åº¦ | æ•ˆæœ |
|------|-------------|--------|------|
| **DPO** | âœ… æ˜¯ | ä¸­ | å¾ˆå¥½ |
| **ORPO** | âŒ å¦ | ä½ | è‰¯å¥½ |
| **IPO** | âœ… æ˜¯ | ä¸­ | å¾ˆå¥½ |
| **KTO** | âŒ å¦ | ä½ | è‰¯å¥½ |

---

## ğŸ¯ æœ€ä½³å®è·µ

### æ•°æ®è´¨é‡

```python
def validate_preference_data(sample):
    """éªŒè¯åå¥½æ•°æ®è´¨é‡"""
    # 1. chosenå’Œrejectedä¸èƒ½ç›¸åŒ
    if sample["chosen"] == sample["rejected"]:
        return False
    
    # 2. å“åº”ä¸èƒ½è¿‡çŸ­
    if len(sample["chosen"]) < 50 or len(sample["rejected"]) < 20:
        return False
    
    # 3. å“åº”éœ€è¦æœ‰å®è´¨å·®å¼‚
    from difflib import SequenceMatcher
    similarity = SequenceMatcher(None, sample["chosen"], sample["rejected"]).ratio()
    if similarity > 0.9:
        return False
    
    return True
```

### è®­ç»ƒç›‘æ§

```python
# å…³æ³¨çš„å…³é”®æŒ‡æ ‡
# 1. rewards/chosen - chosenå“åº”çš„éšå¼å¥–åŠ±
# 2. rewards/rejected - rejectedå“åº”çš„éšå¼å¥–åŠ±
# 3. rewards/margins - ä¸¤è€…å·®è·ï¼ˆåº”è¯¥å¢åŠ ï¼‰
# 4. logps/chosen - chosençš„å¯¹æ•°æ¦‚ç‡
# 5. logps/rejected - rejectedçš„å¯¹æ•°æ¦‚ç‡
```

---

## ğŸ”— ç›¸å…³é˜…è¯»

- [è®­ç»ƒå¾®è°ƒæ¦‚è¿°](/training/) - äº†è§£å®Œæ•´è®­ç»ƒæµç¨‹
- [RLHFå¯¹é½](/training/rlhf) - ä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ å¯¹é½
- [SFTç›‘ç£å¾®è°ƒ](/training/sft) - DPOçš„å‰ç½®æ­¥éª¤

> **ç›¸å…³æ–‡ç« **ï¼š
> - [å¼ºåŒ–å­¦ä¹ å¯¹é½æŒ‡å—ï¼šPPOå’ŒDPOå®æ–½ä¸è¯„ä¼°](https://dd-ff.blog.csdn.net/article/details/153184150)
> - [è¯­è¨€æ¨¡å‹å¯¹é½æŠ€æœ¯è®ºè¿°ï¼šä»PPOåˆ°DPO](https://dd-ff.blog.csdn.net/article/details/153269912)

> **å¤–éƒ¨èµ„æº**ï¼š
> - [DPOåŸå§‹è®ºæ–‡](https://arxiv.org/abs/2305.18290)
> - [Hugging Face TRL DPO](https://huggingface.co/docs/trl/dpo_trainer)
> - [ORPOè®ºæ–‡](https://arxiv.org/abs/2403.07691)

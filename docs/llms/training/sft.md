---
title: SFT ç›‘ç£å¾®è°ƒ
description: Supervised Fine-Tuning - è®©æ¨¡å‹å­¦ä¼šéµå¾ªæŒ‡ä»¤
---

# SFT ç›‘ç£å¾®è°ƒ

> ä»çŸ¥è¯†å‚¨å¤‡åˆ°ä»»åŠ¡æ‰§è¡Œçš„å…³é”®ä¸€æ­¥

## ğŸ¯ æ ¸å¿ƒæ¦‚å¿µ

> æ¥æºï¼š[ä»â€œæ‰©å……ä¹¦åº“â€åˆ°â€œæ•™æˆæŠ€èƒ½â€](https://dd-ff.blog.csdn.net/article/details/152267590) | [RLHFä¹‹PPOã€DPOè¯¦è§£](https://www.zhihu.com/tardis/zm/art/717010380)

![LLMè®­ç»ƒä¸‰é˜¶æ®µ](https://pic3.zhimg.com/v2-3b375dd479626f33ebc50dd7cba374fc_r.jpg)
*LLM è®­ç»ƒæµç¨‹ï¼šé¢„è®­ç»ƒ â†’ SFT â†’ RLHF*

### ä»€ä¹ˆæ˜¯SFTï¼Ÿ

::: tip å®šä¹‰
**SFTï¼ˆSupervised Fine-Tuningï¼‰** æ˜¯åœ¨é¢„è®­ç»ƒæ¨¡å‹åŸºç¡€ä¸Šï¼Œä½¿ç”¨é«˜è´¨é‡ç»“æ„åŒ–æ ‡ç­¾æ•°æ®ï¼ˆæŒ‡ä»¤-è¾“å…¥-è¾“å‡ºå¯¹ï¼‰æ•™å¯¼æ¨¡å‹éµå¾ªç‰¹å®šä»»åŠ¡è¡Œä¸ºå’Œè¾“å‡ºæ ¼å¼çš„è®­ç»ƒæ–¹æ³•ã€‚
:::

**åº•å±‚åŸç†**ï¼šè®­ç»ƒæ¨¡å‹å°†è¾“å…¥ï¼ˆPrompt/Instructionï¼‰æ˜ å°„åˆ°æœŸæœ›è¾“å‡ºï¼ˆCompletionï¼‰ï¼Œå®ç°**è¡Œä¸ºå¯¹é½**ï¼šè®©æ¨¡å‹ä»â€œä»…ä¼šé¢„æµ‹ä¸‹ä¸€ä¸ªè¯â€çš„åŸºåº§æ¨¡å‹ï¼Œè½¬å˜ä¸ºâ€œèƒ½ç†è§£å¹¶æ‰§è¡ŒæŒ‡ä»¤â€çš„èŠå¤©åŠ©æ‰‹æˆ–ä»»åŠ¡è§£å†³è€…ã€‚

### é¢†åŸŸå¤§æ¨¡å‹å®šåˆ¶çš„ä¸¤å¤§ç›®æ ‡

| ç›®æ ‡ | è¯´æ˜ | å®ç°æ–¹å¼ |
|------|------|----------|
| **çŸ¥è¯†æ³¨å…¥** | ç¡®ä¿æ¨¡å‹æŒæ¡ä¸“ä¸šé¢†åŸŸçš„æœ¯è¯­å’Œäº‹å® | CPTï¼ˆæŒç»­é¢„è®­ç»ƒï¼‰ |
| **è¡Œä¸ºå¯¹é½** | æ•™ä¼šæ¨¡å‹æŒ‰ç…§ç”¨æˆ·ç‰¹å®šæŒ‡ä»¤å’Œæ ¼å¼è¾“å‡º | SFTï¼ˆç›‘ç£å¾®è°ƒï¼‰ |

### SFTçš„ä½œç”¨

| é˜¶æ®µ | æ¨¡å‹èƒ½åŠ› | è®­ç»ƒç›®æ ‡ |
|------|----------|----------|
| **é¢„è®­ç»ƒå** | çŸ¥è¯†å‚¨å¤‡ä¸°å¯Œï¼Œä½†ä¸ä¼šå¯¹è¯ | é¢„æµ‹ä¸‹ä¸€ä¸ª Token |
| **SFTå** | ç†è§£æŒ‡ä»¤ï¼ŒæŒ‰è¦æ±‚å›ç­” | ç”Ÿæˆç¬¦åˆæŒ‡ä»¤çš„å“åº” |
| **RLHFå** | è¾“å‡ºç¬¦åˆäººç±»åå¥½ | æœ€å¤§åŒ–å¥–åŠ±ä¿¡å· |

```
Base Model (ç»­å†™èƒ½åŠ›) â†’ SFT â†’ Instruct Model (æŒ‡ä»¤éµå¾ª) â†’ RLHF â†’ Chat Model (å¯¹é½)
```

### SFT vs CPT vs RAG

| ç­–ç•¥ | ç›®æ ‡ | æ•°æ®ç±»å‹ | æˆæœ¬ | æ ¸å¿ƒä¼˜åŠ¿ |
|------|------|----------|------|----------|
| **CPT** | æ³¨å…¥é¢†åŸŸçŸ¥è¯† | æµ·é‡éç»“æ„åŒ–æ–‡æœ¬ | é«˜ï¼ˆ7-100ä¸‡ç¾å…ƒï¼‰ | æ·±åº¦é€‚é…é¢†åŸŸè¯­è¨€ï¼Œä¿®å¤çŸ¥è¯†ç»“æ„æ€§ç¼ºé™· |
| **SFT** | æ•™æˆæŒ‡ä»¤éµå¾ªè¡Œä¸º | é«˜è´¨é‡ç»“æ„åŒ–é—®ç­”æ•°æ® | ä¸­ä½ï¼ˆ0.5-14ä¸‡ç¾å…ƒï¼‰ | ç²¾å‡†æ§åˆ¶è¾“å‡ºæ ¼å¼ã€é£æ ¼å’Œä»»åŠ¡è§£å†³èƒ½åŠ› |
| **RAG** | è®¿é—®å®æ—¶/ç§æœ‰çŸ¥è¯† | å¤–éƒ¨æ–‡æ¡£/çŸ¥è¯†åº“ | ä½ | çŸ¥è¯†å³æ—¶æ›´æ–°ï¼Œé«˜å¯è¿½æº¯æ€§ |

::: warning æˆæœ¬å¯¹æ¯”
- ä»å¤´è®­ç»ƒï¼š~7800ä¸‡ç¾å…ƒï¼ˆGPT-4ä¼°è®¡ï¼‰
- CPTï¼š7-100ä¸‡ç¾å…ƒ
- SFT + PEFTï¼šæˆæœ¬æœ€ä½ï¼Œä»·å€¼å®ç°æ—¶é—´æœ€çŸ­
:::

### SFT vs å¼ºåŒ–å­¦ä¹ 

æ ¹æ® OpenAI è”åˆåˆ›å§‹äºº John Schulman çš„æŠ¥å‘Šï¼ŒSFT å’Œå¼ºåŒ–å­¦ä¹ å„æœ‰ä¼˜åŠ¿ï¼š

| ç»´åº¦ | SFT | å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ |
|------|-----|---------------|
| **åé¦ˆç²’åº¦** | é’ˆå¯¹å•ä¸ª Token | é’ˆå¯¹æ•´ä½“è¾“å‡º |
| **è¡¨è¾¾å¤šæ ·æ€§** | å—é™äºæ ‡æ³¨æ•°æ® | å¯æ¢ç´¢å¤šç§è¡¨è¾¾ |
| **å¹»è§‰é—®é¢˜** | å®¹æ˜“äº§ç”Ÿå¹»è§‰ | å¯é€šè¿‡å¥–åŠ±å‡½æ•°ç¼“è§£ |
| **å¤šè½®å¯¹è¯** | éš¾ä»¥å»ºæ¨¡é•¿æœŸç›®æ ‡ | å¯ç´¯ç§¯å¥–åŠ±ä¼˜åŒ– |
| **è®­ç»ƒéš¾åº¦** | ç®€å•ï¼Œç±»ä¼¼ç›‘ç£å­¦ä¹  | å¤æ‚ï¼Œéœ€è¦è°ƒå‚ |

---

## ğŸ“Š SFTå››ç§æ¨¡å¼

SFTå¹¶éå•ä¸€æµç¨‹ï¼Œæ•°æ®é€‰æ‹©ä¸è®­ç»ƒç›®æ ‡å†³å®šæ¨¡å‹æœ€ç»ˆå½¢æ€ï¼š

### æ¨¡å¼å¯¹æ¯”

| æ¨¡å¼ | æ•°æ®æ¥æº | ç›®æ ‡ | ä¼˜ç‚¹ | ç¼ºç‚¹ |
|------|----------|------|------|------|
| **é€šç”¨SFT** | å¼€æºé€šç”¨æ•°æ®é›† | å»ºç«‹åŸºç¡€æŒ‡ä»¤éµå¾ªèƒ½åŠ› | å¹¿æ³›èƒ½åŠ›ã€å¤šä»»åŠ¡ | é¢†åŸŸè¡¨ç°ä¸€èˆ¬ |
| **é¢†åŸŸSFT** | é¢†åŸŸä¸“ç”¨æ•°æ® | é€‚é…é¢†åŸŸä»»åŠ¡éœ€æ±‚ | ä¸“ä¸šæ€§å¼º | å¯èƒ½é—å¿˜é€šç”¨èƒ½åŠ› |
| **æ··åˆSFT** | é€šç”¨+é¢†åŸŸæ··åˆ | å¹³è¡¡ä¸“ä¸šæ€§ä¸é€šç”¨æ€§ | CFé˜²å¾¡æ ‡å‡†å®è·µ | æ•°æ®é…æ¯”éœ€è°ƒä¼˜ |
| **æŒç»­SFT** | å¢é‡é¢†åŸŸæ•°æ® | é€‚åº”æ–°ä»»åŠ¡ | çµæ´»æ‰©å±• | éœ€è¦é˜²é—å¿˜ç­–ç•¥ |

### A. é€šç”¨SFTï¼ˆæŒ‡ä»¤å¾®è°ƒï¼‰

é€šç”¨ SFT é€šå¸¸è¢«ç§°ä¸º**æŒ‡ä»¤å¾®è°ƒï¼ˆInstruction Tuningï¼‰**ï¼Œæ˜¯ LLM åè®­ç»ƒçš„åˆå§‹é˜¶æ®µï¼š

- ç›®æ ‡ï¼šå»ºç«‹æ¨¡å‹åŸºç¡€æŒ‡ä»¤éµå¾ªèƒ½åŠ›å’Œå¤šä»»åŠ¡å¤„ç†èƒ½åŠ›
- æ•°æ®ï¼šè¦†ç›–ç¿»è¯‘ã€æ‘˜è¦ã€é—®ç­”ç­‰åœºæ™¯çš„å¹¿æ³›ã€å¤šæ ·åŒ–æŒ‡ä»¤æ•°æ®é›†
- ç»“æœï¼šæ¨¡å‹ä» Base Model è½¬åŒ–ä¸º **Instruct Model** æˆ– **Chat Model**

### B. é¢†åŸŸSFT

é¢†åŸŸ SFT æ˜¯â€œåœ¨ç‰¹å®šé¢†åŸŸæ•°æ®é›†ä¸Šè®­ç»ƒæ¨¡å‹ï¼Œé€‚é…é¢†åŸŸä»»åŠ¡éœ€æ±‚â€ï¼š

- æ•°æ®ï¼šé¢†åŸŸä¸“ä¸šæœ¯è¯­ã€ç‰¹å®šæ ¼å¼ã€å¤æ‚ä»»åŠ¡è§„åˆ™æ ‡æ³¨ï¼ˆå¦‚åŒ»ç–—æœ¬ä½“åº“ã€è¯ç‰©ç›¸äº’ä½œç”¨è§„åˆ™ï¼‰
- æŒ‘æˆ˜ï¼š**ç¾éš¾æ€§é—å¿˜**â€”â€”é¢†åŸŸæ•°æ®é€šå¸¸é«˜åº¦é›†ä¸­ã€èŒƒå›´ç‹­çª„

### C. æ··åˆSFTï¼ˆæ¨èï¼‰

::: tip æ ‡å‡†å®è·µ
æ··åˆ SFT å·²æˆä¸ºé¢†åŸŸå¾®è°ƒçš„**æ ‡å‡†å®è·µ**ï¼ˆéå¯é€‰æ¨¡å¼ï¼‰ï¼Œæœ¬è´¨æ˜¯åŸºäºâ€œå›æ”¾ï¼ˆRehearsalï¼‰â€çš„æ•°æ®çº§ä¿éšœæœºåˆ¶ã€‚
:::

**æ ¸å¿ƒåŸç†**ï¼šåœ¨é¢†åŸŸæ•°æ®é›†è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ··åˆé€šç”¨æŒ‡ä»¤æ•°æ®ï¼Œç¡®ä¿æ¨¡å‹å­¦ä¹ é¢†åŸŸæŠ€èƒ½æ—¶ï¼Œä¸é€šç”¨èƒ½åŠ›ç›¸å…³çš„æƒé‡ä¸è¢«å®Œå…¨å¤±æ´»ã€‚

### D. æ¨¡å‹èµ·ç‚¹é€‰æ‹©

SFT å‰çš„æ ¸å¿ƒå†³ç­–â€”â€”é€‰æ‹© **Base Model** è¿˜æ˜¯ **Instruct Model** ä½œä¸ºèµ·ç‚¹ï¼š

| æ¨¡å‹ç±»å‹ | æ ¸å¿ƒä¼˜åŠ¿ | é€‚ç”¨åœºæ™¯ |
|----------|----------|----------|
| **Base Model** | çµæ´»æ€§æœ€é«˜ï¼Œå¯å¼€å‘å…¨æ–°ä¸“ä¸šåŒ–å¯¹è¯æ ¼å¼ | éœ€é«˜åº¦å®šåˆ¶åŒ–è¾“å‡ºæ ¼å¼/ä»»åŠ¡ |
| **Instruct Model** | å·²å…·å¤‡å¯¹è¯ç»“æ„ã€â€œåŠ©æ‰‹â€äººè®¾ã€å¤šè½®ä¸Šä¸‹æ–‡ç†è§£ | é¢†åŸŸä¸“å®¶èŠå¤©æœºå™¨äººï¼Œéœ€è¿è´¯èŠå¤©ä½“éªŒ |

### ç¾éš¾æ€§é—å¿˜é—®é¢˜

::: danger æ ¸å¿ƒæŒ‘æˆ˜
é¢†åŸŸSFTå¯èƒ½å¯¼è‡´æ¨¡å‹"å¿˜è®°"é¢„è®­ç»ƒé˜¶æ®µå­¦åˆ°çš„é€šç”¨çŸ¥è¯†ï¼Œè¿™è¢«ç§°ä¸º**ç¾éš¾æ€§é—å¿˜**ã€‚
:::

**è§£å†³æ–¹æ¡ˆ**ï¼š

| æ–¹æ³• | åŸç† | æ•ˆæœ |
|------|------|------|
| **æ··åˆæ•°æ®** | æ··å…¥ 5-10% é€šç”¨æ•°æ® | ç®€å•æœ‰æ•ˆ |
| **EWC æ­£åˆ™åŒ–** | ä¿æŠ¤é‡è¦å‚æ•°ä¸å˜ | ç†è®ºæ‰å® |
| **LoRA å¾®è°ƒ** | ä»…æ›´æ–°å°‘é‡å‚æ•° | æ¨èæ–¹æ¡ˆ |
| **Replay æœºåˆ¶** | é‡æ”¾å†å²æ•°æ® | è®¡ç®—æˆæœ¬é«˜ |

### SFT æ•°æ®è´¨é‡è¦æ±‚

::: warning å…³é”®æ´å¯Ÿ
ç ”ç©¶è¡¨æ˜ï¼š**é«˜è´¨é‡çš„ 1,000 æ¡æ•°æ® > ä½è´¨é‡çš„ 100,000 æ¡æ•°æ®**
:::

| ç»´åº¦ | è¦æ±‚ |
|------|------|
| **å‡†ç¡®æ€§** | å“åº”å†…å®¹æ­£ç¡®æ— è¯¯ |
| **ç›¸å…³æ€§** | å“åº”åˆ‡é¢˜ï¼Œç¬¦åˆæŒ‡ä»¤ |
| **å¤šæ ·æ€§** | è¦†ç›–å¤šç§ä»»åŠ¡ç±»å‹å’Œè¡¨è¾¾æ–¹å¼ |
| **ä¸€è‡´æ€§** | é£æ ¼ã€æ ¼å¼ä¿æŒç»Ÿä¸€ |
| **å®‰å…¨æ€§** | ä¸å«æœ‰å®³ã€åè§å†…å®¹ |

---

## ğŸ”§ SFTå®ç°

### ä½¿ç”¨Transformers

```python
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer
)
from datasets import load_dataset

# 1. åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
model_name = "meta-llama/Llama-2-7b-hf"
model = AutoModelForCausalLM.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token

# 2. å‡†å¤‡æ•°æ®é›†
def format_instruction(sample):
    """æ ¼å¼åŒ–ä¸ºæŒ‡ä»¤æ ¼å¼"""
    return f"""### æŒ‡ä»¤:
{sample['instruction']}

### è¾“å…¥:
{sample.get('input', '')}

### å›ç­”:
{sample['output']}"""

def tokenize(sample):
    text = format_instruction(sample)
    return tokenizer(
        text,
        truncation=True,
        max_length=2048,
        padding="max_length"
    )

dataset = load_dataset("json", data_files="train.json")
tokenized_dataset = dataset.map(tokenize, remove_columns=dataset["train"].column_names)

# 3. è®­ç»ƒé…ç½®
training_args = TrainingArguments(
    output_dir="./sft_output",
    num_train_epochs=3,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=8,
    learning_rate=2e-5,
    warmup_ratio=0.1,
    logging_steps=10,
    save_strategy="epoch",
    fp16=True,
)

# 4. å¼€å§‹è®­ç»ƒ
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"],
)
trainer.train()
```

### ä½¿ç”¨TRL SFTTrainer

```python
from trl import SFTTrainer, SFTConfig
from transformers import AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-7b-hf")
tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-hf")

# SFTé…ç½®
sft_config = SFTConfig(
    output_dir="./sft_output",
    max_seq_length=2048,
    num_train_epochs=3,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=8,
    learning_rate=2e-5,
    packing=True,  # æ ·æœ¬æ‰“åŒ…ï¼Œæå‡æ•ˆç‡
)

# åˆ›å»ºè®­ç»ƒå™¨
trainer = SFTTrainer(
    model=model,
    args=sft_config,
    train_dataset=dataset,
    tokenizer=tokenizer,
    formatting_func=format_instruction,
)

trainer.train()
```

---

## ğŸ“ æ¨¡æ¿è®¾è®¡

### å¸¸è§æ¨¡æ¿æ ¼å¼

#### Alpacaæ¨¡æ¿
```
Below is an instruction that describes a task. Write a response that appropriately completes the request.

### Instruction:
{instruction}

### Input:
{input}

### Response:
{output}
```

#### ChatMLæ¨¡æ¿
```
<|im_start|>system
{system_message}<|im_end|>
<|im_start|>user
{user_message}<|im_end|>
<|im_start|>assistant
{assistant_message}<|im_end|>
```

#### Llama2æ¨¡æ¿
```
<s>[INST] <<SYS>>
{system_message}
<</SYS>>

{user_message} [/INST] {assistant_message} </s>
```

### æŸå¤±æ©ç 

::: warning é‡è¦
SFTè®­ç»ƒæ—¶ï¼Œåªåº”è¯¥è®¡ç®—**å“åº”éƒ¨åˆ†**çš„æŸå¤±ï¼Œä¸åº”è¯¥è®¡ç®—æŒ‡ä»¤/è¾“å…¥éƒ¨åˆ†çš„æŸå¤±ã€‚
:::

```python
def create_labels_with_mask(input_ids, response_start_idx):
    """åˆ›å»ºå¸¦æ©ç çš„æ ‡ç­¾"""
    labels = input_ids.clone()
    # å°†æŒ‡ä»¤éƒ¨åˆ†çš„æ ‡ç­¾è®¾ä¸º-100ï¼ˆå¿½ç•¥ï¼‰
    labels[:response_start_idx] = -100
    return labels
```

---

## âš™ï¸ è¶…å‚æ•°è°ƒä¼˜

### å…³é”®è¶…å‚æ•°

| å‚æ•° | æ¨èå€¼ | è¯´æ˜ |
|------|--------|------|
| **learning_rate** | 1e-5 ~ 5e-5 | å­¦ä¹ ç‡ï¼Œå¤ªé«˜æ˜“è¿‡æ‹Ÿåˆ |
| **batch_size** | æ ¹æ®æ˜¾å­˜è°ƒæ•´ | æœ‰æ•ˆbatch=batchÃ—gradient_accumulation |
| **epochs** | 1-3 | SFTé€šå¸¸ä¸éœ€è¦å¤ªå¤šè½®æ¬¡ |
| **warmup_ratio** | 0.03-0.1 | é¢„çƒ­æ¯”ä¾‹ |
| **max_seq_length** | 2048-4096 | æœ€å¤§åºåˆ—é•¿åº¦ |

### å­¦ä¹ ç‡è°ƒåº¦

```python
from transformers import get_cosine_schedule_with_warmup

# ä½™å¼¦é€€ç«è°ƒåº¦å™¨
scheduler = get_cosine_schedule_with_warmup(
    optimizer,
    num_warmup_steps=100,
    num_training_steps=1000
)
```

---

## ğŸ“ˆ è¯„ä¼°æ–¹æ³•

### è‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡

```python
from evaluate import load

# åŠ è½½è¯„ä¼°æŒ‡æ ‡
bleu = load("bleu")
rouge = load("rouge")

def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)
    
    # BLEUåˆ†æ•°
    bleu_score = bleu.compute(
        predictions=decoded_preds, 
        references=[[l] for l in decoded_labels]
    )
    
    # ROUGEåˆ†æ•°
    rouge_score = rouge.compute(
        predictions=decoded_preds, 
        references=decoded_labels
    )
    
    return {
        "bleu": bleu_score["bleu"],
        "rouge-l": rouge_score["rougeL"]
    }
```

### äººå·¥è¯„ä¼°ç»´åº¦

| ç»´åº¦ | è¯„ä¼°å†…å®¹ |
|------|----------|
| **ç›¸å…³æ€§** | å›ç­”æ˜¯å¦åˆ‡é¢˜ |
| **å‡†ç¡®æ€§** | å†…å®¹æ˜¯å¦æ­£ç¡® |
| **æµç•…æ€§** | è¡¨è¾¾æ˜¯å¦è‡ªç„¶ |
| **å®Œæ•´æ€§** | æ˜¯å¦è¦†ç›–è¦ç‚¹ |
| **å®‰å…¨æ€§** | æ˜¯å¦æœ‰å®³å†…å®¹ |

---

## ğŸ”— ç›¸å…³é˜…è¯»

- [è®­ç»ƒå¾®è°ƒæ¦‚è¿°](/llms/training/) - äº†è§£å®Œæ•´è®­ç»ƒæµç¨‹
- [æ•°æ®å¤„ç†](/llms/training/data) - å‡†å¤‡é«˜è´¨é‡è®­ç»ƒæ•°æ®
- [LoRAé«˜æ•ˆå¾®è°ƒ](/llms/training/lora) - ä½èµ„æºSFTæ–¹æ¡ˆ
- [RLHFå¯¹é½](/llms/training/rlhf) - SFTä¹‹åçš„å¯¹é½

> **ç›¸å…³æ–‡ç« **ï¼š
> - [ä»"æ‰©å……ä¹¦åº“"åˆ°"æ•™æˆæŠ€èƒ½"](https://dd-ff.blog.csdn.net/article/details/152267590)
> - [æ·±å…¥æ¢ç§˜LLMçš„"æš—è¯­"ï¼šç‰¹æ®ŠTokenä¸æ¨¡æ¿](https://dd-ff.blog.csdn.net/article/details/152328698)
> - [Genesis-LLMå…¨æµç¨‹å¼€æºé¡¹ç›®è§£æ](https://dd-ff.blog.csdn.net/article/details/155355144)

> **å¤–éƒ¨èµ„æº**ï¼š
> - [Hugging Face TRL](https://huggingface.co/docs/trl/sft_trainer)
> - [LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory)

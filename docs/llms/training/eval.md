---
title: æ¨¡åž‹è¯„ä¼°
description: LLMè¯„ä¼°æ–¹æ³•ã€æŒ‡æ ‡ä¸ŽåŸºå‡†æµ‹è¯•
---

# æ¨¡åž‹è¯„ä¼°

> ç§‘å­¦åº¦é‡æ¨¡åž‹èƒ½åŠ›ä¸Žè®­ç»ƒæ•ˆæžœ

## ðŸŽ¯ è¯„ä¼°ç»´åº¦

### è¯„ä¼°ç›®æ ‡

| ç»´åº¦ | è¯„ä¼°å†…å®¹ | å…³é”®æŒ‡æ ‡ |
|------|----------|----------|
| **è¯­è¨€èƒ½åŠ›** | ç†è§£ã€ç”Ÿæˆã€æŽ¨ç† | Perplexityã€BLEU |
| **çŸ¥è¯†æ°´å¹³** | äº‹å®žå‡†ç¡®æ€§ | åŸºå‡†æµ‹è¯•å¾—åˆ† |
| **æŒ‡ä»¤éµå¾ª** | æŒ‰è¦æ±‚å®Œæˆä»»åŠ¡ | ä»»åŠ¡å®ŒæˆçŽ‡ |
| **å®‰å…¨æ€§** | æœ‰å®³å†…å®¹ã€åè§ | å®‰å…¨è¯„åˆ† |
| **å®žç”¨æ€§** | äººç±»æ»¡æ„åº¦ | äººå·¥è¯„ä¼° |

---

## ðŸ“Š è‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡

### è¯­è¨€è´¨é‡

```python
from evaluate import load

# 1. Perplexity - å›°æƒ‘åº¦
perplexity = load("perplexity")
results = perplexity.compute(
    predictions=["ç”Ÿæˆçš„æ–‡æœ¬"],
    model_id="gpt2"
)
print(f"Perplexity: {results['mean_perplexity']}")

# 2. BLEU - æœºå™¨ç¿»è¯‘è´¨é‡
bleu = load("bleu")
results = bleu.compute(
    predictions=["ç”Ÿæˆçš„ç¿»è¯‘"],
    references=[["å‚è€ƒç¿»è¯‘1", "å‚è€ƒç¿»è¯‘2"]]
)
print(f"BLEU: {results['bleu']}")

# 3. ROUGE - æ‘˜è¦è´¨é‡
rouge = load("rouge")
results = rouge.compute(
    predictions=["ç”Ÿæˆçš„æ‘˜è¦"],
    references=["å‚è€ƒæ‘˜è¦"]
)
print(f"ROUGE-L: {results['rougeL']}")
```

### ç”Ÿæˆè´¨é‡

| æŒ‡æ ‡ | è¯„ä¼°å†…å®¹ | é€‚ç”¨åœºæ™¯ |
|------|----------|----------|
| **Perplexity** | è¯­è¨€æ¨¡åž‹å›°æƒ‘åº¦ | è¯­è¨€æµç•…æ€§ |
| **BLEU** | n-gramç²¾ç¡®åŒ¹é… | ç¿»è¯‘ã€é—®ç­” |
| **ROUGE** | å¬å›žçŽ‡å¯¼å‘ | æ‘˜è¦ |
| **BERTScore** | è¯­ä¹‰ç›¸ä¼¼åº¦ | é€šç”¨æ–‡æœ¬ç”Ÿæˆ |
| **METEOR** | è¯å¹²åŒ¹é…+åŒä¹‰è¯ | ç¿»è¯‘ |

```python
# BERTScore - è¯­ä¹‰ç›¸ä¼¼åº¦
from bert_score import score

P, R, F1 = score(
    cands=["ç”Ÿæˆçš„æ–‡æœ¬"],
    refs=["å‚è€ƒæ–‡æœ¬"],
    lang="zh"
)
print(f"BERTScore F1: {F1.mean()}")
```

---

## ðŸ† åŸºå‡†æµ‹è¯•

### ä¸­æ–‡åŸºå‡†

| åŸºå‡† | è¯„ä¼°å†…å®¹ | è¯´æ˜Ž |
|------|----------|------|
| **C-Eval** | ä¸­æ–‡ç»¼åˆèƒ½åŠ› | 52ä¸ªå­¦ç§‘ï¼Œ1.4ä¸‡é¢˜ |
| **CMMLU** | ä¸­æ–‡å¤šä»»åŠ¡ | 67ä¸ªä¸»é¢˜ |
| **GAOKAO** | é«˜è€ƒé¢˜ç›® | çœŸå®žè€ƒè¯•åœºæ™¯ |

### è‹±æ–‡åŸºå‡†

| åŸºå‡† | è¯„ä¼°å†…å®¹ | è¯´æ˜Ž |
|------|----------|------|
| **MMLU** | å¤šä»»åŠ¡ç†è§£ | 57ä¸ªå­¦ç§‘ |
| **HellaSwag** | å¸¸è¯†æŽ¨ç† | å®Œæˆå¥å­ |
| **ARC** | ç§‘å­¦æŽ¨ç† | å°å­¦ç§‘å­¦é¢˜ |
| **TruthfulQA** | çœŸå®žæ€§ | æŠ—å¹»è§‰èƒ½åŠ› |
| **GSM8K** | æ•°å­¦æŽ¨ç† | å°å­¦æ•°å­¦é¢˜ |

### ä½¿ç”¨lm-evaluation-harness

```bash
# å®‰è£…
pip install lm-eval

# è¿è¡Œè¯„ä¼°
lm_eval --model hf \
    --model_args pretrained=meta-llama/Llama-2-7b-hf \
    --tasks mmlu,hellaswag,arc_easy \
    --batch_size 8 \
    --output_path ./results
```

```python
# Python API
from lm_eval import evaluator

results = evaluator.simple_evaluate(
    model="hf",
    model_args="pretrained=meta-llama/Llama-2-7b-hf",
    tasks=["mmlu", "hellaswag"],
    batch_size=8
)

print(results["results"])
```

---

## ðŸ‘¥ äººå·¥è¯„ä¼°

### è¯„ä¼°ç»´åº¦

| ç»´åº¦ | è¯„åˆ†æ ‡å‡† |
|------|----------|
| **ç›¸å…³æ€§** | å›žç­”æ˜¯å¦åˆ‡é¢˜ï¼ˆ1-5åˆ†ï¼‰ |
| **å‡†ç¡®æ€§** | ä¿¡æ¯æ˜¯å¦æ­£ç¡®ï¼ˆ1-5åˆ†ï¼‰ |
| **æµç•…æ€§** | è¡¨è¾¾æ˜¯å¦è‡ªç„¶ï¼ˆ1-5åˆ†ï¼‰ |
| **æœ‰å¸®åŠ©æ€§** | æ˜¯å¦è§£å†³é—®é¢˜ï¼ˆ1-5åˆ†ï¼‰ |
| **å®‰å…¨æ€§** | æ˜¯å¦å­˜åœ¨æœ‰å®³å†…å®¹ï¼ˆæ˜¯/å¦ï¼‰ |

### å¯¹æ¯”è¯„ä¼°ï¼ˆA/B Testï¼‰

```python
def pairwise_comparison(response_a: str, response_b: str, prompt: str):
    """è®©è¯„ä¼°è€…é€‰æ‹©æ›´å¥½çš„å›žç­”"""
    evaluation_prompt = f"""
è¯·æ¯”è¾ƒä»¥ä¸‹ä¸¤ä¸ªå›žç­”ï¼Œé€‰æ‹©æ›´å¥½çš„ä¸€ä¸ªï¼š

é—®é¢˜ï¼š{prompt}

å›žç­”Aï¼š{response_a}

å›žç­”Bï¼š{response_b}

è¯·è¾“å‡º "A" æˆ– "B" æˆ– "å¹³æ‰‹"ï¼Œå¹¶è¯´æ˜Žç†ç”±ã€‚
"""
    return evaluation_prompt
```

### ä½¿ç”¨LLMä½œä¸ºè¯„ä¼°è€…

```python
def llm_as_judge(response: str, reference: str, criteria: str):
    """ä½¿ç”¨LLMè¯„ä¼°å“åº”è´¨é‡"""
    prompt = f"""è¯·æ ¹æ®ä»¥ä¸‹æ ‡å‡†è¯„ä¼°å›žç­”è´¨é‡ï¼Œç»™å‡º1-10åˆ†ï¼š

è¯„ä¼°æ ‡å‡†ï¼š{criteria}

å‚è€ƒç­”æ¡ˆï¼š{reference}

å¾…è¯„ä¼°å›žç­”ï¼š{response}

è¯·è¾“å‡ºï¼š
åˆ†æ•°ï¼š[1-10]
ç†ç”±ï¼š[ç®€è¦è¯´æ˜Ž]
"""
    return llm.generate(prompt)
```

---

## ðŸ“ˆ è®­ç»ƒç›‘æŽ§æŒ‡æ ‡

### è®­ç»ƒæ›²çº¿

```python
# å…³é”®ç›‘æŽ§æŒ‡æ ‡
training_metrics = {
    "train_loss": "è®­ç»ƒæŸå¤±ï¼Œåº”æŒç»­ä¸‹é™",
    "eval_loss": "éªŒè¯æŸå¤±ï¼Œå…³æ³¨æ˜¯å¦è¿‡æ‹Ÿåˆ",
    "learning_rate": "å­¦ä¹ çŽ‡å˜åŒ–",
    "grad_norm": "æ¢¯åº¦èŒƒæ•°ï¼Œæ£€æµ‹æ¢¯åº¦çˆ†ç‚¸",
}

# TensorBoardå¯è§†åŒ–
from torch.utils.tensorboard import SummaryWriter

writer = SummaryWriter("./logs")
writer.add_scalar("train/loss", loss, step)
writer.add_scalar("eval/loss", eval_loss, step)
```

### è¿‡æ‹Ÿåˆæ£€æµ‹

```
è®­ç»ƒæŸå¤±â†“ + éªŒè¯æŸå¤±â†“ = æ­£å¸¸å­¦ä¹ 
è®­ç»ƒæŸå¤±â†“ + éªŒè¯æŸå¤±â†‘ = è¿‡æ‹Ÿåˆï¼
è®­ç»ƒæŸå¤±â†’ + éªŒè¯æŸå¤±â†’ = å­¦ä¹ åœæ»ž
```

---

## ðŸ”„ è¯„ä¼°æµç¨‹

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  è¯„ä¼°æµç¨‹                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   è‡ªåŠ¨è¯„ä¼°       â”‚   åŸºå‡†æµ‹è¯•       â”‚   äººå·¥è¯„ä¼°       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  BLEU/ROUGE     â”‚  MMLU/C-Eval    â”‚  A/Bå¯¹æ¯”        â”‚
â”‚  BERTScore      â”‚  GSM8K          â”‚  å¤šç»´åº¦æ‰“åˆ†      â”‚
â”‚  Perplexity     â”‚  HellaSwag      â”‚  LLM-as-Judge   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ðŸ”— ç›¸å…³é˜…è¯»

- [è®­ç»ƒå¾®è°ƒæ¦‚è¿°](/llms/training/) - äº†è§£å®Œæ•´è®­ç»ƒæµç¨‹
- [Agentè¯„ä¼°](/llms/agent/evaluation) - Agentä¸“ç”¨è¯„ä¼°

> **å¤–éƒ¨èµ„æº**ï¼š
> - [lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness)
> - [OpenCompass](https://opencompass.org.cn/)
> - [Hugging Face Evaluate](https://huggingface.co/docs/evaluate)

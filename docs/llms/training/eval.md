---
title: æ¨¡åž‹è¯„ä¼°
description: LLMè¯„ä¼°æ–¹æ³•ã€æŒ‡æ ‡ä¸ŽåŸºå‡†æµ‹è¯•
---

# æ¨¡åž‹è¯„ä¼°

> ç§‘å­¦åº¦é‡æ¨¡åž‹èƒ½åŠ›ä¸Žè®­ç»ƒæ•ˆæžœ

## ðŸŽ¯ è¯„ä¼°ç»´åº¦

> æ¥æºï¼š[RLHFä¹‹PPOã€DPOè¯¦è§£](https://www.zhihu.com/tardis/zm/art/717010380)

![æ¨¡åž‹è¯„ä¼°æµç¨‹](https://pic1.zhimg.com/v2-8499b498b656207243ee53b6e297eeb8_r.jpg)
*LLM è®­ç»ƒä¸Žè¯„ä¼°æµç¨‹*

### è¯„ä¼°ç›®æ ‡

| ç»´åº¦ | è¯„ä¼°å†…å®¹ | å…³é”®æŒ‡æ ‡ |
|------|----------|----------|
| **è¯­è¨€èƒ½åŠ›** | ç†è§£ã€ç”Ÿæˆã€æŽ¨ç† | Perplexityã€BLEU |
| **çŸ¥è¯†æ°´å¹³** | äº‹å®žå‡†ç¡®æ€§ | åŸºå‡†æµ‹è¯•å¾—åˆ† |
| **æŒ‡ä»¤éµå¾ª** | æŒ‰è¦æ±‚å®Œæˆä»»åŠ¡ | ä»»åŠ¡å®ŒæˆçŽ‡ |
| **å®‰å…¨æ€§** | æœ‰å®³å†…å®¹ã€åè§ | å®‰å…¨è¯„åˆ† |
| **å®žç”¨æ€§** | äººç±»æ»¡æ„åº¦ | äººå·¥è¯„ä¼° |

### è¯„ä¼°æ—¶æœº

| é˜¶æ®µ | è¯„ä¼°é‡ç‚¹ | æ–¹æ³• |
|------|----------|------|
| **é¢„è®­ç»ƒ** | è¯­è¨€å»ºæ¨¡èƒ½åŠ› | Perplexityã€ä¸‹æ¸¸ä»»åŠ¡é›¶æ ·æœ¬ |
| **SFT åŽ** | æŒ‡ä»¤éµå¾ªèƒ½åŠ› | ä»»åŠ¡å®ŒæˆçŽ‡ã€BLEU/ROUGE |
| **RLHF åŽ** | äººç±»åå¥½å¯¹é½ | äººå·¥è¯„ä¼°ã€å¥–åŠ±æ¨¡åž‹å¾—åˆ† |
| **éƒ¨ç½²å‰** | å®‰å…¨æ€§ã€å¹»è§‰ | çº¢é˜Ÿæµ‹è¯•ã€TruthfulQA |

---

## ðŸ“Š è‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡

### è¯­è¨€è´¨é‡

```python
from evaluate import load

# 1. Perplexity - å›°æƒ‘åº¦
perplexity = load("perplexity")
results = perplexity.compute(
    predictions=["ç”Ÿæˆçš„æ–‡æœ¬"],
    model_id="gpt2"
)
print(f"Perplexity: {results['mean_perplexity']}")

# 2. BLEU - æœºå™¨ç¿»è¯‘è´¨é‡
bleu = load("bleu")
results = bleu.compute(
    predictions=["ç”Ÿæˆçš„ç¿»è¯‘"],
    references=[["å‚è€ƒç¿»è¯‘1", "å‚è€ƒç¿»è¯‘2"]]
)
print(f"BLEU: {results['bleu']}")

# 3. ROUGE - æ‘˜è¦è´¨é‡
rouge = load("rouge")
results = rouge.compute(
    predictions=["ç”Ÿæˆçš„æ‘˜è¦"],
    references=["å‚è€ƒæ‘˜è¦"]
)
print(f"ROUGE-L: {results['rougeL']}")
```

### ç”Ÿæˆè´¨é‡

| æŒ‡æ ‡ | è¯„ä¼°å†…å®¹ | é€‚ç”¨åœºæ™¯ |
|------|----------|----------|
| **Perplexity** | è¯­è¨€æ¨¡åž‹å›°æƒ‘åº¦ | è¯­è¨€æµç•…æ€§ |
| **BLEU** | n-gramç²¾ç¡®åŒ¹é… | ç¿»è¯‘ã€é—®ç­” |
| **ROUGE** | å¬å›žçŽ‡å¯¼å‘ | æ‘˜è¦ |
| **BERTScore** | è¯­ä¹‰ç›¸ä¼¼åº¦ | é€šç”¨æ–‡æœ¬ç”Ÿæˆ |
| **METEOR** | è¯å¹²åŒ¹é…+åŒä¹‰è¯ | ç¿»è¯‘ |

```python
# BERTScore - è¯­ä¹‰ç›¸ä¼¼åº¦
from bert_score import score

P, R, F1 = score(
    cands=["ç”Ÿæˆçš„æ–‡æœ¬"],
    refs=["å‚è€ƒæ–‡æœ¬"],
    lang="zh"
)
print(f"BERTScore F1: {F1.mean()}")
```

---

## ðŸ† åŸºå‡†æµ‹è¯•

### åŸºå‡†æµ‹è¯•åˆ†ç±»

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      LLM è¯„ä¼°åŸºå‡†                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   çŸ¥è¯†ç†è§£       â”‚   æŽ¨ç†èƒ½åŠ›       â”‚   å®‰å…¨ä¸Žå¯¹é½             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  MMLU/C-Eval    â”‚  GSM8K/MATH     â”‚  TruthfulQA             â”‚
â”‚  CMMLU/GAOKAO   â”‚  ARC/HellaSwag  â”‚  çº¢é˜Ÿæµ‹è¯•               â”‚
â”‚  TriviaQA       â”‚  BBH            â”‚  åè§æ£€æµ‹               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ä¸­æ–‡åŸºå‡†

| åŸºå‡† | è¯„ä¼°å†…å®¹ | è¯´æ˜Ž | éš¾åº¦ |
|------|----------|------|------|
| **C-Eval** | ä¸­æ–‡ç»¼åˆèƒ½åŠ› | 52 ä¸ªå­¦ç§‘ï¼Œ1.4 ä¸‡é¢˜ | ä¸­é«˜ |
| **CMMLU** | ä¸­æ–‡å¤šä»»åŠ¡ | 67 ä¸ªä¸»é¢˜ | ä¸­ |
| **GAOKAO** | é«˜è€ƒé¢˜ç›® | çœŸå®žè€ƒè¯•åœºæ™¯ | é«˜ |
| **AGIEval** | äººç±»è€ƒè¯• | æ³•è€ƒã€å…¬åŠ¡å‘˜ç­‰ | é«˜ |

### è‹±æ–‡åŸºå‡†

| åŸºå‡† | è¯„ä¼°å†…å®¹ | è¯´æ˜Ž | éš¾åº¦ |
|------|----------|------|------|
| **MMLU** | å¤šä»»åŠ¡ç†è§£ | 57 ä¸ªå­¦ç§‘ | ä¸­é«˜ |
| **HellaSwag** | å¸¸è¯†æŽ¨ç† | å®Œæˆå¥å­ | ä¸­ |
| **ARC** | ç§‘å­¦æŽ¨ç† | å°å­¦ç§‘å­¦é¢˜ | ä¸­ |
| **TruthfulQA** | çœŸå®žæ€§ | æŠ—å¹»è§‰èƒ½åŠ› | é«˜ |
| **GSM8K** | æ•°å­¦æŽ¨ç† | å°å­¦æ•°å­¦é¢˜ | ä¸­ |
| **HumanEval** | ä»£ç ç”Ÿæˆ | ç¼–ç¨‹èƒ½åŠ› | é«˜ |
| **MATH** | æ•°å­¦ç«žèµ› | é«˜éš¾åº¦æ•°å­¦ | æžé«˜ |

### ä½¿ç”¨lm-evaluation-harness

```bash
# å®‰è£…
pip install lm-eval

# è¿è¡Œè¯„ä¼°
lm_eval --model hf \
    --model_args pretrained=meta-llama/Llama-2-7b-hf \
    --tasks mmlu,hellaswag,arc_easy \
    --batch_size 8 \
    --output_path ./results
```

```python
# Python API
from lm_eval import evaluator

results = evaluator.simple_evaluate(
    model="hf",
    model_args="pretrained=meta-llama/Llama-2-7b-hf",
    tasks=["mmlu", "hellaswag"],
    batch_size=8
)

print(results["results"])
```

---

## ðŸ‘¥ äººå·¥è¯„ä¼°

### ä¸ºä»€ä¹ˆéœ€è¦äººå·¥è¯„ä¼°

| é—®é¢˜ | è¯´æ˜Ž |
|------|------|
| **è‡ªåŠ¨æŒ‡æ ‡å±€é™** | BLEU/ROUGE ä¸Žäººç±»åˆ¤æ–­ç›¸å…³æ€§æœ‰é™ |
| **ä¸»è§‚æ€§ä»»åŠ¡** | åˆ›æ„å†™ä½œã€å¯¹è¯è´¨é‡éš¾ä»¥é‡åŒ– |
| **å®‰å…¨æ€§æ£€æµ‹** | æœ‰å®³å†…å®¹éœ€äººå·¥åˆ¤æ–­ |
| **åå¥½å¯¹é½éªŒè¯** | RLHF æ•ˆæžœéœ€äººç±»åé¦ˆ |

### è¯„ä¼°ç»´åº¦

| ç»´åº¦ | è¯„åˆ†æ ‡å‡† | è¯´æ˜Ž |
|------|----------|------|
| **ç›¸å…³æ€§** | 1-5åˆ† | å›žç­”æ˜¯å¦åˆ‡é¢˜ |
| **å‡†ç¡®æ€§** | 1-5åˆ† | ä¿¡æ¯æ˜¯å¦æ­£ç¡® |
| **æµç•…æ€§** | 1-5åˆ† | è¡¨è¾¾æ˜¯å¦è‡ªç„¶ |
| **æœ‰å¸®åŠ©æ€§** | 1-5åˆ† | æ˜¯å¦è§£å†³é—®é¢˜ |
| **å®‰å…¨æ€§** | æ˜¯/å¦ | æ˜¯å¦å­˜åœ¨æœ‰å®³å†…å®¹ |
| **ä¸€è‡´æ€§** | 1-5åˆ† | å¤šè½®å¯¹è¯æ˜¯å¦è¿žè´¯ |

### å¯¹æ¯”è¯„ä¼°ï¼ˆA/B Testï¼‰

```python
def pairwise_comparison(response_a: str, response_b: str, prompt: str):
    """è®©è¯„ä¼°è€…é€‰æ‹©æ›´å¥½çš„å›žç­”"""
    evaluation_prompt = f"""
è¯·æ¯”è¾ƒä»¥ä¸‹ä¸¤ä¸ªå›žç­”ï¼Œé€‰æ‹©æ›´å¥½çš„ä¸€ä¸ªï¼š

é—®é¢˜ï¼š{prompt}

å›žç­”Aï¼š{response_a}

å›žç­”Bï¼š{response_b}

è¯·è¾“å‡º "A" æˆ– "B" æˆ– "å¹³æ‰‹"ï¼Œå¹¶è¯´æ˜Žç†ç”±ã€‚
"""
    return evaluation_prompt
```

### ä½¿ç”¨LLMä½œä¸ºè¯„ä¼°è€…

```python
def llm_as_judge(response: str, reference: str, criteria: str):
    """ä½¿ç”¨LLMè¯„ä¼°å“åº”è´¨é‡"""
    prompt = f"""è¯·æ ¹æ®ä»¥ä¸‹æ ‡å‡†è¯„ä¼°å›žç­”è´¨é‡ï¼Œç»™å‡º1-10åˆ†ï¼š

è¯„ä¼°æ ‡å‡†ï¼š{criteria}

å‚è€ƒç­”æ¡ˆï¼š{reference}

å¾…è¯„ä¼°å›žç­”ï¼š{response}

è¯·è¾“å‡ºï¼š
åˆ†æ•°ï¼š[1-10]
ç†ç”±ï¼š[ç®€è¦è¯´æ˜Ž]
"""
    return llm.generate(prompt)
```

---

## ðŸ“ˆ è®­ç»ƒç›‘æŽ§æŒ‡æ ‡

### è®­ç»ƒæ›²çº¿

```python
# å…³é”®ç›‘æŽ§æŒ‡æ ‡
training_metrics = {
    "train_loss": "è®­ç»ƒæŸå¤±ï¼Œåº”æŒç»­ä¸‹é™",
    "eval_loss": "éªŒè¯æŸå¤±ï¼Œå…³æ³¨æ˜¯å¦è¿‡æ‹Ÿåˆ",
    "learning_rate": "å­¦ä¹ çŽ‡å˜åŒ–",
    "grad_norm": "æ¢¯åº¦èŒƒæ•°ï¼Œæ£€æµ‹æ¢¯åº¦çˆ†ç‚¸",
    "reward": "RLHF é˜¶æ®µçš„å¥–åŠ±ä¿¡å·",
    "kl_divergence": "ç­–ç•¥ä¸Žå‚è€ƒæ¨¡åž‹çš„ KL æ•£åº¦",
}

# TensorBoardå¯è§†åŒ–
from torch.utils.tensorboard import SummaryWriter

writer = SummaryWriter("./logs")
writer.add_scalar("train/loss", loss, step)
writer.add_scalar("eval/loss", eval_loss, step)
```

### è¿‡æ‹Ÿåˆæ£€æµ‹

| è®­ç»ƒæŸå¤± | éªŒè¯æŸå¤± | çŠ¶æ€ | å»ºè®® |
|----------|----------|------|------|
| â†“ | â†“ | âœ… æ­£å¸¸å­¦ä¹  | ç»§ç»­è®­ç»ƒ |
| â†“ | â†‘ | âŒ è¿‡æ‹Ÿåˆ | æ—©åœã€æ­£åˆ™åŒ–ã€å‡å°‘ epoch |
| â†’ | â†’ | âš ï¸ å­¦ä¹ åœæ»ž | è°ƒå¤§å­¦ä¹ çŽ‡ã€æ£€æŸ¥æ•°æ® |
| â†‘ | â†‘ | âŒ è®­ç»ƒå¤±è´¥ | å‡å°å­¦ä¹ çŽ‡ã€æ£€æŸ¥æ¢¯åº¦ |

### SFT ç‰¹æœ‰ç›‘æŽ§

| æŒ‡æ ‡ | è¯´æ˜Ž | æ­£å¸¸èŒƒå›´ |
|------|------|----------|
| **Loss** | äº¤å‰ç†µæŸå¤± | æŒç»­ä¸‹é™ï¼Œæœ€ç»ˆ 1.5-2.5 |
| **Perplexity** | å›°æƒ‘åº¦ | 5-15ï¼ˆå–å†³äºŽä»»åŠ¡ï¼‰ |
| **Token Accuracy** | Token çº§å‡†ç¡®çŽ‡ | é€æ­¥æå‡ |
| **Generation Quality** | ç”Ÿæˆæ ·æœ¬è´¨é‡ | å®šæœŸäººå·¥æ£€æŸ¥ |

---

## ðŸ”„ è¯„ä¼°æµç¨‹

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  è¯„ä¼°æµç¨‹                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   è‡ªåŠ¨è¯„ä¼°       â”‚   åŸºå‡†æµ‹è¯•       â”‚   äººå·¥è¯„ä¼°       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  BLEU/ROUGE     â”‚  MMLU/C-Eval    â”‚  A/Bå¯¹æ¯”        â”‚
â”‚  BERTScore      â”‚  GSM8K          â”‚  å¤šç»´åº¦æ‰“åˆ†      â”‚
â”‚  Perplexity     â”‚  HellaSwag      â”‚  LLM-as-Judge   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ðŸ” LLM-as-Judge è¯¦è§£

### ä¸ºä»€ä¹ˆä½¿ç”¨ LLM ä½œä¸ºè¯„ä¼°è€…

| æ–¹æ³• | ä¼˜ç‚¹ | ç¼ºç‚¹ |
|------|------|------|
| **äººå·¥è¯„ä¼°** | æœ€å‡†ç¡® | æˆæœ¬é«˜ã€é€Ÿåº¦æ…¢ |
| **è‡ªåŠ¨æŒ‡æ ‡** | å¿«é€Ÿã€å¯å¤çŽ° | ä¸Žäººç±»åˆ¤æ–­ç›¸å…³æ€§ä½Ž |
| **LLM-as-Judge** | å¹³è¡¡æˆæœ¬ä¸Žè´¨é‡ | å¯èƒ½æœ‰åè§ |

### å¸¸è§è¯„ä¼°æ¨¡å¼

```python
# 1. å•ç‚¹è¯„åˆ†ï¼ˆPoint-wiseï¼‰
def pointwise_eval(response, criteria):
    prompt = f"""è¯·æ ¹æ®ä»¥ä¸‹æ ‡å‡†è¯„ä¼°å›žç­”ï¼Œç»™å‡º 1-10 åˆ†ï¼š
    
æ ‡å‡†ï¼š{criteria}
å›žç­”ï¼š{response}

è¾“å‡ºæ ¼å¼ï¼š
åˆ†æ•°ï¼š[1-10]
ç†ç”±ï¼š[ç®€è¦è¯´æ˜Ž]"""
    return llm.generate(prompt)

# 2. å¯¹æ¯”è¯„ä¼°ï¼ˆPairwiseï¼‰
def pairwise_eval(response_a, response_b, question):
    prompt = f"""æ¯”è¾ƒä»¥ä¸‹ä¸¤ä¸ªå›žç­”ï¼Œé€‰æ‹©æ›´å¥½çš„ä¸€ä¸ªï¼š
    
é—®é¢˜ï¼š{question}
å›žç­”Aï¼š{response_a}
å›žç­”Bï¼š{response_b}

è¾“å‡ºï¼šA æˆ– B æˆ– å¹³æ‰‹"""
    return llm.generate(prompt)

# 3. å‚è€ƒå¯¹æ¯”ï¼ˆReference-guidedï¼‰
def reference_eval(response, reference, criteria):
    prompt = f"""å‚è€ƒæ ‡å‡†ç­”æ¡ˆè¯„ä¼°å›žç­”è´¨é‡ï¼š
    
æ ‡å‡†ç­”æ¡ˆï¼š{reference}
å¾…è¯„ä¼°ï¼š{response}
è¯„ä¼°æ ‡å‡†ï¼š{criteria}"""
    return llm.generate(prompt)
```

---

## ðŸ”— ç›¸å…³é˜…è¯»

- [è®­ç»ƒå¾®è°ƒæ¦‚è¿°](/llms/training/) - äº†è§£å®Œæ•´è®­ç»ƒæµç¨‹
- [Agentè¯„ä¼°](/llms/agent/evaluation) - Agentä¸“ç”¨è¯„ä¼°

> **ç›¸å…³æ–‡ç« **ï¼š
> - [RLHFä¹‹PPOã€DPOè¯¦è§£](https://www.zhihu.com/tardis/zm/art/717010380)

> **å¤–éƒ¨èµ„æº**ï¼š
> - [lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness)
> - [OpenCompass](https://opencompass.org.cn/)
> - [Hugging Face Evaluate](https://huggingface.co/docs/evaluate)

---
title: éƒ¨ç½²ä¸ŽæŽ¨ç†ä¼˜åŒ–
description: æ¨¡åž‹åŽ‹ç¼©ã€é‡åŒ–ä¸Žé«˜æ•ˆæŽ¨ç†
---

# éƒ¨ç½²ä¸ŽæŽ¨ç†ä¼˜åŒ–

> è®©å¤§æ¨¡åž‹è·‘å¾—æ›´å¿«ã€æ›´çœ

## ðŸŽ¯ æ ¸å¿ƒæŒ‘æˆ˜

> æ¥æºï¼š[åŽ‹ç¼©å·¨å…½ï¼šæ·±å…¥æŽ¢ç©¶å¤§è¯­è¨€æ¨¡åž‹åŽ‹ç¼©çš„åº•å±‚ç§‘å­¦](https://dd-ff.blog.csdn.net/article/details/150932519)

### éƒ¨ç½²éš¾ç‚¹

| æŒ‘æˆ˜ | é—®é¢˜ | è§£å†³æ–¹æ¡ˆ |
|------|------|----------|
| **æ˜¾å­˜å ç”¨** | 7Bæ¨¡åž‹éœ€14GB+ | é‡åŒ–åŽ‹ç¼© |
| **æŽ¨ç†é€Ÿåº¦** | è‡ªå›žå½’ç”Ÿæˆæ…¢ | KV Cacheã€æ‰¹å¤„ç† |
| **æˆæœ¬** | GPUæ˜‚è´µ | CPUæŽ¨ç†ã€è¾¹ç¼˜éƒ¨ç½² |
| **å»¶è¿Ÿ** | é¦–Tokenæ—¶é—´é•¿ | æŽ¨æµ‹è§£ç  |

---

## ðŸ—œï¸ æ¨¡åž‹é‡åŒ–

### é‡åŒ–ç±»åž‹

| ç±»åž‹ | ç²¾åº¦ | æ˜¾å­˜èŠ‚çœ | ç²¾åº¦æŸå¤± |
|------|------|----------|----------|
| **FP32** | 32ä½ | åŸºå‡† | æ—  |
| **FP16/BF16** | 16ä½ | 50% | æžå° |
| **INT8** | 8ä½ | 75% | å° |
| **INT4** | 4ä½ | 87.5% | ä¸­ç­‰ |
| **INT2** | 2ä½ | 93.75% | è¾ƒå¤§ |

### é‡åŒ–æ–¹æ³•

| æ–¹æ³• | åŽŸç† | é€‚ç”¨åœºæ™¯ |
|------|------|----------|
| **PTQï¼ˆè®­ç»ƒåŽé‡åŒ–ï¼‰** | ç›´æŽ¥é‡åŒ–å·²è®­ç»ƒæ¨¡åž‹ | å¿«é€Ÿéƒ¨ç½² |
| **QATï¼ˆé‡åŒ–æ„ŸçŸ¥è®­ç»ƒï¼‰** | è®­ç»ƒæ—¶æ¨¡æ‹Ÿé‡åŒ– | è¿½æ±‚ç²¾åº¦ |
| **GPTQ** | åŸºäºŽHessiançš„é€å±‚é‡åŒ– | 4-bité«˜ç²¾åº¦ |
| **AWQ** | æ¿€æ´»æ„ŸçŸ¥é‡åŒ– | ä¿æŠ¤é‡è¦æƒé‡ |
| **GGUF** | llama.cppæ ¼å¼ | CPUæŽ¨ç† |

### BitsAndBytesé‡åŒ–

```python
from transformers import AutoModelForCausalLM, BitsAndBytesConfig

# 8-bité‡åŒ–
model_8bit = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-2-7b-hf",
    load_in_8bit=True,
    device_map="auto"
)

# 4-bité‡åŒ–ï¼ˆNF4ï¼‰
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True
)

model_4bit = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-2-7b-hf",
    quantization_config=bnb_config,
    device_map="auto"
)
```

### GPTQé‡åŒ–

```python
from transformers import AutoModelForCausalLM, GPTQConfig

gptq_config = GPTQConfig(
    bits=4,
    dataset="c4",
    tokenizer=tokenizer
)

model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-2-7b-hf",
    quantization_config=gptq_config,
    device_map="auto"
)
```

---

## ðŸš€ æŽ¨ç†ä¼˜åŒ–

### vLLMé«˜æ€§èƒ½æŽ¨ç†

```python
from vllm import LLM, SamplingParams

# åŠ è½½æ¨¡åž‹
llm = LLM(
    model="meta-llama/Llama-2-7b-chat-hf",
    tensor_parallel_size=1,  # GPUæ•°é‡
    gpu_memory_utilization=0.9
)

# é‡‡æ ·å‚æ•°
sampling_params = SamplingParams(
    temperature=0.7,
    top_p=0.9,
    max_tokens=512
)

# æ‰¹é‡æŽ¨ç†
prompts = ["ä½ å¥½", "ä»‹ç»ä¸€ä¸‹äººå·¥æ™ºèƒ½"]
outputs = llm.generate(prompts, sampling_params)

for output in outputs:
    print(output.outputs[0].text)
```

### vLLMæ ¸å¿ƒä¼˜åŒ–

| æŠ€æœ¯ | è¯´æ˜Ž |
|------|------|
| **PagedAttention** | ç±»ä¼¼è™šæ‹Ÿå†…å­˜ç®¡ç†KV Cache |
| **Continuous Batching** | åŠ¨æ€æ‰¹å¤„ç†ï¼Œæå‡åžå |
| **Tensor Parallelism** | å¤šGPUå¹¶è¡Œ |
| **Prefix Caching** | ç¼“å­˜å…±äº«å‰ç¼€ |

---

## ðŸ’» æœ¬åœ°éƒ¨ç½²

> æ¥æºï¼š[llama.cppå·¥ä½œæµä¸ŽGGUFè½¬æ¢æŒ‡å—](https://dd-ff.blog.csdn.net/article/details/154353525)

### llama.cppéƒ¨ç½²

```bash
# 1. å…‹éš†å¹¶ç¼–è¯‘
git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp
make

# 2. è½¬æ¢æ¨¡åž‹ä¸ºGGUFæ ¼å¼
python convert_hf_to_gguf.py /path/to/model --outfile model.gguf

# 3. é‡åŒ–
./llama-quantize model.gguf model-q4_k_m.gguf Q4_K_M

# 4. è¿è¡ŒæŽ¨ç†
./llama-cli -m model-q4_k_m.gguf -p "ä½ å¥½" -n 128
```

### GGUFé‡åŒ–çº§åˆ«

| é‡åŒ–ç±»åž‹ | å¤§å°(7B) | è´¨é‡ | æŽ¨è |
|----------|----------|------|------|
| **Q2_K** | ~2.5GB | è¾ƒå·® | æžé™åŽ‹ç¼© |
| **Q4_K_M** | ~4GB | è‰¯å¥½ | âœ… æŽ¨è |
| **Q5_K_M** | ~5GB | å¾ˆå¥½ | ç²¾åº¦ä¼˜å…ˆ |
| **Q8_0** | ~7GB | æœ€ä½³ | ä¸è¿½æ±‚åŽ‹ç¼© |

### Ollamaå¿«é€Ÿéƒ¨ç½²

```bash
# å®‰è£…Ollama
curl -fsSL https://ollama.com/install.sh | sh

# è¿è¡Œæ¨¡åž‹
ollama run llama2

# æˆ–ä½¿ç”¨è‡ªå®šä¹‰æ¨¡åž‹
ollama create mymodel -f Modelfile
ollama run mymodel
```

---

## âœ‚ï¸ æ¨¡åž‹å‰ªæž

### ç»“æž„åŒ–å‰ªæž

```python
import torch.nn.utils.prune as prune

def prune_model(model, amount=0.3):
    """å¯¹æ¨¡åž‹è¿›è¡Œå‰ªæž"""
    for name, module in model.named_modules():
        if isinstance(module, torch.nn.Linear):
            prune.l1_unstructured(module, name='weight', amount=amount)
            prune.remove(module, 'weight')
    return model
```

### çŸ¥è¯†è’¸é¦

```python
from transformers import DistilBertForSequenceClassification

# æ•™å¸ˆæ¨¡åž‹ï¼ˆå¤§æ¨¡åž‹ï¼‰
teacher = AutoModelForCausalLM.from_pretrained("large_model")

# å­¦ç”Ÿæ¨¡åž‹ï¼ˆå°æ¨¡åž‹ï¼‰
student = AutoModelForCausalLM.from_pretrained("small_model")

# è’¸é¦æŸå¤±
def distillation_loss(student_logits, teacher_logits, temperature=2.0):
    soft_targets = F.softmax(teacher_logits / temperature, dim=-1)
    soft_predictions = F.log_softmax(student_logits / temperature, dim=-1)
    return F.kl_div(soft_predictions, soft_targets, reduction='batchmean')
```

---

## ðŸ“Š æŽ¨ç†æ¡†æž¶å¯¹æ¯”

| æ¡†æž¶ | ç‰¹ç‚¹ | é€‚ç”¨åœºæ™¯ |
|------|------|----------|
| **vLLM** | PagedAttentionï¼Œé«˜åžå | ç”Ÿäº§æœåŠ¡ |
| **TGI** | HuggingFaceå®˜æ–¹ | ä¼ä¸šéƒ¨ç½² |
| **llama.cpp** | CPUæŽ¨ç†ï¼ŒGGUFæ ¼å¼ | æœ¬åœ°/è¾¹ç¼˜ |
| **Ollama** | å¼€ç®±å³ç”¨ | å¿«é€Ÿä½“éªŒ |
| **TensorRT-LLM** | NVIDIAä¼˜åŒ– | è¿½æ±‚æžè‡´æ€§èƒ½ |

---

## ðŸ”— ç›¸å…³é˜…è¯»

- [è®­ç»ƒå¾®è°ƒæ¦‚è¿°](/training/) - äº†è§£å®Œæ•´è®­ç»ƒæµç¨‹
- [LoRAé«˜æ•ˆå¾®è°ƒ](/training/lora) - QLoRAç»“åˆé‡åŒ–

> **ç›¸å…³æ–‡ç« **ï¼š
> - [åŽ‹ç¼©å·¨å…½ï¼šå¤§è¯­è¨€æ¨¡åž‹åŽ‹ç¼©çš„åº•å±‚ç§‘å­¦](https://dd-ff.blog.csdn.net/article/details/150932519)
> - [llama.cppå·¥ä½œæµä¸ŽGGUFè½¬æ¢æŒ‡å—](https://dd-ff.blog.csdn.net/article/details/154353525)

> **å¤–éƒ¨èµ„æº**ï¼š
> - [vLLMæ–‡æ¡£](https://docs.vllm.ai/)
> - [llama.cpp](https://github.com/ggerganov/llama.cpp)
> - [Ollama](https://ollama.com/)

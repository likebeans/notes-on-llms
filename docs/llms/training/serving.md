---
title: éƒ¨ç½²ä¸æ¨ç†ä¼˜åŒ–
description: æ¨¡å‹å‹ç¼©ã€é‡åŒ–ä¸é«˜æ•ˆæ¨ç†
---

# éƒ¨ç½²ä¸æ¨ç†ä¼˜åŒ–

> è®©å¤§æ¨¡å‹è·‘å¾—æ›´å¿«ã€æ›´çœ

## ğŸ¯ æ ¸å¿ƒæŒ‘æˆ˜

> æ¥æºï¼š[å‹ç¼©å·¨å…½ï¼šæ·±å…¥æ¢ç©¶å¤§è¯­è¨€æ¨¡å‹å‹ç¼©çš„åº•å±‚ç§‘å­¦](https://dd-ff.blog.csdn.net/article/details/150932519)

### éƒ¨ç½²éš¾ç‚¹

| æŒ‘æˆ˜ | é—®é¢˜ | è§£å†³æ–¹æ¡ˆ |
|------|------|----------|
| **æ˜¾å­˜å ç”¨** | 7Bæ¨¡å‹éœ€14GB+ | é‡åŒ–å‹ç¼© |
| **æ¨ç†é€Ÿåº¦** | è‡ªå›å½’ç”Ÿæˆæ…¢ | KV Cacheã€æ‰¹å¤„ç† |
| **æˆæœ¬** | GPUæ˜‚è´µ | CPUæ¨ç†ã€è¾¹ç¼˜éƒ¨ç½² |
| **å»¶è¿Ÿ** | é¦–Tokenæ—¶é—´é•¿ | æ¨æµ‹è§£ç  |

---

## ğŸ—œï¸ æ¨¡å‹é‡åŒ–

### é‡åŒ–ç±»å‹

| ç±»å‹ | ç²¾åº¦ | æ˜¾å­˜èŠ‚çœ | ç²¾åº¦æŸå¤± |
|------|------|----------|----------|
| **FP32** | 32ä½ | åŸºå‡† | æ—  |
| **FP16/BF16** | 16ä½ | 50% | æå° |
| **INT8** | 8ä½ | 75% | å° |
| **INT4** | 4ä½ | 87.5% | ä¸­ç­‰ |
| **INT2** | 2ä½ | 93.75% | è¾ƒå¤§ |

### é‡åŒ–æ–¹æ³•

| æ–¹æ³• | åŸç† | é€‚ç”¨åœºæ™¯ |
|------|------|----------|
| **PTQï¼ˆè®­ç»ƒåé‡åŒ–ï¼‰** | ç›´æ¥é‡åŒ–å·²è®­ç»ƒæ¨¡å‹ | å¿«é€Ÿéƒ¨ç½² |
| **QATï¼ˆé‡åŒ–æ„ŸçŸ¥è®­ç»ƒï¼‰** | è®­ç»ƒæ—¶æ¨¡æ‹Ÿé‡åŒ– | è¿½æ±‚ç²¾åº¦ |
| **GPTQ** | åŸºäºHessiançš„é€å±‚é‡åŒ– | 4-bité«˜ç²¾åº¦ |
| **AWQ** | æ¿€æ´»æ„ŸçŸ¥é‡åŒ– | ä¿æŠ¤é‡è¦æƒé‡ |
| **GGUF** | llama.cppæ ¼å¼ | CPUæ¨ç† |

### BitsAndBytesé‡åŒ–

```python
from transformers import AutoModelForCausalLM, BitsAndBytesConfig

# 8-bité‡åŒ–
model_8bit = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-2-7b-hf",
    load_in_8bit=True,
    device_map="auto"
)

# 4-bité‡åŒ–ï¼ˆNF4ï¼‰
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True
)

model_4bit = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-2-7b-hf",
    quantization_config=bnb_config,
    device_map="auto"
)
```

### GPTQé‡åŒ–

```python
from transformers import AutoModelForCausalLM, GPTQConfig

gptq_config = GPTQConfig(
    bits=4,
    dataset="c4",
    tokenizer=tokenizer
)

model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-2-7b-hf",
    quantization_config=gptq_config,
    device_map="auto"
)
```

---

## ğŸš€ æ¨ç†ä¼˜åŒ–

> æ¥æºï¼š[FlashAttention è¯¦è§£](https://zhuanlan.zhihu.com/p/676655352) | [FlashAttention V2](https://zhuanlan.zhihu.com/p/691067658) | [vLLM å®˜æ–¹åšå®¢](https://blog.vllm.ai/2023/06/20/vllm.html) | [PagedAttention è®ºæ–‡](https://arxiv.org/pdf/2309.06180)

### FlashAttention

![FlashAttentionåŸç†](https://pic2.zhimg.com/v2-4078b99c76f608b79da281d597e2f149_r.jpg)
*FlashAttention åˆ†å—è®¡ç®—åŸç†*

FlashAttention æ˜¯ä¸€ç§ IO æ„ŸçŸ¥çš„æ³¨æ„åŠ›è®¡ç®—æ–¹æ³•ï¼Œå·²å¹¿æ³›åº”ç”¨äº GPT-3/4ã€Llama2ã€Falcon2 ç­‰ LLMã€‚

**æ ¸å¿ƒæŠ€æœ¯**ï¼š

| æŠ€æœ¯ | è¯´æ˜ |
|------|------|
| **Tilingï¼ˆåˆ†å—ï¼‰** | å°† Qã€Kã€V åˆ†æˆå°å—æ”¾å…¥ SRAM |
| **Kernel Fusion** | å¤šä¸ªè®¡ç®—æ­¥éª¤åˆå¹¶ä¸ºå•ä¸€ CUDA kernel |
| **Recomputation** | åå‘ä¼ æ’­æ—¶é‡ç®—ä¸­é—´ç»“æœï¼Œç”¨è®¡ç®—æ¢å­˜å‚¨ |
| **Online Softmax** | åˆ†å—è®¡ç®— Softmaxï¼Œæ— éœ€å®Œæ•´æ³¨æ„åŠ›çŸ©é˜µ |

**æ•ˆæœ**ï¼š
- HBM è¯»å†™é‡ä» $O(N^2)$ é™åˆ° $O(N)$
- è®­ç»ƒ/æ¨ç†é€Ÿåº¦æå‡ **2-4Ã—**

### vLLMé«˜æ€§èƒ½æ¨ç†

![vLLMæ€§èƒ½å¯¹æ¯”](https://blog.vllm.ai/assets/figures/perf_a100_n1_light.png)
*vLLM ååé‡å¯¹æ¯”ï¼šA100 GPU*

![PagedAttentionåŸç†](https://blog.vllm.ai/assets/figures/annimation0.gif)
*PagedAttentionï¼šKV Cache åˆ†å—å­˜å‚¨*

```python
from vllm import LLM, SamplingParams

# åŠ è½½æ¨¡å‹
llm = LLM(
    model="meta-llama/Llama-2-7b-chat-hf",
    tensor_parallel_size=1,  # GPUæ•°é‡
    gpu_memory_utilization=0.9
)

# é‡‡æ ·å‚æ•°
sampling_params = SamplingParams(
    temperature=0.7,
    top_p=0.9,
    max_tokens=512
)

# æ‰¹é‡æ¨ç†
prompts = ["ä½ å¥½", "ä»‹ç»ä¸€ä¸‹äººå·¥æ™ºèƒ½"]
outputs = llm.generate(prompts, sampling_params)

for output in outputs:
    print(output.outputs[0].text)
```

### vLLMæ ¸å¿ƒä¼˜åŒ–

| æŠ€æœ¯ | è¯´æ˜ |
|------|------|
| **PagedAttention** | ç±»ä¼¼è™šæ‹Ÿå†…å­˜ç®¡ç†KV Cacheï¼Œæ˜¾å­˜æµªè´¹ < 4% |
| **Continuous Batching** | åŠ¨æ€æ‰¹å¤„ç†ï¼Œæå‡åå |
| **Tensor Parallelism** | å¤šGPUå¹¶è¡Œ |
| **Prefix Caching** | ç¼“å­˜å…±äº«å‰ç¼€ |
| **Copy-on-Write** | å¹¶è¡Œé‡‡æ ·å…±äº« Prompt KV Cache |

**æ€§èƒ½å¯¹æ¯”**ï¼š

| å¯¹æ¯” | ååé‡æå‡ |
|------|----------|
| vs HuggingFace Transformers | **14-24Ã—** |
| vs HuggingFace TGI | **2.2-3.5Ã—** |

**å®é™…éƒ¨ç½²æ•°æ®**ï¼ˆLMSYSï¼‰ï¼š
- æ—¥å‡å¤„ç† **3ä¸‡+** è¯·æ±‚ï¼Œå³°å€¼ **6ä¸‡**
- GPU ä½¿ç”¨é‡å‡å°‘ **50%**

---

## ğŸ’» æœ¬åœ°éƒ¨ç½²

> æ¥æºï¼š[llama.cppå·¥ä½œæµä¸GGUFè½¬æ¢æŒ‡å—](https://dd-ff.blog.csdn.net/article/details/154353525)

### llama.cppéƒ¨ç½²

```bash
# 1. å…‹éš†å¹¶ç¼–è¯‘
git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp
make

# 2. è½¬æ¢æ¨¡å‹ä¸ºGGUFæ ¼å¼
python convert_hf_to_gguf.py /path/to/model --outfile model.gguf

# 3. é‡åŒ–
./llama-quantize model.gguf model-q4_k_m.gguf Q4_K_M

# 4. è¿è¡Œæ¨ç†
./llama-cli -m model-q4_k_m.gguf -p "ä½ å¥½" -n 128
```

### GGUFé‡åŒ–çº§åˆ«

| é‡åŒ–ç±»å‹ | å¤§å°(7B) | è´¨é‡ | æ¨è |
|----------|----------|------|------|
| **Q2_K** | ~2.5GB | è¾ƒå·® | æé™å‹ç¼© |
| **Q4_K_M** | ~4GB | è‰¯å¥½ | âœ… æ¨è |
| **Q5_K_M** | ~5GB | å¾ˆå¥½ | ç²¾åº¦ä¼˜å…ˆ |
| **Q8_0** | ~7GB | æœ€ä½³ | ä¸è¿½æ±‚å‹ç¼© |

### Ollamaå¿«é€Ÿéƒ¨ç½²

```bash
# å®‰è£…Ollama
curl -fsSL https://ollama.com/install.sh | sh

# è¿è¡Œæ¨¡å‹
ollama run llama2

# æˆ–ä½¿ç”¨è‡ªå®šä¹‰æ¨¡å‹
ollama create mymodel -f Modelfile
ollama run mymodel
```

---

## âœ‚ï¸ æ¨¡å‹å‰ªæ

### ç»“æ„åŒ–å‰ªæ

```python
import torch.nn.utils.prune as prune

def prune_model(model, amount=0.3):
    """å¯¹æ¨¡å‹è¿›è¡Œå‰ªæ"""
    for name, module in model.named_modules():
        if isinstance(module, torch.nn.Linear):
            prune.l1_unstructured(module, name='weight', amount=amount)
            prune.remove(module, 'weight')
    return model
```

### çŸ¥è¯†è’¸é¦

```python
from transformers import DistilBertForSequenceClassification

# æ•™å¸ˆæ¨¡å‹ï¼ˆå¤§æ¨¡å‹ï¼‰
teacher = AutoModelForCausalLM.from_pretrained("large_model")

# å­¦ç”Ÿæ¨¡å‹ï¼ˆå°æ¨¡å‹ï¼‰
student = AutoModelForCausalLM.from_pretrained("small_model")

# è’¸é¦æŸå¤±
def distillation_loss(student_logits, teacher_logits, temperature=2.0):
    soft_targets = F.softmax(teacher_logits / temperature, dim=-1)
    soft_predictions = F.log_softmax(student_logits / temperature, dim=-1)
    return F.kl_div(soft_predictions, soft_targets, reduction='batchmean')
```

---

## ğŸ“Š æ¨ç†æ¡†æ¶å¯¹æ¯”

| æ¡†æ¶ | ç‰¹ç‚¹ | é€‚ç”¨åœºæ™¯ |
|------|------|----------|
| **vLLM** | PagedAttentionï¼Œé«˜åå | ç”Ÿäº§æœåŠ¡ |
| **TGI** | HuggingFaceå®˜æ–¹ | ä¼ä¸šéƒ¨ç½² |
| **llama.cpp** | CPUæ¨ç†ï¼ŒGGUFæ ¼å¼ | æœ¬åœ°/è¾¹ç¼˜ |
| **Ollama** | å¼€ç®±å³ç”¨ | å¿«é€Ÿä½“éªŒ |
| **TensorRT-LLM** | NVIDIAä¼˜åŒ– | è¿½æ±‚æè‡´æ€§èƒ½ |

---

## ğŸ”— ç›¸å…³é˜…è¯»

- [è®­ç»ƒå¾®è°ƒæ¦‚è¿°](/llms/training/) - äº†è§£å®Œæ•´è®­ç»ƒæµç¨‹
- [LoRAé«˜æ•ˆå¾®è°ƒ](/llms/training/lora) - QLoRAç»“åˆé‡åŒ–

> **ç›¸å…³æ–‡ç« **ï¼š
> - [å‹ç¼©å·¨å…½ï¼šå¤§è¯­è¨€æ¨¡å‹å‹ç¼©çš„åº•å±‚ç§‘å­¦](https://dd-ff.blog.csdn.net/article/details/150932519)
> - [llama.cppå·¥ä½œæµä¸GGUFè½¬æ¢æŒ‡å—](https://dd-ff.blog.csdn.net/article/details/154353525)

> **å¤–éƒ¨èµ„æº**ï¼š
> - [vLLMæ–‡æ¡£](https://docs.vllm.ai/)
> - [llama.cpp](https://github.com/ggerganov/llama.cpp)
> - [Ollama](https://ollama.com/)

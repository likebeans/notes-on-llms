---
title: è®­ç»ƒæ•°æ®å¤„ç†
description: é«˜è´¨é‡å¾®è°ƒæ•°æ®çš„å‡†å¤‡ä¸å¤„ç†
---

# è®­ç»ƒæ•°æ®å¤„ç†

> "åƒåœ¾è¿›ï¼Œåƒåœ¾å‡º"â€”â€”æ•°æ®è´¨é‡å†³å®šæ¨¡å‹ä¸Šé™

## ğŸ¯ æ ¸å¿ƒåŸåˆ™

> æ¥æºï¼š[åƒåœ¾è¿›ï¼Œåƒåœ¾å‡ºï¼šæ‰“é€ é«˜è´¨é‡LLMå¾®è°ƒæ•°æ®é›†çš„ç»ˆææŒ‡å—](https://dd-ff.blog.csdn.net/article/details/152254276)

::: tip å…³é”®æ´å¯Ÿ
å¾®è°ƒå¤§è¯­è¨€æ¨¡å‹çš„æ ¸å¿ƒåœ¨äº**é«˜è´¨é‡æ•°æ®**ã€‚ç ”ç©¶è¡¨æ˜ï¼š
- é«˜è´¨é‡çš„1,000æ¡æ•°æ® > ä½è´¨é‡çš„100,000æ¡æ•°æ®
- æ•°æ®å¤šæ ·æ€§æ¯”æ•°é‡æ›´é‡è¦
- æ··å…¥5-10%é€šç”¨æ•°æ®å¯é˜²æ­¢ç¾éš¾æ€§é—å¿˜
:::

---

## ğŸ“Š æ•°æ®æ ¼å¼

### ä¸»æµæ ¼å¼å¯¹æ¯”

| æ ¼å¼ | ç»“æ„ | é€‚ç”¨åœºæ™¯ | ç¤ºä¾‹ |
|------|------|----------|------|
| **Alpaca** | instruction/input/output | å•è½®æŒ‡ä»¤ä»»åŠ¡ | é—®ç­”ã€ç¿»è¯‘ |
| **ShareGPT** | conversationsæ•°ç»„ | å¤šè½®å¯¹è¯ | èŠå¤©æœºå™¨äºº |
| **OpenAI** | messagesæ•°ç»„ | é€šç”¨æ ¼å¼ | APIå¾®è°ƒ |

### Alpacaæ ¼å¼

```json
{
  "instruction": "å°†ä»¥ä¸‹è‹±æ–‡ç¿»è¯‘æˆä¸­æ–‡",
  "input": "Hello, how are you?",
  "output": "ä½ å¥½ï¼Œä½ å¥½å—ï¼Ÿ"
}
```

### ShareGPTæ ¼å¼

```json
{
  "conversations": [
    {"from": "human", "value": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±"},
    {"from": "gpt", "value": "ä½ å¥½ï¼æˆ‘æ˜¯ä¸€ä¸ªAIåŠ©æ‰‹..."},
    {"from": "human", "value": "ä½ èƒ½åšä»€ä¹ˆï¼Ÿ"},
    {"from": "gpt", "value": "æˆ‘å¯ä»¥å¸®åŠ©ä½ å›ç­”é—®é¢˜..."}
  ]
}
```

### OpenAIæ ¼å¼

```json
{
  "messages": [
    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„åŠ©æ‰‹"},
    {"role": "user", "content": "ä½ å¥½"},
    {"role": "assistant", "content": "ä½ å¥½ï¼æœ‰ä»€ä¹ˆå¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ"}
  ]
}
```

---

## ğŸ”§ æ•°æ®å¤„ç†æµç¨‹

```
åŸå§‹æ•°æ®
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  æ•°æ®æ¸…æ´—    â”‚ â†’ å»é™¤å™ªå£°ã€ä¿®å¤æ ¼å¼é”™è¯¯
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PIIè„±æ•    â”‚ â†’ åŒ¿ååŒ–ä¸ªäººéšç§ä¿¡æ¯
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  è´¨é‡è¿‡æ»¤    â”‚ â†’ è¿‡æ»¤ä½è´¨é‡æ ·æœ¬
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  æ ¼å¼è½¬æ¢    â”‚ â†’ è½¬ä¸ºç›®æ ‡è®­ç»ƒæ ¼å¼
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  æ•°æ®å¢å¼º    â”‚ â†’ æå‡å¤šæ ·æ€§
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
  è®­ç»ƒæ•°æ®é›†
```

### æ•°æ®æ¸…æ´—

```python
import re

def clean_text(text: str) -> str:
    """æ¸…æ´—æ–‡æœ¬æ•°æ®"""
    # 1. å»é™¤HTMLæ ‡ç­¾
    text = re.sub(r'<[^>]+>', '', text)
    
    # 2. è§„èŒƒåŒ–ç©ºç™½å­—ç¬¦
    text = re.sub(r'\s+', ' ', text)
    
    # 3. å»é™¤ç‰¹æ®Šæ§åˆ¶å­—ç¬¦
    text = ''.join(c for c in text if c.isprintable() or c in '\n\t')
    
    # 4. ä¿®å¤ç¼–ç é—®é¢˜
    text = text.encode('utf-8', errors='ignore').decode('utf-8')
    
    return text.strip()

def filter_low_quality(samples: list) -> list:
    """è¿‡æ»¤ä½è´¨é‡æ ·æœ¬"""
    filtered = []
    for sample in samples:
        # é•¿åº¦æ£€æŸ¥
        if len(sample.get('output', '')) < 10:
            continue
        # é‡å¤æ£€æŸ¥
        if sample.get('output', '') == sample.get('input', ''):
            continue
        # è¯­è¨€æ£€æŸ¥ï¼ˆå¯é€‰ï¼‰
        if not is_valid_language(sample.get('output', '')):
            continue
        filtered.append(sample)
    return filtered
```

### PIIè„±æ•

```python
import re

class PIIAnonymizer:
    """ä¸ªäººéšç§ä¿¡æ¯è„±æ•"""
    
    PATTERNS = {
        'email': r'\b[\w.-]+@[\w.-]+\.\w+\b',
        'phone': r'\b1[3-9]\d{9}\b',
        'id_card': r'\b\d{17}[\dXx]\b',
        'ip': r'\b\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}\b',
    }
    
    def anonymize(self, text: str) -> str:
        for pii_type, pattern in self.PATTERNS.items():
            text = re.sub(pattern, f'[{pii_type.upper()}]', text)
        return text
```

---

## ğŸ“ˆ æ•°æ®è´¨é‡è¯„ä¼°

### åµŒå…¥ç©ºé—´åˆ†æ

```python
from sentence_transformers import SentenceTransformer
from sklearn.cluster import KMeans
import numpy as np

def analyze_diversity(texts: list, n_clusters: int = 10):
    """åˆ†ææ•°æ®é›†å¤šæ ·æ€§"""
    # ç”ŸæˆåµŒå…¥
    model = SentenceTransformer('all-MiniLM-L6-v2')
    embeddings = model.encode(texts)
    
    # èšç±»åˆ†æ
    kmeans = KMeans(n_clusters=n_clusters)
    labels = kmeans.fit_predict(embeddings)
    
    # è®¡ç®—å¤šæ ·æ€§æŒ‡æ ‡
    cluster_sizes = np.bincount(labels)
    diversity_score = 1 - (cluster_sizes.std() / cluster_sizes.mean())
    
    return {
        "diversity_score": diversity_score,
        "cluster_distribution": cluster_sizes.tolist(),
        "largest_cluster_ratio": cluster_sizes.max() / len(texts)
    }
```

### è¶…çº§è¿‡æ»¤æŠ€æœ¯

```python
def super_filter(samples: list, threshold: float = 0.9) -> list:
    """è¶…çº§è¿‡æ»¤ï¼šå»é™¤é«˜åº¦ç›¸ä¼¼çš„æ ·æœ¬"""
    from sklearn.metrics.pairwise import cosine_similarity
    
    model = SentenceTransformer('all-MiniLM-L6-v2')
    texts = [s['instruction'] + s.get('input', '') for s in samples]
    embeddings = model.encode(texts)
    
    # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
    sim_matrix = cosine_similarity(embeddings)
    
    # å»é‡
    keep_indices = []
    for i in range(len(samples)):
        is_duplicate = False
        for j in keep_indices:
            if sim_matrix[i][j] > threshold:
                is_duplicate = True
                break
        if not is_duplicate:
            keep_indices.append(i)
    
    return [samples[i] for i in keep_indices]
```

---

## ğŸ—„ï¸ Parquetæ ¼å¼ä¼˜åŒ–

> æ¥æºï¼š[ParquetèŒƒå¼ï¼šå¤§è¯­è¨€æ¨¡å‹è®­ç»ƒæ•°æ®æ ¼å¼ä¼˜åŒ–](https://dd-ff.blog.csdn.net/article/details/154654277)

### ä¸ºä»€ä¹ˆä½¿ç”¨Parquetï¼Ÿ

| æŒ‡æ ‡ | CSV/JSON | Parquet | æå‡ |
|------|----------|---------|------|
| **å­˜å‚¨ç©ºé—´** | 100% | 13% | 87%â†“ |
| **æŸ¥è¯¢é€Ÿåº¦** | 1x | 34.8x | 34.8xâ†‘ |
| **æ•°æ®æ‰«æ** | 100% | 0.2% | 99.8%â†“ |

### è½¬æ¢ç¤ºä¾‹

```python
import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq

def json_to_parquet(json_path: str, parquet_path: str):
    """å°†JSONæ•°æ®è½¬æ¢ä¸ºParquetæ ¼å¼"""
    # è¯»å–JSON
    df = pd.read_json(json_path, lines=True)
    
    # è½¬æ¢ä¸ºParquet
    table = pa.Table.from_pandas(df)
    pq.write_table(
        table, 
        parquet_path,
        compression='snappy',  # å‹ç¼©ç®—æ³•
        row_group_size=10000   # è¡Œç»„å¤§å°
    )

def read_parquet_efficiently(parquet_path: str, columns: list = None):
    """é«˜æ•ˆè¯»å–Parquetï¼ˆåˆ—è£å‰ªï¼‰"""
    return pq.read_table(
        parquet_path,
        columns=columns  # åªè¯»å–éœ€è¦çš„åˆ—
    ).to_pandas()
```

---

## ğŸ·ï¸ ç‰¹æ®ŠTokenä¸æ¨¡æ¿

> æ¥æºï¼š[æ·±å…¥æ¢ç§˜LLMçš„"æš—è¯­"ï¼šç‰¹æ®ŠTokenä¸LlamaFactoryçš„æ¨¡æ¿é­”æ³•](https://dd-ff.blog.csdn.net/article/details/152328698)

::: danger å…³é”®è­¦å‘Š
**90%çš„å¾®è°ƒæ€§èƒ½ä¸‹é™**å¯å½’å› äºè®­ç»ƒä¸æ¨ç†é˜¶æ®µæ¨¡æ¿ç»“æ„ä¸ä¸€è‡´ï¼
:::

### å¸¸è§ç‰¹æ®ŠToken

| Token | ä½œç”¨ | ç¤ºä¾‹ |
|-------|------|------|
| `<s>` / `<bos>` | åºåˆ—å¼€å§‹ | æ ‡è®°è¾“å…¥èµ·ç‚¹ |
| `</s>` / `<eos>` | åºåˆ—ç»“æŸ | æ ‡è®°è¾“å‡ºç»ˆç‚¹ |
| `[INST]` / `[/INST]` | æŒ‡ä»¤è¾¹ç•Œ | Llamaæ ¼å¼ |
| `<|im_start|>` / `<|im_end|>` | æ¶ˆæ¯è¾¹ç•Œ | ChatMLæ ¼å¼ |

### ChatMLæ¨¡æ¿ç¤ºä¾‹

```
<|im_start|>system
ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„AIåŠ©æ‰‹ã€‚<|im_end|>
<|im_start|>user
ä½ å¥½<|im_end|>
<|im_start|>assistant
ä½ å¥½ï¼æœ‰ä»€ä¹ˆå¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ<|im_end|>
```

### æ¨¡æ¿åŒ¹é…æ£€æŸ¥

```python
def validate_template_consistency(train_template: str, infer_template: str) -> bool:
    """éªŒè¯è®­ç»ƒå’Œæ¨ç†æ¨¡æ¿ä¸€è‡´æ€§"""
    # æå–ç‰¹æ®ŠToken
    train_tokens = set(re.findall(r'<\|?\w+\|?>', train_template))
    infer_tokens = set(re.findall(r'<\|?\w+\|?>', infer_template))
    
    if train_tokens != infer_tokens:
        print(f"è­¦å‘Šï¼šTokenä¸ä¸€è‡´ï¼")
        print(f"è®­ç»ƒ: {train_tokens}")
        print(f"æ¨ç†: {infer_tokens}")
        return False
    return True
```

---

## ğŸ”— ç›¸å…³é˜…è¯»

- [è®­ç»ƒå¾®è°ƒæ¦‚è¿°](/training/) - äº†è§£å®Œæ•´è®­ç»ƒæµç¨‹
- [SFTç›‘ç£å¾®è°ƒ](/training/sft) - å¦‚ä½•ä½¿ç”¨å‡†å¤‡å¥½çš„æ•°æ®
- [LoRAé«˜æ•ˆå¾®è°ƒ](/training/lora) - ä½èµ„æºè®­ç»ƒæ–¹æ¡ˆ

> **ç›¸å…³æ–‡ç« **ï¼š
> - [åƒåœ¾è¿›ï¼Œåƒåœ¾å‡ºï¼šæ‰“é€ é«˜è´¨é‡å¾®è°ƒæ•°æ®é›†](https://dd-ff.blog.csdn.net/article/details/152254276)
> - [ParquetèŒƒå¼ï¼šè®­ç»ƒæ•°æ®æ ¼å¼ä¼˜åŒ–](https://dd-ff.blog.csdn.net/article/details/154654277)
> - [æ·±å…¥æ¢ç§˜LLMçš„"æš—è¯­"ï¼šç‰¹æ®ŠTokenä¸æ¨¡æ¿](https://dd-ff.blog.csdn.net/article/details/152328698)

> **å¤–éƒ¨èµ„æº**ï¼š
> - [Hugging Face Datasets](https://huggingface.co/docs/datasets)
> - [Apache Parquet](https://parquet.apache.org/)

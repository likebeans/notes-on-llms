---
title: æ¨¡æ€è¿æ¥å™¨
description: LLaVA çº¿æ€§æŠ•å½±ä¸ BLIP-2 Q-Former æ¶æ„è¯¦è§£
---

# æ¨¡æ€è¿æ¥å™¨ï¼šLLM ä¸è§†è§‰çš„æ¡¥æ¢

> è¿æ¥å™¨ï¼ˆConnector/Projectorï¼‰è´Ÿè´£å°†è§†è§‰ç¼–ç å™¨è¾“å‡ºçš„ç‰¹å¾é€‚é…åˆ?LLM çš„è¾“å…¥ç©ºé—´ï¼Œå…¶è®¾è®¡ç›´æ¥å½±å“æ¨¡å‹çš„å‚æ•°æ•ˆç‡å’Œè¯­ä¹‰ç†è§£æ·±åº¦ã€?

---

## æ¶æ„æ€»è§ˆ

```mermaid
flowchart LR
    subgraph è§†è§‰ç¼–ç 
        IMG[å›¾åƒ] --> VIT[Vision Encoder (ViT/CLIP)]
        VIT --> VF[è§†è§‰ç‰¹å¾ - NÃ—D]
    end
    
    subgraph è¿æ¥å™?
        VF --> CONN[Connector]
        CONN --> LF[LLM å…¼å®¹ç‰¹å¾ - MÃ—D']
    end
    
    subgraph è¯­è¨€æ¨¡å‹
        LF --> LLM[LLM Backbone]
        TXT[æ–‡æœ¬ Token] --> LLM
        LLM --> OUT[è¾“å‡º]
    end
```

---

## ä¸»æµæ–¹æ¡ˆå¯¹æ¯”

| ç‰¹æ€?| LLaVA (Linear) | BLIP-2 (Q-Former) | Flamingo (Perceiver) |
| :--- | :--- | :--- | :--- |
| **æ ¸å¿ƒæœºåˆ¶** | ä¸¤å±‚ MLP | Transformer æŸ¥è¯¢å™?| Cross-Attention |
| **è¾“å‡º Token æ•?* | å–å†³äº?Patch æ•?| å›ºå®šï¼ˆå¦‚ 32ï¼?| å›ºå®šï¼ˆå¦‚ 64ï¼?|
| **ä¿¡æ¯ä¿ç•™** | å®Œæ•´è§†è§‰ç»†èŠ‚ | å‹ç¼©æå–å…³é”®ç‰¹å¾ | é€‰æ‹©æ€§å‹ç¼?|
| **è®­ç»ƒå¤æ‚åº?* | ä½?| é«˜ï¼ˆä¸¤é˜¶æ®µï¼‰ | ä¸?|
| **LLM æ˜¯å¦å†»ç»“** | å¯é€?| é€šå¸¸å†»ç»“ | å†»ç»“ |
| **ä¼˜åŠ¿åœºæ™¯** | OCRã€ç»†ç²’åº¦ | é«˜æ•ˆæ¨ç† | å¤šå›¾äº¤ç»‡ |

---

## LLaVA çº¿æ€§æŠ•å½?

LLaVA é‡‡ç”¨æç®€è®¾è®¡å“²å­¦ï¼?*ç®€å•ä½†æœ‰æ•ˆ**ã€?

### æ¶æ„è®¾è®¡

```mermaid
flowchart LR
    V[CLIP ViT-L/14 è¾“å‡º - 576Ã—1024] --> L1[Linear - 1024â†?096]
    L1 --> ACT[GELU]
    ACT --> L2[Linear - 4096â†?096]
    L2 --> OUT[LLM è¾“å…¥ - 576Ã—4096]
```

### å®ç°ç»†èŠ‚

```python
class LLaVAProjector(nn.Module):
    def __init__(self, vision_dim=1024, llm_dim=4096):
        super().__init__()
        self.projector = nn.Sequential(
            nn.Linear(vision_dim, llm_dim),
            nn.GELU(),
            nn.Linear(llm_dim, llm_dim)
        )
    
    def forward(self, vision_features):
        # vision_features: [B, N, vision_dim]
        return self.projector(vision_features)
        # output: [B, N, llm_dim]
```

### ä¼˜åŠ¿ä¸ä»£ä»?

| ä¼˜åŠ¿ | ä»£ä»· |
| :--- | :--- |
| âœ?ä¿ç•™å®Œæ•´è§†è§‰ä¿¡æ¯ | â?Token æ•°é‡å¤šï¼ˆ576 ä¸ªï¼‰ |
| âœ?è®­ç»ƒç®€å•å¿«é€?| â?æ¨ç†æˆæœ¬é«?|
| âœ?OCR/ç»†èŠ‚ä»»åŠ¡è¡¨ç°å¥?| â?æ˜¾å­˜å ç”¨å¤?|
| âœ?å‚æ•°é‡æå°?| â?é•¿æ–‡æœ¬ä¸Šä¸‹æ–‡å—é™ |

### LLaVA è®­ç»ƒç­–ç•¥

#### æ•°æ®ç”Ÿæˆç­–ç•¥ï¼šåˆ©ç”?GPT-4 åˆæˆæŒ‡ä»¤

**æ ¸å¿ƒæ€æƒ³**ï¼šä½¿ç”?GPT-4ï¼ˆçº¯æ–‡æœ¬ï¼‰åŸºäº?COCO Caption å’?Bounding Box ä¿¡æ¯ç”Ÿæˆå¤æ‚å¤šè½®å¯¹è¯ã€?

**Prompt è®¾è®¡**ï¼?

```python
# ç¤ºä¾‹ Prompt
prompt = f"""
åŸºäºä»¥ä¸‹å›¾åƒæè¿°å’Œå¯¹è±¡ä½ç½®ä¿¡æ¯ï¼Œç”Ÿæˆä¸‰ç§ç±»å‹çš„å¤šè½®å¯¹è¯ï¼š

å›¾åƒæè¿°ï¼š{coco_caption}
å¯¹è±¡ä½ç½®ï¼?
- person: [x1, y1, x2, y2]
- bicycle: [x1, y1, x2, y2]

è¯·ç”Ÿæˆï¼š
1. è¯¦ç»†æè¿°ï¼ˆDetailed Descriptionï¼‰ï¼šå¯¹å›¾åƒè¿›è¡Œå…¨é¢æè¿?
2. æ¨ç†é—®ç­”ï¼ˆReasoning QAï¼‰ï¼šåŸºäºå›¾åƒå†…å®¹çš„æ¨ç†é—®é¢?
3. å¤æ‚å¯¹è¯ï¼ˆComplex Conversationï¼‰ï¼šå¤šè½®äº¤äº’å¼å¯¹è¯?
"""
```

**ç”Ÿæˆç¤ºä¾‹**ï¼?

```json
{
  "image": "COCO_val2014_000000001234.jpg",
  "conversations": [
    {
      "from": "human",
      "value": "<image>\nè¯·è¯¦ç»†æè¿°è¿™å¼ å›¾ç‰‡ã€?
    },
    {
      "from": "gpt",
      "value": "å›¾ç‰‡ä¸­å±•ç¤ºäº†ä¸€ä¸ªäººéª‘ç€è‡ªè¡Œè½¦åœ¨å…¬å›­è·¯ä¸Šã€‚è¿™ä¸ªäººç©¿ç€è“è‰²çš„è¿åŠ¨æœï¼Œæˆ´ç€å¤´ç›”ï¼Œçœ‹èµ·æ¥éå¸¸ä¸“ä¸šã€‚èƒŒæ™¯æ˜¯ç»©ä¸½çš„å…¬å›­æ™¯è‰²ï¼Œæœ‰ç»¿æ ‘å’Œè‰åœ°ã€?
    }
  ]
}
```

**æ•°æ®è§„æ¨¡**ï¼?

- **Stage 1**ï¼?58K CC3M å›¾æ–‡å¯¹ï¼ˆç®€å?Captionï¼?
- **Stage 2**ï¼?65K å¤šæ¨¡æ€æŒ‡ä»¤æ•°æ?
  - 158K GPT-4 ç”Ÿæˆçš„å¯¹è¯?
  - 507K å…¶ä»–ä»»åŠ¡æ•°æ®

#### ä¸¤é˜¶æ®µè®­ç»ƒè¯¦ç»†å‚æ•?

| é˜¶æ®µ | æ•°æ® | å†»ç»“æ¨¡å— | è®­ç»ƒæ¨¡å— | Epoch | å­¦ä¹ ç?| Batch Size |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| **Stage 1** | 558K CC3M | ViT + LLM | Projector | 1 | 1e-3 | 256 |
| **Stage 2** | 665KæŒ‡ä»¤ | ViT | Projector + LLM | 3 | 2e-5 | 128 |

**è®­ç»ƒæ—¶é—´**ï¼?

- Stage 1ï¼šçº¦ 5 å°æ—¶ï¼?Ã—A100 80Gï¼?
- Stage 2ï¼šçº¦ 20 å°æ—¶ï¼?Ã—A100 80Gï¼?

| é˜¶æ®µ | æ•°æ® | è®­ç»ƒæ¨¡å— | ç›®çš„ |
| :--- | :--- | :--- | :--- |
| **Stage 1** | 558K å›¾æ–‡å¯?| ä»?Projector | ç‰¹å¾å¯¹é½ |
| **Stage 2** | 665K æŒ‡ä»¤æ•°æ® | Projector + LLM | æŒ‡ä»¤å¾®è°ƒ |

---

## BLIP-2 Q-Former

BLIP-2 å¼•å…¥ **Q-Formerï¼ˆQuerying Transformerï¼?* ä½œä¸ºè§†è§‰ä¸è¯­è¨€çš„ç“¶é¢ˆå±‚ã€?

### æ¶æ„è®¾è®¡

```mermaid
flowchart TB
    subgraph Q-Former
        Q[å¯å­¦ä¹?Queries - 32Ã—768] --> SA[Self-Attention]
        SA --> CA[Cross-Attention]
        VIT[å†»ç»“ ViT è¾“å‡º - 257Ã—1024] --> CA
        CA --> FFN[Feed Forward]
        FFN --> OUT[32 ä¸ªè§†è§?Token]
    end
    
    OUT --> LLM[å†»ç»“ LLM]
```

### æ ¸å¿ƒæœºåˆ¶

**å¯å­¦ä¹ æŸ¥è¯¢å‘é‡ï¼ˆLearnable Queriesï¼?*ï¼?

- åˆå§‹åŒ?32 ä¸ªæŸ¥è¯¢å‘é‡ï¼Œæ¯ä¸ªç»´åº¦ 768
- é€šè¿‡ Cross-Attention ä¸è§†è§‰ç‰¹å¾äº¤äº?
- å¼ºåˆ¶ä»æµ·é‡è§†è§‰ä¿¡æ¯ä¸­"æç‚¼"å…³é”®ç‰¹å¾

**åŒæµç»“æ„**ï¼?

- **å›¾åƒ Transformer**ï¼šä¸è§†è§‰ç‰¹å¾äº¤äº’
- **æ–‡æœ¬ Transformer**ï¼šä¸æ–‡æœ¬ç‰¹å¾äº¤äº’
- ä¸¤è€…å…±äº?Self-Attention å±?

### ä¸¤é˜¶æ®µé¢„è®­ç»ƒ

```mermaid
flowchart LR
    subgraph "Stage 1: è¡¨ç¤ºå­¦ä¹ "
        V1[è§†è§‰ç‰¹å¾] --> Q1[Q-Former]
        Q1 --> L1[ITC + ITM + ITG]
    end
    
    subgraph "Stage 2: ç”Ÿæˆå­¦ä¹ "
        V2[è§†è§‰ç‰¹å¾] --> Q2[Q-Former]
        Q2 --> LLM[å†»ç»“ LLM]
        LLM --> L2[Language Modeling]
    end
```

**Stage 1 æŸå¤±å‡½æ•°**ï¼?

- **ITC (Image-Text Contrastive)**ï¼šå¯¹æ¯”å­¦ä¹ å¯¹é½?
- **ITM (Image-Text Matching)**ï¼šäºŒåˆ†ç±»åŒ¹é…
- **ITG (Image-grounded Text Generation)**ï¼šå›¾åƒæ¡ä»¶æ–‡æœ¬ç”Ÿæˆ?

**Stage 2**ï¼?

- å°?Q-Former è¾“å‡ºä½œä¸º LLM çš„è½¯æç¤ºï¼ˆSoft Promptï¼?
- ä»…è®­ç»?Q-Formerï¼ŒLLM å®Œå…¨å†»ç»“

### ä¿¡æ¯å‹ç¼©åˆ†æ

| è¾“å…¥ | è¾“å‡º | å‹ç¼©ç?|
| :--- | :--- | :--- |
| ViT-L: 257Ã—1024 | 32Ã—768 | **~8Ã—** |
| ViT-G: 577Ã—1408 | 32Ã—768 | **~18Ã—** |

### Q-Former è®­ç»ƒè¯¦ç»†æµç¨‹

#### Stage 1ï¼šä¸‰åˆä¸€æŸå¤±å‡½æ•°

**ä»£ç å®ç°**ï¼?

```python
def stage1_training(image, text, qformer, vision_encoder):
    """
    BLIP-2 Stage 1: è§†è§‰-è¯­è¨€è¡¨å¾å­¦ä¹ 
    """
    # 1. Image-Text Contrastive (ITC) - å¯¹æ¯”å­¦ä¹ 
    with torch.no_grad():
        image_features = vision_encoder(image)  # å†»ç»“ViT
    
    # Q-Formerç¼–ç ï¼ˆä»…Self-Attentionï¼Œä¸ç”¨Cross-Attentionï¼?
    image_embeds = qformer.encode_image(image_features, mode='unimodal')
    text_embeds = qformer.encode_text(text, mode='unimodal')
    
    # å¯¹æ¯”æŸå¤±
    loss_itc = contrastive_loss(image_embeds, text_embeds)
    
    # 2. Image-Text Matching (ITM) - äºŒåˆ†ç±»åŒ¹é…?
    # éš¾è´Ÿæ ·æœ¬æŒ–æ˜ï¼šä»å¯¹æ¯”å­¦ä¹ ä¸­é€‰ç›¸ä¼¼ä½†ä¸åŒ¹é…çš„æ ·æœ¬
    with torch.no_grad():
        neg_indices = select_hard_negatives(image_embeds, text_embeds)
    
    # æ­£æ ·æœ?
    pos_score = qformer.match(image_features, text, label=1)
    # è´Ÿæ ·æœ?
    neg_score = qformer.match(image_features, text[neg_indices], label=0)
    
    loss_itm = binary_cross_entropy(pos_score, neg_score)
    
    # 3. Image-grounded Text Generation (ITG) - å›¾åƒæ¡ä»¶ç”Ÿæˆ
    # Q-Formerè¾“å‡ºä½œä¸ºPrefixes
    visual_prefix = qformer.encode_image(image_features, mode='multimodal')
    loss_itg = language_modeling_loss(visual_prefix, text)
    
    # æ€»æŸå¤?
    total_loss = loss_itc + loss_itm + loss_itg
    return total_loss
```

**éš¾è´Ÿæ ·æœ¬æŒ–æ˜ç­–ç•¥**ï¼?

- ä»?Batch å†…æ‰¾ä¸å½“å‰å›¾åƒç›¸ä¼¼åº¦æœ€é«˜çš„ k ä¸ªè´Ÿæ ·æœ¬
- è®©æ¨¡å‹å­¦ä¼šåŒºåˆ†ç»†å¾®å·®åˆ?

#### Stage 2ï¼šè½¯æç¤ºç”Ÿæˆ

```python
def stage2_training(image, text, qformer, vision_encoder, llm):
    """
    BLIP-2 Stage 2: è§†è§‰åˆ°è¯­è¨€çš„ç”Ÿæˆå­¦ä¹?
    """
    with torch.no_grad():
        image_features = vision_encoder(image)  # å†»ç»“ViT
    
    # Q-Formerè¾“å‡º32ä¸ªQuery
    queries = qformer.forward(image_features)  # [B, 32, 768]
    
    # çº¿æ€§æŠ•å½±åˆ°LLMè¯åµŒå…¥ç»´åº?
    soft_prompts = linear_projection(queries)  # [B, 32, llm_dim]
    
    # å‰ç½®äºæ–‡æœ?Token ä¹‹å‰
    inputs_embeds = torch.cat([soft_prompts, llm.embed_tokens(text)], dim=1)
    
    with torch.no_grad():
        outputs = llm(inputs_embeds=inputs_embeds)  # å†»ç»“LLM
    
    # è¯­è¨€å»ºæ¨¡æŸå¤±ï¼ˆä»…åœ¨æ–‡æœ¬éƒ¨åˆ†ï¼‰
    loss = language_modeling_loss(outputs, text)
    return loss
```

**å…³é”®è®¾è®¡**ï¼?

- Q-Former è¾“å‡ºä½œä¸º **è½¯æç¤?*ï¼Œä¸è®¡ç®—å…¶æŸå¤?
- LLM å®Œå…¨å†»ç»“ï¼Œä»…è®­ç»ƒ Q-Former å’ŒæŠ•å½±å±‚
- ä¿æŠ¤ LLM çš„è¯­è¨€èƒ½åŠ›ä¸è¢«ç ´å

---

## Flamingo Perceiver Resampler

Flamingo ä½¿ç”¨ Perceiver æ¶æ„å¤„ç†å¤šå›¾åœºæ™¯ã€?

### æ¶æ„ç‰¹ç‚¹

```mermaid
flowchart TB
    IMG1[å›¾åƒ1] --> VIT
    IMG2[å›¾åƒ2] --> VIT
    IMG3[å›¾åƒ3] --> VIT
    VIT[å…±äº« ViT] --> CAT[æ‹¼æ¥ç‰¹å¾]
    CAT --> PR[Perceiver Resampler]
    L[å¯å­¦ä¹?Latents] --> PR
    PR --> OUT[å›ºå®š Token æ•°]
```

**æ ¸å¿ƒæ€æƒ³**ï¼?

- ä½¿ç”¨å›ºå®šæ•°é‡çš„å¯å­¦ä¹  Latent å‘é‡
- é€šè¿‡ Cross-Attention ä»ä»»æ„æ•°é‡å›¾åƒä¸­æå–ç‰¹å¾
- è¾“å‡º Token æ•°é‡æ’å®šï¼Œä¸è¾“å…¥å›¾åƒæ•°é‡æ— å…³

### Gated Cross-Attention

Flamingo åœ?LLM æ¯å±‚æ’å…¥ Gated Cross-Attentionï¼?

```python
# Flamingo Gated Cross-Attention
y = x + tanh(gate) * CrossAttention(x, vision_features)
```

- `gate` åˆå§‹åŒ–ä¸º 0ï¼Œè®­ç»ƒæ—¶é€æ¸å­¦ä¹ 
- ä¿æŠ¤é¢„è®­ç»?LLM æƒé‡ä¸è¢«ç ´å

---

## è®¾è®¡é€‰æ‹©æŒ‡å—

### åœºæ™¯ â†?æ–¹æ¡ˆæ˜ å°„

| åœºæ™¯ | æ¨èæ–¹æ¡ˆ | ç†ç”± |
| :--- | :--- | :--- |
| **OCR/æ–‡æ¡£ç†è§£** | LLaVA Linear | éœ€è¦å®Œæ•´è§†è§‰ç»†èŠ?|
| **èµ„æºå—é™/é«˜å¹¶å?* | Q-Former | Token æ•°é‡å°?|
| **å¤šå›¾äº¤ç»‡å¯¹è¯** | Perceiver | å›ºå®šè¾“å‡ºé•¿åº¦ |
| **å¿«é€Ÿè¿­ä»?ç ”ç©¶** | LLaVA Linear | è®­ç»ƒç®€å?|

### Token æ•°é‡å¯¹æ¨ç†çš„å½±å“

å‡è®¾ LLM ä¸Šä¸‹æ–‡çª—å£ä¸º 4096 Tokenï¼?

| æ–¹æ¡ˆ | è§†è§‰ Token | å‰©ä½™æ–‡æœ¬ Token | æ¨ç†æˆæœ¬ |
| :--- | :--- | :--- | :--- |
| **LLaVA (576)** | 576 | 3520 | é«?|
| **Q-Former (32)** | 32 | 4064 | ä½?|
| **AnyRes (2880)** | 2880 | 1216 | æé«˜ |

---

## è¿›é˜¶ï¼šåŠ¨æ€?Token æ–¹æ¡ˆ

### LLaVA-NeXT AnyRes

è§£å†³é«˜åˆ†è¾¨ç‡å›¾åƒç»†èŠ‚ä¸¢å¤±é—®é¢˜ï¼?

```mermaid
flowchart TB
    IMG[é«˜åˆ†è¾¨ç‡å›¾åƒ] --> GRID[é€‰æ‹©ç½‘æ ¼é…ç½® - 2Ã—2, 1Ã—3, 3Ã—1...]
    IMG --> GLOBAL[å…¨å±€è§†å›¾]
    GRID --> SPLIT[åˆ‡åˆ†å­å›¾]
    SPLIT --> VIT1[ViT ç¼–ç ]
    GLOBAL --> VIT2[ViT ç¼–ç ]
    VIT1 --> CAT[æ‹¼æ¥ç‰¹å¾]
    VIT2 --> CAT
    CAT --> PROJ[Projector]
```

**Token æ•°é‡è®¡ç®—**ï¼?

- å…¨å±€è§†å›¾ï¼?76 Token
- æ¯ä¸ªå­å›¾ï¼?76 Token
- 2Ã—2 é…ç½®æ€»è®¡ï¼?76 + 4Ã—576 = 2880 Token

### Token å‹ç¼©æŠ€æœ?

| æŠ€æœ?| æ–¹æ³• | å‹ç¼©ç?|
| :--- | :--- | :--- |
| **Spatial Pooling** | 2Ã—2 å¹³å‡æ± åŒ– | 4Ã— |
| **Token Merging** | ç›¸ä¼¼ Token åˆå¹¶ | 2-4Ã— |
| **Resampler** | Perceiver æ¶æ„ | å¯å˜ |

---

## å‚è€ƒèµ„æº?

| è®ºæ–‡ | ä¸»é¢˜ |
| :--- | :--- |
| [Visual Instruction Tuning (LLaVA)](https://arxiv.org/abs/2304.08485) | çº¿æ€§æŠ•å½?|
| [BLIP-2](https://arxiv.org/abs/2301.12597) | Q-Former |
| [Flamingo](https://arxiv.org/abs/2204.14198) | Perceiver Resampler |
| [LLaVA-NeXT](https://llava-vl.github.io/blog/2024-01-30-llava-next/) | AnyRes |


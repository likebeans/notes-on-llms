---
title: å‘é‡æ•°æ®åº“è¯¦è§£
description: å‘é‡æ•°æ®åº“åŸç†ã€é€‰å‹ä¸é«˜æ€§èƒ½æ£€ç´¢å®ç°
---

# å‘é‡æ•°æ®åº“è¯¦è§£

> æŒæ¡å‘é‡æ•°æ®åº“çš„æ ¸å¿ƒæŠ€æœ¯ï¼Œé€‰æ‹©åˆé€‚çš„å­˜å‚¨ä¸æ£€ç´¢æ–¹æ¡ˆ

## ğŸ¯ æ ¸å¿ƒæ¦‚å¿µ

### ä»€ä¹ˆæ˜¯å‘é‡æ•°æ®åº“ï¼Ÿ

**å‘é‡æ•°æ®åº“**æ˜¯ä¸“é—¨ç”¨äºå­˜å‚¨ã€ç´¢å¼•å’Œæ£€ç´¢é«˜ç»´å‘é‡æ•°æ®çš„æ•°æ®åº“ç³»ç»Ÿï¼Œæ˜¯RAGç³»ç»Ÿçš„æ ¸å¿ƒåŸºç¡€è®¾æ–½ã€‚

**ä¼ ç»Ÿæ•°æ®åº“ vs å‘é‡æ•°æ®åº“**ï¼š
```python
# ä¼ ç»Ÿæ•°æ®åº“
SELECT * FROM articles WHERE title LIKE '%RAG%'  # ç²¾ç¡®åŒ¹é…

# å‘é‡æ•°æ®åº“  
SELECT * FROM embeddings ORDER BY cosine_distance(vector, query_vector) LIMIT 5  # è¯­ä¹‰ç›¸ä¼¼åº¦
```

### ä¸ºä»€ä¹ˆéœ€è¦å‘é‡æ•°æ®åº“ï¼Ÿ

::: tip æ ¸å¿ƒä»·å€¼
**é«˜ç»´æ£€ç´¢**ï¼šæ”¯æŒæ•°ç™¾åˆ°æ•°åƒç»´å‘é‡çš„é«˜æ•ˆç›¸ä¼¼æ€§æœç´¢  
**è¯­ä¹‰ç†è§£**ï¼šåŸºäºå‘é‡è·ç¦»è¿›è¡Œè¯­ä¹‰åŒ¹é…ï¼Œä¸å†ä¾èµ–å…³é”®è¯  
**è§„æ¨¡åŒ–å¤„ç†**ï¼šæ”¯æŒç™¾ä¸‡åˆ°äº¿çº§å‘é‡çš„å­˜å‚¨ä¸æ¯«ç§’çº§æ£€ç´¢
:::

---

## ğŸ—ï¸ å‘é‡æ£€ç´¢ç®—æ³•åŸç†

> åŸºäº[ã€ŠRAGFlowçš„æ£€ç´¢ç¥å™¨-HNSWï¼šé«˜ç»´å‘é‡ç©ºé—´ä¸­çš„é«˜æ•ˆè¿‘ä¼¼æœ€è¿‘é‚»æœç´¢ç®—æ³•ã€‹](https://dd-ff.blog.csdn.net/article/details/149275016)

### æ£€ç´¢ç®—æ³•æ¼”è¿›

| ç®—æ³•ç±»å‹ | ä»£è¡¨ç®—æ³• | æ—¶é—´å¤æ‚åº¦ | ä¼˜åŠ¿ | åŠ£åŠ¿ |
|----------|----------|------------|------|------|
| **æš´åŠ›æœç´¢** | Linear Scan | O(n) | ç²¾åº¦100% | é€Ÿåº¦æ…¢ |
| **æ ‘ç»“æ„** | KD-Tree | O(log n) | ä½ç»´æ•ˆæœå¥½ | é«˜ç»´æ€§èƒ½å·® |
| **å“ˆå¸Œæ–¹æ³•** | LSH | O(1) | é€Ÿåº¦å¿« | ç²¾åº¦ä¸ç¨³å®š |
| **å›¾æ–¹æ³•** | **HNSW** | O(log n) | é«˜ç²¾åº¦+é«˜é€Ÿåº¦ | å†…å­˜å ç”¨å¤§ |

### HNSWç®—æ³•æ·±åº¦è§£æ

#### æ ¸å¿ƒæ€æƒ³

HNSWï¼ˆHierarchical Navigable Small Worldï¼‰ç»“åˆäº†**è·³è¡¨**å’Œ**å°ä¸–ç•Œç½‘ç»œ**çš„ä¼˜åŠ¿ï¼š

1. **è·³è¡¨å¯å‘**ï¼šæ„å»ºå¤šå±‚ç´¢å¼•ï¼Œå¿«é€Ÿç¼©å°æœç´¢èŒƒå›´
2. **å°ä¸–ç•Œç½‘ç»œ**ï¼šé€šè¿‡"æ·å¾„"è¿æ¥å®ç°å¿«é€Ÿå¯¼èˆª
3. **å±‚æ¬¡ç»“æ„**ï¼šä»ç²—ç²’åº¦åˆ°ç»†ç²’åº¦çš„æ¸è¿›å¼æœç´¢

#### ç®—æ³•æ¶æ„

```python
# HNSWçš„å±‚æ¬¡åŒ–ç»“æ„
Layer 2: [å…¥å£èŠ‚ç‚¹] ----é•¿è·ç¦»è¿æ¥----> [èŠ‚ç‚¹A]
         â†“                               â†“
Layer 1: [èŠ‚ç‚¹ç¾¤1] ----ä¸­è·ç¦»è¿æ¥----> [èŠ‚ç‚¹ç¾¤2] 
         â†“                               â†“
Layer 0: [æ‰€æœ‰èŠ‚ç‚¹] --çŸ­è·ç¦»è¿æ¥--> [ç›®æ ‡åŒºåŸŸ]
```

**æœç´¢æµç¨‹**ï¼š
1. **é¡¶å±‚å¯¼èˆª**ï¼šä»å…¥å£ç‚¹å¼€å§‹ï¼Œå¿«é€Ÿå®šä½å¤§è‡´åŒºåŸŸ
2. **é€å±‚ä¸‹é™**ï¼šåœ¨æ¯å±‚æ‰¾åˆ°å±€éƒ¨æœ€ä¼˜ç‚¹ï¼Œä½œä¸ºä¸‹å±‚èµ·ç‚¹  
3. **åº•å±‚ç²¾æœ**ï¼šåœ¨æœ€å¯†é›†çš„åº•å±‚è¿›è¡Œç²¾ç¡®æ£€ç´¢

#### æ€§èƒ½è¡¨ç°

::: info å®éªŒæ•°æ®ï¼ˆ10,000æ¡768ç»´å‘é‡ï¼‰
**å¹³å‡æŸ¥è¯¢æ—¶é—´**ï¼š5.58æ¯«ç§’  
**æ£€ç´¢ç²¾åº¦**ï¼šæ¥è¿‘100%ï¼ˆç›¸æ¯”æš´åŠ›æœç´¢ï¼‰  
**å†…å­˜å¼€é”€**ï¼šçº¦ä¸ºåŸå§‹æ•°æ®çš„1.5-2å€
:::

---

## ğŸ“Š ä¸»æµå‘é‡æ•°æ®åº“å¯¹æ¯”

### å¼€æºè§£å†³æ–¹æ¡ˆ

| æ•°æ®åº“ | ç®—æ³•æ”¯æŒ | è¯­è¨€ | ç‰¹ç‚¹ | é€‚ç”¨åœºæ™¯ |
|--------|----------|------|------|----------|
| **Chroma** | HNSW | Python | è½»é‡ã€æ˜“ç”¨ | åŸå‹å¼€å‘ã€å°è§„æ¨¡ |
| **Weaviate** | HNSW | Go | åŠŸèƒ½ä¸°å¯Œã€GraphQL | ä¸­å‹é¡¹ç›® |  
| **Qdrant** | HNSW | Rust | é«˜æ€§èƒ½ã€äº‘åŸç”Ÿ | é«˜å¹¶å‘åœºæ™¯ |
| **Milvus** | HNSW/IVF/FAISS | C++ | ä¼ä¸šçº§ã€åˆ†å¸ƒå¼ | å¤§è§„æ¨¡ç”Ÿäº§ |

### äº‘æœåŠ¡æ–¹æ¡ˆ

| æœåŠ¡ | æä¾›å•† | ä¼˜åŠ¿ | å®šä»·æ¨¡å¼ |
|------|--------|------|----------|
| **Pinecone** | Pinecone | æ‰˜ç®¡å¼ã€æ˜“æ‰©å±• | æŒ‰å‘é‡æ•°+æŸ¥è¯¢é‡ |
| **Zilliz Cloud** | Zilliz | åŸºäºMilvus | æŒ‰èµ„æºä½¿ç”¨é‡ |
| **Supabase Vector** | Supabase | é›†æˆPostgreSQL | æŒ‰å­˜å‚¨+è®¡ç®— |
| **Elastic Cloud** | Elastic | ä¸ESç”Ÿæ€æ•´åˆ | æŒ‰èŠ‚ç‚¹èµ„æº |

### é€‰å‹æŒ‡å—

::: details æŒ‰åœºæ™¯é€‰æ‹©
**ğŸš€ å¿«é€ŸåŸå‹**ï¼šChroma - 5åˆ†é’Ÿä¸Šæ‰‹ï¼Œæœ¬åœ°å¼€å‘å‹å¥½  
**ğŸ“ˆ ä¸­å‹é¡¹ç›®**ï¼šQdrant - æ€§èƒ½å¥½ï¼Œéƒ¨ç½²ç®€å•  
**ğŸ¢ ä¼ä¸šç”Ÿäº§**ï¼šMilvus - åŠŸèƒ½å…¨é¢ï¼Œæ”¯æŒé›†ç¾¤  
**â˜ï¸ æ‰˜ç®¡æœåŠ¡**ï¼šPinecone - å…è¿ç»´ï¼ŒæŒ‰éœ€ä»˜è´¹
:::

---

## ğŸ’» å®æˆ˜ä»£ç ç¤ºä¾‹

### Chroma å¿«é€Ÿä¸Šæ‰‹

```python
import chromadb
from chromadb.config import Settings

# 1. åˆå§‹åŒ–å®¢æˆ·ç«¯
client = chromadb.Client(Settings(
    persist_directory="./chroma_db",  # æ•°æ®æŒä¹…åŒ–ç›®å½•
    anonymized_telemetry=False
))

# 2. åˆ›å»ºé›†åˆ
collection = client.create_collection(
    name="rag_documents",
    metadata={"description": "RAGç³»ç»Ÿæ–‡æ¡£å‘é‡"}
)

# 3. æ·»åŠ æ–‡æ¡£
documents = [
    "æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ˜¯ä¸€ç§ç»“åˆæ£€ç´¢å’Œç”Ÿæˆçš„AIæŠ€æœ¯",
    "å‘é‡æ•°æ®åº“ç”¨äºå­˜å‚¨å’Œæ£€ç´¢é«˜ç»´å‘é‡æ•°æ®",
    "HNSWç®—æ³•æä¾›äº†é«˜æ•ˆçš„è¿‘ä¼¼æœ€è¿‘é‚»æœç´¢"
]

# è‡ªåŠ¨å‘é‡åŒ–å¹¶å­˜å‚¨
collection.add(
    documents=documents,
    ids=[f"doc_{i}" for i in range(len(documents))],
    metadatas=[{"source": "manual", "index": i} for i in range(len(documents))]
)

# 4. è¯­ä¹‰æœç´¢
results = collection.query(
    query_texts=["ä»€ä¹ˆæ˜¯RAGæŠ€æœ¯ï¼Ÿ"],
    n_results=2,
    include=["documents", "distances", "metadatas"]
)

print("æ£€ç´¢ç»“æœ:")
for i, (doc, distance) in enumerate(zip(results['documents'][0], results['distances'][0])):
    print(f"{i+1}. ç›¸ä¼¼åº¦: {1-distance:.3f}")
    print(f"   å†…å®¹: {doc[:50]}...")
```

### Qdrant é«˜æ€§èƒ½éƒ¨ç½²

```python
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams, PointStruct

# 1. è¿æ¥QdrantæœåŠ¡
client = QdrantClient(
    url="http://localhost:6333",  # æœ¬åœ°éƒ¨ç½²
    # url="https://your-cluster.qdrant.tech",  # äº‘æœåŠ¡
    # api_key="your-api-key"
)

# 2. åˆ›å»ºé›†åˆ
collection_name = "document_vectors"
client.create_collection(
    collection_name=collection_name,
    vectors_config=VectorParams(
        size=1536,  # OpenAI embeddingç»´åº¦
        distance=Distance.COSINE
    )
)

# 3. æ‰¹é‡æ’å…¥å‘é‡
from openai import OpenAI
openai_client = OpenAI()

def get_embeddings(texts):
    response = openai_client.embeddings.create(
        model="text-embedding-3-small",
        input=texts
    )
    return [item.embedding for item in response.data]

# å‡†å¤‡æ•°æ®
documents = [
    "RAGç³»ç»Ÿçš„æ ¸å¿ƒæ˜¯æ£€ç´¢å’Œç”Ÿæˆçš„ç»“åˆ",
    "å‘é‡æ•°æ®åº“æ”¯æŒé«˜ç»´å‘é‡çš„ç›¸ä¼¼æ€§æœç´¢",
    "HNSWç®—æ³•åœ¨ç²¾åº¦å’Œé€Ÿåº¦é—´è¾¾åˆ°æœ€ä½³å¹³è¡¡"
]

embeddings = get_embeddings(documents)

# æ„é€ ç‚¹æ•°æ®
points = []
for i, (doc, vector) in enumerate(zip(documents, embeddings)):
    points.append(PointStruct(
        id=i,
        vector=vector,
        payload={
            "text": doc,
            "timestamp": "2024-01-01",
            "category": "rag_tech"
        }
    ))

# æ‰¹é‡ä¸Šä¼ 
client.upsert(
    collection_name=collection_name,
    points=points
)

# 4. é«˜çº§æ£€ç´¢
query_text = "å‘é‡æœç´¢ç®—æ³•"
query_vector = get_embeddings([query_text])[0]

search_results = client.search(
    collection_name=collection_name,
    query_vector=query_vector,
    limit=5,
    score_threshold=0.7,  # ç›¸ä¼¼åº¦é˜ˆå€¼
    with_payload=True,
    with_vectors=False
)

print("æ£€ç´¢ç»“æœ:")
for result in search_results:
    print(f"ID: {result.id}, å¾—åˆ†: {result.score:.3f}")
    print(f"å†…å®¹: {result.payload['text']}")
    print("---")
```

### Milvus ä¼ä¸šçº§æ–¹æ¡ˆ

```python
from pymilvus import connections, Collection, FieldSchema, CollectionSchema, DataType, utility

# 1. è¿æ¥Milvus
connections.connect(
    alias="default",
    host="localhost",  # MilvusæœåŠ¡åœ°å€
    port="19530"
)

# 2. å®šä¹‰æ•°æ®ç»“æ„
fields = [
    FieldSchema(name="id", dtype=DataType.INT64, is_primary=True, auto_id=True),
    FieldSchema(name="text", dtype=DataType.VARCHAR, max_length=65535),
    FieldSchema(name="vector", dtype=DataType.FLOAT_VECTOR, dim=1536),
    FieldSchema(name="category", dtype=DataType.VARCHAR, max_length=100)
]

schema = CollectionSchema(
    fields=fields,
    description="RAGæ–‡æ¡£å‘é‡é›†åˆ",
    enable_dynamic_field=True
)

# 3. åˆ›å»ºé›†åˆ
collection_name = "rag_collection"
if utility.has_collection(collection_name):
    utility.drop_collection(collection_name)

collection = Collection(name=collection_name, schema=schema)

# 4. åˆ›å»ºç´¢å¼•ï¼ˆHNSWï¼‰
index_params = {
    "metric_type": "COSINE",
    "index_type": "HNSW",
    "params": {
        "M": 16,        # æ¯ä¸ªèŠ‚ç‚¹çš„æœ€å¤§è¿æ¥æ•°
        "efConstruction": 200  # æ„å»ºæ—¶çš„æœç´¢æ·±åº¦
    }
}

collection.create_index(
    field_name="vector",
    index_params=index_params,
    timeout=300
)

# 5. æ’å…¥æ•°æ®
import random

def generate_test_data(n=1000):
    texts = [f"è¿™æ˜¯ç¬¬{i}æ¡RAGç›¸å…³æ–‡æ¡£å†…å®¹" for i in range(n)]
    vectors = [[random.random() for _ in range(1536)] for _ in range(n)]
    categories = ["tech", "business", "research"] * (n // 3 + 1)
    
    return {
        "text": texts,
        "vector": vectors,
        "category": categories[:n]
    }

data = generate_test_data(10000)
collection.insert(data)
collection.flush()  # ç¡®ä¿æ•°æ®å†™å…¥

# 6. åŠ è½½é›†åˆåˆ°å†…å­˜
collection.load()

# 7. æ‰§è¡Œæœç´¢
search_params = {
    "metric_type": "COSINE",
    "params": {"ef": 64}  # æœç´¢æ—¶çš„å€™é€‰æ•°é‡
}

query_vector = [[random.random() for _ in range(1536)]]
results = collection.search(
    data=query_vector,
    anns_field="vector",
    param=search_params,
    limit=10,
    expr='category == "tech"',  # è¿‡æ»¤æ¡ä»¶
    output_fields=["text", "category"],
    timeout=30
)

print("æ£€ç´¢ç»“æœ:")
for hits in results:
    for hit in hits:
        print(f"ID: {hit.id}, è·ç¦»: {hit.distance:.3f}")
        print(f"æ–‡æœ¬: {hit.entity.get('text')}")
        print(f"åˆ†ç±»: {hit.entity.get('category')}")
        print("---")
```

---

## âš¡ æ€§èƒ½ä¼˜åŒ–ç­–ç•¥

### 1. ç´¢å¼•å‚æ•°è°ƒä¼˜

#### HNSWå‚æ•°è¯¦è§£

```python
# Qdrant HNSWé…ç½®
hnsw_config = {
    "m": 16,           # è¿æ¥æ•°ï¼šè¶Šå¤§ç²¾åº¦è¶Šé«˜ï¼Œå†…å­˜è¶Šå¤§
    "ef_construct": 200, # æ„å»ºå‚æ•°ï¼šå½±å“æ„å»ºè´¨é‡
    "max_indexing_threads": 4,  # æ„å»ºçº¿ç¨‹æ•°
}

# æœç´¢å‚æ•°
search_params = {
    "ef": 128,         # æœç´¢å€™é€‰æ•°ï¼šè¶Šå¤§ç²¾åº¦è¶Šé«˜ï¼Œé€Ÿåº¦è¶Šæ…¢
    "exact": False     # æ˜¯å¦ç²¾ç¡®æœç´¢
}
```

#### å‚æ•°é€‰æ‹©å»ºè®®

| åœºæ™¯ | M | ef_construct | ef | å†…å­˜å ç”¨ | é€Ÿåº¦ | ç²¾åº¦ |
|------|---|--------------|----|-----------|----- |------|
| **é«˜ç²¾åº¦** | 64 | 400 | 200 | é«˜ | æ…¢ | æé«˜ |
| **å¹³è¡¡** | 16 | 200 | 64 | ä¸­ | ä¸­ | é«˜ |
| **é«˜é€Ÿåº¦** | 8 | 100 | 32 | ä½ | å¿« | ä¸­ç­‰ |

### 2. æ•°æ®é¢„å¤„ç†ä¼˜åŒ–

```python
def optimize_vectors(embeddings):
    """å‘é‡é¢„å¤„ç†ä¼˜åŒ–"""
    import numpy as np
    from sklearn.preprocessing import normalize
    
    # 1. æ ‡å‡†åŒ–å¤„ç†
    normalized = normalize(embeddings, norm='l2')
    
    # 2. é™ç»´ï¼ˆå¯é€‰ï¼‰
    from sklearn.decomposition import PCA
    pca = PCA(n_components=512)  # ä»1536é™åˆ°512
    reduced = pca.fit_transform(normalized)
    
    # 3. é‡åŒ–å‹ç¼©
    quantized = (reduced * 127).astype(np.int8)  # 8ä½é‡åŒ–
    
    return quantized
```

### 3. æ‰¹é‡æ“ä½œä¼˜åŒ–

```python
class BatchProcessor:
    def __init__(self, collection, batch_size=1000):
        self.collection = collection
        self.batch_size = batch_size
        self.buffer = []
    
    def add(self, document, vector, metadata=None):
        """æ·»åŠ åˆ°ç¼“å†²åŒº"""
        self.buffer.append({
            'doc': document,
            'vector': vector,
            'metadata': metadata or {}
        })
        
        # è¾¾åˆ°æ‰¹æ¬¡å¤§å°æ—¶è‡ªåŠ¨æäº¤
        if len(self.buffer) >= self.batch_size:
            self.flush()
    
    def flush(self):
        """æ‰¹é‡æäº¤"""
        if not self.buffer:
            return
        
        documents = [item['doc'] for item in self.buffer]
        vectors = [item['vector'] for item in self.buffer]
        metadatas = [item['metadata'] for item in self.buffer]
        
        self.collection.add(
            documents=documents,
            embeddings=vectors,
            metadatas=metadatas,
            ids=[f"batch_{i}" for i in range(len(documents))]
        )
        
        self.buffer.clear()
        print(f"å·²æäº¤ {len(documents)} æ¡è®°å½•")

# ä½¿ç”¨ç¤ºä¾‹
processor = BatchProcessor(collection)

for doc, vector in document_stream:
    processor.add(doc, vector, {"timestamp": time.time()})

processor.flush()  # å¤„ç†å‰©ä½™æ•°æ®
```

---

## ğŸ”§ ç”Ÿäº§éƒ¨ç½²å®è·µ

### Docker å®¹å™¨åŒ–éƒ¨ç½²

```yaml
# docker-compose.yml
version: '3.8'

services:
  qdrant:
    image: qdrant/qdrant:v1.7.0
    ports:
      - "6333:6333"
      - "6334:6334"
    volumes:
      - ./qdrant_storage:/qdrant/storage
    environment:
      - QDRANT__LOG_LEVEL=INFO
    deploy:
      resources:
        limits:
          memory: 4G
        reservations:
          memory: 2G

  milvus-etcd:
    image: quay.io/coreos/etcd:v3.5.0
    environment:
      - ETCD_AUTO_COMPACTION_MODE=revision
      - ETCD_AUTO_COMPACTION_RETENTION=1000
    volumes:
      - ./etcd:/etcd
    command: etcd -advertise-client-urls=http://127.0.0.1:2379 -listen-client-urls http://0.0.0.0:2379 --data-dir /etcd

  milvus-minio:
    image: minio/minio:RELEASE.2023-03-20T20-16-18Z
    environment:
      MINIO_ACCESS_KEY: minioadmin
      MINIO_SECRET_KEY: minioadmin
    ports:
      - "9001:9001"
      - "9000:9000"
    volumes:
      - ./minio:/minio_data
    command: minio server /minio_data --console-address ":9001"

  milvus-standalone:
    image: milvusdb/milvus:v2.3.0
    command: ["milvus", "run", "standalone"]
    environment:
      ETCD_ENDPOINTS: milvus-etcd:2379
      MINIO_ADDRESS: milvus-minio:9000
    volumes:
      - ./milvus.yaml:/milvus/configs/milvus.yaml
    ports:
      - "19530:19530"
      - "9091:9091"
    depends_on:
      - "milvus-etcd"
      - "milvus-minio"
```

### ç›‘æ§ä¸è¿ç»´

```python
class VectorDBMonitor:
    def __init__(self, client):
        self.client = client
    
    def health_check(self):
        """å¥åº·æ£€æŸ¥"""
        try:
            # æ‰§è¡Œç®€å•æŸ¥è¯¢
            start_time = time.time()
            result = self.client.search(...)
            latency = time.time() - start_time
            
            return {
                "status": "healthy",
                "latency": latency,
                "timestamp": time.time()
            }
        except Exception as e:
            return {
                "status": "error", 
                "error": str(e),
                "timestamp": time.time()
            }
    
    def performance_metrics(self):
        """æ€§èƒ½æŒ‡æ ‡"""
        return {
            "collection_size": self.client.count(),
            "memory_usage": self.get_memory_usage(),
            "qps": self.calculate_qps(),
            "avg_latency": self.get_avg_latency()
        }
```

---

## ğŸ”— ç›¸å…³é˜…è¯»

- [RAGèŒƒå¼æ¼”è¿›](/llms/rag/paradigms) - äº†è§£RAGæŠ€æœ¯å‘å±•è„‰ç»œ
- [EmbeddingæŠ€æœ¯è¯¦è§£](/llms/rag/embedding) - ç†è§£å‘é‡åŒ–åŸç†
- [æ£€ç´¢ç­–ç•¥ä¼˜åŒ–](/llms/rag/retrieval) - ä¼˜åŒ–æ£€ç´¢æ•ˆæœ
- [æ€§èƒ½è¯„ä¼°æ–¹æ³•](/llms/rag/evaluation) - è¯„ä¼°å‘é‡æ£€ç´¢æ€§èƒ½

> **ç›¸å…³æ–‡ç« **ï¼š
> - [RAGFlowçš„æ£€ç´¢ç¥å™¨-HNSWï¼šé«˜ç»´å‘é‡ç©ºé—´ä¸­çš„é«˜æ•ˆè¿‘ä¼¼æœ€è¿‘é‚»æœç´¢ç®—æ³•](https://dd-ff.blog.csdn.net/article/details/149275016)
> - [æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç»¼è¿°ï¼šæŠ€æœ¯èŒƒå¼ã€æ ¸å¿ƒç»„ä»¶ä¸æœªæ¥å±•æœ›](https://dd-ff.blog.csdn.net/article/details/149274498)

> **å¤–éƒ¨èµ„æº**ï¼š
> - [Milvuså®˜æ–¹æ–‡æ¡£](https://milvus.io/docs) - åˆ†å¸ƒå¼å‘é‡æ•°æ®åº“
> - [Qdrantå®˜æ–¹æ–‡æ¡£](https://qdrant.tech/documentation/) - é«˜æ€§èƒ½å‘é‡æœç´¢å¼•æ“
> - [Chromaå®˜æ–¹æ–‡æ¡£](https://docs.trychroma.com/) - è½»é‡çº§å‘é‡æ•°æ®åº“
> - [FAISS GitHub](https://github.com/facebookresearch/faiss) - Metaå¼€æºå‘é‡æ£€ç´¢åº“

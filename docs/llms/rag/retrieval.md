---
title: æ£€ç´¢ç­–ç•¥ä¼˜åŒ–
description: RAGç³»ç»Ÿä¸­çš„æ£€ç´¢æŠ€æœ¯ä¸ç­–ç•¥è¯¦è§£
---

# æ£€ç´¢ç­–ç•¥ä¼˜åŒ–

> æŒæ¡å¤šç§æ£€ç´¢æŠ€æœ¯ï¼Œæ„å»ºé«˜æ•ˆå‡†ç¡®çš„RAGæ£€ç´¢ç³»ç»Ÿ

## ğŸ¯ æ ¸å¿ƒæ¦‚å¿µ

### ä»€ä¹ˆæ˜¯æ£€ç´¢ç­–ç•¥ï¼Ÿ

**æ£€ç´¢ç­–ç•¥**æ˜¯RAGç³»ç»Ÿä¸­è´Ÿè´£ä»å‘é‡æ•°æ®åº“ä¸­æ‰¾åˆ°ä¸ç”¨æˆ·æŸ¥è¯¢æœ€ç›¸å…³æ–‡æ¡£çš„æŠ€æœ¯æ–¹æ¡ˆã€‚å®ƒç›´æ¥å½±å“RAGç³»ç»Ÿçš„å‡†ç¡®æ€§å’Œå“åº”é€Ÿåº¦ã€‚

**æ£€ç´¢çš„æ ¸å¿ƒæŒ‘æˆ˜**ï¼š
- **å¬å›ç‡ vs ç²¾ç¡®ç‡**ï¼šå¦‚ä½•åœ¨æ£€ç´¢æ›´å¤šç›¸å…³å†…å®¹çš„åŒæ—¶å‡å°‘å™ªéŸ³
- **è¯­ä¹‰ç†è§£ vs ç²¾ç¡®åŒ¹é…**ï¼šå¹³è¡¡è¯­ä¹‰ç›¸ä¼¼æ€§å’Œå…³é”®è¯åŒ¹é…
- **æ•ˆç‡ vs è´¨é‡**ï¼šåœ¨æ£€ç´¢é€Ÿåº¦å’Œç»“æœè´¨é‡é—´æ‰¾åˆ°å¹³è¡¡

### å½“å‰æ£€ç´¢æ–¹æ³•çš„å±€é™æ€§

> æ¥æºï¼š[RAGæŠ€æœ¯çš„5ç§èŒƒå¼](https://hub.baai.ac.cn/view/43613)

::: warning å…³é”®æ´å¯Ÿ
å½“å‰RAGç³»ç»Ÿçš„å¤§å¤šæ•°æ£€ç´¢æ–¹æ³•ä¾èµ–äº**å…³é”®è¯å’Œç›¸ä¼¼æ€§æœç´¢**ï¼Œè¿™é™åˆ¶äº†RAGç³»ç»Ÿçš„æ•´ä½“å‡†ç¡®æ€§ã€‚å¦‚æœæ£€ç´¢(R)éƒ¨åˆ†æä¾›çš„ä¸Šä¸‹æ–‡ä¸ç›¸å…³ï¼Œæ— è®ºç”Ÿæˆ(G)éƒ¨åˆ†å¦‚ä½•ä¼˜åŒ–ï¼Œç­”æ¡ˆä¹Ÿå°†ä¸å‡†ç¡®ã€‚
:::

| æ£€ç´¢æ–¹æ³• | æŠ€æœ¯åŸç† | å±€é™æ€§ |
|----------|----------|--------|
| **BM25** | åŸºäºè¯é¢‘(TF)ã€é€†æ–‡æ¡£é¢‘ç‡(IDF)å’Œæ–‡æ¡£é•¿åº¦ | æ— æ³•æ•æ‰è¯­ä¹‰å…³ç³» |
| **å¯†é›†å‘é‡** | kè¿‘é‚»(KNN)ç®—æ³•ï¼Œä½™å¼¦ç›¸ä¼¼åº¦ | ä¾èµ–Embeddingæ¨¡å‹è´¨é‡ |
| **ç¨€ç–ç¼–ç å™¨** | æ‰©å±•æœ¯è¯­æ˜ å°„ï¼Œä¿æŒé«˜ç»´è§£é‡Šæ€§ | å¤„ç†å¤æ‚æŸ¥è¯¢èƒ½åŠ›æœ‰é™ |

### æ£€ç´¢è¯„ä¼°æŒ‡æ ‡é€ŸæŸ¥

| æŒ‡æ ‡ | å…¬å¼è¦ç‚¹ | ä½œç”¨ |
|------|----------|------|
| **Recall@K** | æ£€ç´¢åˆ°çš„ç›¸å…³æ–‡æ¡£ / æ€»ç›¸å…³æ–‡æ¡£ | è¡¡é‡å¬å›èƒ½åŠ› |
| **Precision@K** | æ£€ç´¢åˆ°çš„ç›¸å…³æ–‡æ¡£ / æ£€ç´¢çš„æ€»æ–‡æ¡£ | è¡¡é‡ç²¾ç¡®åº¦ |
| **F1@K** | Recallå’ŒPrecisionçš„è°ƒå’Œå¹³å‡ | ç»¼åˆè¯„ä¼° |
| **MAP** | å¹³å‡ç²¾åº¦å‡å€¼ | æ’åºè´¨é‡ |

---

## ğŸ“Š æ£€ç´¢ç­–ç•¥åˆ†ç±»

### æŒ‰æ£€ç´¢æ–¹å¼åˆ†ç±»

| æ£€ç´¢ç±»å‹ | åŸç† | ä¼˜åŠ¿ | åŠ£åŠ¿ | é€‚ç”¨åœºæ™¯ |
|----------|------|------|------|----------|
| **ç¨ å¯†æ£€ç´¢** | å‘é‡ç›¸ä¼¼åº¦è®¡ç®— | è¯­ä¹‰ç†è§£å¼º | ä¾èµ–æ¨¡å‹è´¨é‡ | æ¦‚å¿µæ€§æŸ¥è¯¢ |
| **ç¨€ç–æ£€ç´¢** | å…³é”®è¯åŒ¹é…ï¼ˆBM25ï¼‰ | ç²¾ç¡®åŒ¹é…å¥½ | ç¼ºä¹è¯­ä¹‰ç†è§£ | ç‰¹å®šæœ¯è¯­æŸ¥è¯¢ |
| **æ··åˆæ£€ç´¢** | ç¨ å¯†+ç¨€ç–èåˆ | å…¼é¡¾ä¸¤è€…ä¼˜åŠ¿ | å¤æ‚åº¦é«˜ | é€šç”¨åœºæ™¯ |

### æŒ‰æŸ¥è¯¢å¤„ç†åˆ†ç±»

| ç­–ç•¥ | æŠ€æœ¯è¦ç‚¹ | æ•ˆæœ |
|------|----------|------|
| **åŸå§‹æŸ¥è¯¢** | ç›´æ¥ä½¿ç”¨ç”¨æˆ·è¾“å…¥ | ç®€å•ç›´æ¥ |
| **æŸ¥è¯¢æ”¹å†™** | LLMé‡å†™æŸ¥è¯¢è¯­å¥ | æå‡åŒ¹é…åº¦ |
| **æŸ¥è¯¢æ‰©å±•** | æ·»åŠ åŒä¹‰è¯ã€ç›¸å…³è¯ | æé«˜å¬å›ç‡ |
| **å¤šæŸ¥è¯¢** | åˆ†è§£ä¸ºå¤šä¸ªå­æŸ¥è¯¢ | è¦†ç›–æ›´å…¨é¢ |

---

## ğŸ” ç¨ å¯†æ£€ç´¢ï¼ˆDense Retrievalï¼‰

### æ ¸å¿ƒåŸç†

ç¨ å¯†æ£€ç´¢é€šè¿‡è®¡ç®—æŸ¥è¯¢å‘é‡ä¸æ–‡æ¡£å‘é‡çš„ç›¸ä¼¼åº¦æ¥åŒ¹é…ç›¸å…³å†…å®¹ï¼š

```python
# ç¨ å¯†æ£€ç´¢çš„æ•°å­¦åŸç†
similarity = cosine_similarity(query_vector, document_vector)
# æˆ–ä½¿ç”¨ç‚¹ç§¯
similarity = dot_product(query_vector, document_vector)
```

### ç›¸ä¼¼åº¦è®¡ç®—æ–¹æ³•

#### 1. ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆæ¨èï¼‰
```python
import numpy as np

def cosine_similarity(vec1, vec2):
    """è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦"""
    dot_product = np.dot(vec1, vec2)
    norm1 = np.linalg.norm(vec1)
    norm2 = np.linalg.norm(vec2)
    return dot_product / (norm1 * norm2)

# ç¤ºä¾‹
query_vec = [0.1, 0.2, 0.3]
doc_vec = [0.15, 0.18, 0.32]
sim = cosine_similarity(query_vec, doc_vec)
print(f"ç›¸ä¼¼åº¦: {sim:.3f}")  # è¾“å‡ºï¼š0.999
```

#### 2. æ¬§å‡ é‡Œå¾—è·ç¦»
```python
def euclidean_distance(vec1, vec2):
    """æ¬§å‡ é‡Œå¾—è·ç¦»ï¼ˆè¶Šå°è¶Šç›¸ä¼¼ï¼‰"""
    return np.linalg.norm(np.array(vec1) - np.array(vec2))

# è½¬æ¢ä¸ºç›¸ä¼¼åº¦åˆ†æ•°
def euclidean_similarity(vec1, vec2):
    distance = euclidean_distance(vec1, vec2)
    return 1 / (1 + distance)  # è·ç¦»è¶Šå°ï¼Œç›¸ä¼¼åº¦è¶Šé«˜
```

### å®æˆ˜ä»£ç 

```python
class DenseRetriever:
    def __init__(self, embedding_model, vector_db):
        self.embedding_model = embedding_model
        self.vector_db = vector_db
    
    def retrieve(self, query: str, top_k: int = 5, threshold: float = 0.7):
        """ç¨ å¯†æ£€ç´¢å®ç°"""
        # 1. æŸ¥è¯¢å‘é‡åŒ–
        query_vector = self.embedding_model.encode(query)
        
        # 2. å‘é‡æ£€ç´¢
        results = self.vector_db.search(
            vector=query_vector,
            top_k=top_k * 2,  # å¤šæ£€ç´¢ä¸€äº›å€™é€‰
            metric="cosine"
        )
        
        # 3. ç›¸ä¼¼åº¦è¿‡æ»¤
        filtered_results = []
        for result in results:
            if result.score >= threshold:
                filtered_results.append(result)
        
        return filtered_results[:top_k]

# ä½¿ç”¨ç¤ºä¾‹
from sentence_transformers import SentenceTransformer

model = SentenceTransformer('BAAI/bge-large-zh-v1.5')
retriever = DenseRetriever(model, vector_db)

results = retriever.retrieve("ä»€ä¹ˆæ˜¯RAGæŠ€æœ¯ï¼Ÿ", top_k=5)
for result in results:
    print(f"ç›¸ä¼¼åº¦: {result.score:.3f} | å†…å®¹: {result.text[:100]}...")
```

---

## ğŸ”¤ ç¨€ç–æ£€ç´¢ï¼ˆSparse Retrievalï¼‰

### BM25ç®—æ³•è¯¦è§£

BM25ï¼ˆBest Matching 25ï¼‰æ˜¯æœ€ç»å…¸çš„ç¨€ç–æ£€ç´¢ç®—æ³•ï¼ŒåŸºäºè¯é¢‘-é€†æ–‡æ¡£é¢‘ç‡ï¼ˆTF-IDFï¼‰æ”¹è¿›ï¼š

**BM25å…¬å¼**ï¼š
```
BM25(q,d) = Î£ IDF(qi) * (f(qi,d) * (k1 + 1)) / (f(qi,d) + k1 * (1 - b + b * |d|/avgdl))
```

å…¶ä¸­ï¼š
- `f(qi,d)`ï¼šè¯qiåœ¨æ–‡æ¡£dä¸­çš„é¢‘ç‡
- `|d|`ï¼šæ–‡æ¡£dçš„é•¿åº¦
- `avgdl`ï¼šå¹³å‡æ–‡æ¡£é•¿åº¦
- `k1`, `b`ï¼šè°ƒèŠ‚å‚æ•°

### å®æˆ˜å®ç°

```python
from rank_bm25 import BM25Okapi
import jieba

class SparseRetriever:
    def __init__(self, documents):
        # ä¸­æ–‡åˆ†è¯
        self.tokenized_docs = [list(jieba.cut(doc)) for doc in documents]
        self.bm25 = BM25Okapi(self.tokenized_docs)
        self.documents = documents
    
    def retrieve(self, query: str, top_k: int = 5):
        """BM25æ£€ç´¢"""
        # æŸ¥è¯¢åˆ†è¯
        tokenized_query = list(jieba.cut(query))
        
        # è®¡ç®—BM25åˆ†æ•°
        scores = self.bm25.get_scores(tokenized_query)
        
        # æ’åºè·å–top-k
        top_indices = scores.argsort()[-top_k:][::-1]
        
        results = []
        for idx in top_indices:
            results.append({
                'text': self.documents[idx],
                'score': scores[idx],
                'index': idx
            })
        
        return results

# ä½¿ç”¨ç¤ºä¾‹
documents = [
    "æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯ç»“åˆäº†ä¿¡æ¯æ£€ç´¢å’Œæ–‡æœ¬ç”Ÿæˆ",
    "å‘é‡æ•°æ®åº“æ˜¯å­˜å‚¨é«˜ç»´å‘é‡å¹¶æ”¯æŒç›¸ä¼¼æ€§æœç´¢çš„æ•°æ®åº“",
    "è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„é¢„è®­ç»ƒæ¨¡å‹å¦‚BERTæ”¹å˜äº†NLPé¢†åŸŸ"
]

sparse_retriever = SparseRetriever(documents)
results = sparse_retriever.retrieve("RAGæŠ€æœ¯åŸç†", top_k=2)

for result in results:
    print(f"BM25åˆ†æ•°: {result['score']:.3f}")
    print(f"å†…å®¹: {result['text']}")
    print("---")
```

---

## ğŸ”€ æ··åˆæ£€ç´¢ï¼ˆHybrid Retrievalï¼‰

### æ ¸å¿ƒæ€æƒ³

æ··åˆæ£€ç´¢ç»“åˆç¨ å¯†æ£€ç´¢å’Œç¨€ç–æ£€ç´¢çš„ä¼˜åŠ¿ï¼Œé€šè¿‡åŠ æƒèåˆè·å¾—æ›´å¥½çš„æ£€ç´¢æ•ˆæœã€‚

### èåˆç­–ç•¥

#### 1. åˆ†æ•°åŠ æƒèåˆ

```python
class HybridRetriever:
    def __init__(self, dense_retriever, sparse_retriever, alpha=0.7):
        self.dense_retriever = dense_retriever
        self.sparse_retriever = sparse_retriever
        self.alpha = alpha  # ç¨ å¯†æ£€ç´¢æƒé‡
    
    def retrieve(self, query: str, top_k: int = 5):
        """æ··åˆæ£€ç´¢å®ç°"""
        # 1. åˆ†åˆ«è·å–ç¨ å¯†å’Œç¨€ç–æ£€ç´¢ç»“æœ
        dense_results = self.dense_retriever.retrieve(query, top_k * 2)
        sparse_results = self.sparse_retriever.retrieve(query, top_k * 2)
        
        # 2. æ„å»ºæ–‡æ¡£IDåˆ°åˆ†æ•°çš„æ˜ å°„
        doc_scores = {}
        
        # ç¨ å¯†æ£€ç´¢åˆ†æ•°
        for result in dense_results:
            doc_id = result.get('doc_id', result.get('index'))
            doc_scores[doc_id] = doc_scores.get(doc_id, {})
            doc_scores[doc_id]['dense'] = result.score
            doc_scores[doc_id]['text'] = result.text
        
        # ç¨€ç–æ£€ç´¢åˆ†æ•°
        for result in sparse_results:
            doc_id = result.get('doc_id', result.get('index'))
            doc_scores[doc_id] = doc_scores.get(doc_id, {})
            doc_scores[doc_id]['sparse'] = result['score']
            doc_scores[doc_id]['text'] = result['text']
        
        # 3. åˆ†æ•°å½’ä¸€åŒ–å’Œèåˆ
        final_results = []
        for doc_id, scores in doc_scores.items():
            dense_score = scores.get('dense', 0)
            sparse_score = scores.get('sparse', 0)
            
            # å½’ä¸€åŒ–å¤„ç†
            dense_norm = self._normalize_score(dense_score, 'cosine')
            sparse_norm = self._normalize_score(sparse_score, 'bm25')
            
            # åŠ æƒèåˆ
            final_score = self.alpha * dense_norm + (1 - self.alpha) * sparse_norm
            
            final_results.append({
                'doc_id': doc_id,
                'text': scores['text'],
                'final_score': final_score,
                'dense_score': dense_score,
                'sparse_score': sparse_score
            })
        
        # 4. æŒ‰æœ€ç»ˆåˆ†æ•°æ’åº
        final_results.sort(key=lambda x: x['final_score'], reverse=True)
        return final_results[:top_k]
    
    def _normalize_score(self, score, score_type):
        """åˆ†æ•°å½’ä¸€åŒ–"""
        if score_type == 'cosine':
            # ä½™å¼¦ç›¸ä¼¼åº¦å·²åœ¨[0,1]èŒƒå›´å†…
            return score
        elif score_type == 'bm25':
            # BM25åˆ†æ•°å½’ä¸€åŒ–åˆ°[0,1]
            return 1 / (1 + np.exp(-score))  # sigmoidå½’ä¸€åŒ–
        return score

# ä½¿ç”¨ç¤ºä¾‹
hybrid_retriever = HybridRetriever(
    dense_retriever=dense_retriever,
    sparse_retriever=sparse_retriever,
    alpha=0.7  # 70%ç¨ å¯†æ£€ç´¢ï¼Œ30%ç¨€ç–æ£€ç´¢
)

results = hybrid_retriever.retrieve("RAGç³»ç»Ÿæ¶æ„è®¾è®¡", top_k=5)
for result in results:
    print(f"ç»¼åˆåˆ†æ•°: {result['final_score']:.3f}")
    print(f"ç¨ å¯†åˆ†æ•°: {result['dense_score']:.3f}")
    print(f"ç¨€ç–åˆ†æ•°: {result['sparse_score']:.3f}")
    print(f"å†…å®¹: {result['text'][:100]}...")
    print("---")
```

#### 2. å€’æ•°æ’åèåˆï¼ˆRRFï¼‰

```python
def reciprocal_rank_fusion(results_list, k=60):
    """å€’æ•°æ’åèåˆç®—æ³•"""
    doc_scores = {}
    
    for results in results_list:
        for rank, result in enumerate(results):
            doc_id = result.get('doc_id', result.get('index'))
            
            # RRFå…¬å¼ï¼š1/(k + rank)
            rrf_score = 1 / (k + rank + 1)
            
            if doc_id in doc_scores:
                doc_scores[doc_id]['score'] += rrf_score
            else:
                doc_scores[doc_id] = {
                    'score': rrf_score,
                    'text': result.get('text', result.get('content', ''))
                }
    
    # æŒ‰RRFåˆ†æ•°æ’åº
    sorted_results = sorted(
        doc_scores.items(), 
        key=lambda x: x[1]['score'], 
        reverse=True
    )
    
    return [
        {
            'doc_id': doc_id,
            'text': data['text'],
            'rrf_score': data['score']
        }
        for doc_id, data in sorted_results
    ]

# ä½¿ç”¨ç¤ºä¾‹
dense_results = dense_retriever.retrieve("RAGæŠ€æœ¯", top_k=10)
sparse_results = sparse_retriever.retrieve("RAGæŠ€æœ¯", top_k=10)

rrf_results = reciprocal_rank_fusion([dense_results, sparse_results])
print("RRFèåˆç»“æœ:")
for result in rrf_results[:5]:
    print(f"RRFåˆ†æ•°: {result['rrf_score']:.3f}")
    print(f"å†…å®¹: {result['text'][:100]}...")
    print("---")
```

---

## ğŸš€ é«˜çº§æ£€ç´¢ç­–ç•¥

### 1. æŸ¥è¯¢æ”¹å†™ä¸æ‰©å±•

#### Query Rewriting
```python
from openai import OpenAI

class QueryRewriter:
    def __init__(self):
        self.client = OpenAI()
    
    def rewrite_query(self, original_query: str):
        """ä½¿ç”¨LLMæ”¹å†™æŸ¥è¯¢"""
        prompt = f"""
        è¯·å°†ä»¥ä¸‹ç”¨æˆ·æŸ¥è¯¢æ”¹å†™ä¸ºæ›´é€‚åˆæ£€ç´¢çš„å½¢å¼ï¼Œè¦æ±‚ï¼š
        1. ä¿æŒåŸæ„ä¸å˜
        2. ä½¿ç”¨æ›´ç²¾ç¡®çš„æŠ€æœ¯æœ¯è¯­
        3. æ‰©å±•å…³é”®æ¦‚å¿µ
        4. å¦‚æœæŸ¥è¯¢æ¨¡ç³Šï¼Œè¯·æä¾›å¤šä¸ªå¯èƒ½çš„è§£é‡Š
        
        åŸæŸ¥è¯¢ï¼š{original_query}
        
        æ”¹å†™åçš„æŸ¥è¯¢ï¼š
        """
        
        response = self.client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.3
        )
        
        return response.choices[0].message.content.strip()

# ä½¿ç”¨ç¤ºä¾‹
rewriter = QueryRewriter()

original = "RAGæ˜¯ä»€ä¹ˆ"
rewritten = rewriter.rewrite_query(original)
print(f"åŸæŸ¥è¯¢: {original}")
print(f"æ”¹å†™å: {rewritten}")

# ä½¿ç”¨æ”¹å†™åçš„æŸ¥è¯¢è¿›è¡Œæ£€ç´¢
results = retriever.retrieve(rewritten, top_k=5)
```

#### HyDEï¼ˆHypothetical Document Embeddingsï¼‰
```python
class HyDERetriever:
    def __init__(self, llm_client, embedding_model, vector_db):
        self.llm_client = llm_client
        self.embedding_model = embedding_model
        self.vector_db = vector_db
    
    def generate_hypothetical_answer(self, query: str):
        """ç”Ÿæˆå‡è®¾æ€§å›ç­”"""
        prompt = f"""
        è¯·åŸºäºä»¥ä¸‹é—®é¢˜ç”Ÿæˆä¸€ä¸ªè¯¦ç»†ã€å‡†ç¡®çš„å›ç­”ã€‚å³ä½¿ä½ ä¸ç¡®å®šç­”æ¡ˆï¼Œä¹Ÿè¦ç”Ÿæˆä¸€ä¸ªåˆç†çš„å‡è®¾æ€§å›ç­”ã€‚
        
        é—®é¢˜ï¼š{query}
        
        å›ç­”ï¼š
        """
        
        response = self.llm_client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.7
        )
        
        return response.choices[0].message.content
    
    def retrieve(self, query: str, top_k: int = 5):
        """HyDEæ£€ç´¢ç­–ç•¥"""
        # 1. ç”Ÿæˆå‡è®¾æ€§æ–‡æ¡£
        hypothetical_doc = self.generate_hypothetical_answer(query)
        
        # 2. å¯¹å‡è®¾æ€§æ–‡æ¡£è¿›è¡Œå‘é‡åŒ–
        hypo_vector = self.embedding_model.encode(hypothetical_doc)
        
        # 3. ä½¿ç”¨å‡è®¾æ€§æ–‡æ¡£å‘é‡è¿›è¡Œæ£€ç´¢
        results = self.vector_db.search(
            vector=hypo_vector,
            top_k=top_k,
            metric="cosine"
        )
        
        return results

# ä½¿ç”¨ç¤ºä¾‹
hyde_retriever = HyDERetriever(openai_client, embedding_model, vector_db)
results = hyde_retriever.retrieve("RAGç³»ç»Ÿçš„ä¼˜ç¼ºç‚¹", top_k=5)
```

### 2. å¤šè·¯å¬å›

```python
class MultiPathRetriever:
    def __init__(self, retrievers_config):
        self.retrievers = retrievers_config
    
    def multi_retrieve(self, query: str, top_k_per_path: int = 10, final_top_k: int = 5):
        """å¤šè·¯å¬å›ç­–ç•¥"""
        all_results = []
        
        # 1. å¤šç§ç­–ç•¥å¹¶è¡Œæ£€ç´¢
        for name, retriever in self.retrievers.items():
            try:
                results = retriever.retrieve(query, top_k_per_path)
                # æ·»åŠ æ¥æºæ ‡è¯†
                for result in results:
                    result['source_retriever'] = name
                all_results.extend(results)
                print(f"{name} æ£€ç´¢åˆ° {len(results)} æ¡ç»“æœ")
            except Exception as e:
                print(f"{name} æ£€ç´¢å¤±è´¥: {e}")
        
        # 2. å»é‡åˆå¹¶
        unique_results = self._deduplicate_results(all_results)
        
        # 3. é‡æ–°æ’åº
        final_results = self._rerank_results(unique_results, query)
        
        return final_results[:final_top_k]
    
    def _deduplicate_results(self, results):
        """ç»“æœå»é‡"""
        seen_texts = set()
        unique_results = []
        
        for result in results:
            text_hash = hash(result['text'][:100])  # ä½¿ç”¨å‰100å­—ç¬¦å»é‡
            if text_hash not in seen_texts:
                seen_texts.add(text_hash)
                unique_results.append(result)
        
        return unique_results
    
    def _rerank_results(self, results, query):
        """ç»“æœé‡æ’åº"""
        # è¿™é‡Œå¯ä»¥ä½¿ç”¨æ›´å¤æ‚çš„æ’åºé€»è¾‘
        # ç®€å•ç¤ºä¾‹ï¼šæŒ‰åˆ†æ•°æ’åº
        return sorted(results, key=lambda x: x.get('score', 0), reverse=True)

# é…ç½®å¤šä¸ªæ£€ç´¢å™¨
retrievers_config = {
    'dense': dense_retriever,
    'sparse': sparse_retriever,
    'hyde': hyde_retriever
}

multi_retriever = MultiPathRetriever(retrievers_config)
results = multi_retriever.multi_retrieve("RAGç³»ç»Ÿè®¾è®¡åŸåˆ™", final_top_k=5)

print("å¤šè·¯å¬å›ç»“æœ:")
for result in results:
    print(f"æ¥æº: {result['source_retriever']}")
    print(f"åˆ†æ•°: {result.get('score', 'N/A')}")
    print(f"å†…å®¹: {result['text'][:150]}...")
    print("---")
```

---

## ğŸ“Š æ£€ç´¢æ•ˆæœä¼˜åŒ–

### 1. å‚æ•°è°ƒä¼˜æŒ‡å—

| å‚æ•° | å»ºè®®å€¼ | å½±å“ | è°ƒä¼˜ç­–ç•¥ |
|------|--------|------|----------|
| **top_k** | 5-20 | å¬å›æ•°é‡ | æ ¹æ®ä¸‹æ¸¸å¤„ç†èƒ½åŠ›è°ƒæ•´ |
| **ç›¸ä¼¼åº¦é˜ˆå€¼** | 0.7-0.85 | ç»“æœè´¨é‡ | é€šè¿‡éªŒè¯é›†ç¡®å®šæœ€ä¼˜å€¼ |
| **æ··åˆæƒé‡Î±** | 0.6-0.8 | æ£€ç´¢ç­–ç•¥å¹³è¡¡ | A/Bæµ‹è¯•ç¡®å®š |
| **chunk_size** | 500-1000 | æ–‡æ¡£ç²’åº¦ | å¹³è¡¡ä¸Šä¸‹æ–‡ä¸ç²¾ç¡®æ€§ |

### 2. æ£€ç´¢è´¨é‡è¯„ä¼°

```python
class RetrievalEvaluator:
    def __init__(self, test_queries, ground_truth):
        self.test_queries = test_queries
        self.ground_truth = ground_truth
    
    def evaluate_retrieval(self, retriever, top_k=5):
        """è¯„ä¼°æ£€ç´¢æ€§èƒ½"""
        metrics = {
            'recall': [],
            'precision': [],
            'mrr': [],  # Mean Reciprocal Rank
            'ndcg': []  # Normalized Discounted Cumulative Gain
        }
        
        for query_id, query in self.test_queries.items():
            results = retriever.retrieve(query, top_k)
            relevant_docs = self.ground_truth[query_id]
            
            # è®¡ç®—å„é¡¹æŒ‡æ ‡
            retrieved_docs = [r.get('doc_id') for r in results]
            
            # Recall@K
            recall = len(set(retrieved_docs) & set(relevant_docs)) / len(relevant_docs)
            metrics['recall'].append(recall)
            
            # Precision@K
            precision = len(set(retrieved_docs) & set(relevant_docs)) / len(retrieved_docs)
            metrics['precision'].append(precision)
            
            # MRR
            mrr = self._calculate_mrr(retrieved_docs, relevant_docs)
            metrics['mrr'].append(mrr)
        
        # è®¡ç®—å¹³å‡å€¼
        avg_metrics = {k: np.mean(v) for k, v in metrics.items()}
        return avg_metrics
    
    def _calculate_mrr(self, retrieved, relevant):
        """è®¡ç®—å¹³å‡å€’æ•°æ’å"""
        for i, doc_id in enumerate(retrieved):
            if doc_id in relevant:
                return 1.0 / (i + 1)
        return 0.0

# ä½¿ç”¨ç¤ºä¾‹
test_queries = {
    'q1': 'RAGæŠ€æœ¯åŸç†',
    'q2': 'å‘é‡æ•°æ®åº“é€‰å‹',
    # ... æ›´å¤šæµ‹è¯•æŸ¥è¯¢
}

ground_truth = {
    'q1': ['doc_1', 'doc_5', 'doc_12'],  # ç›¸å…³æ–‡æ¡£ID
    'q2': ['doc_3', 'doc_8'],
    # ... å¯¹åº”çš„ç›¸å…³æ–‡æ¡£
}

evaluator = RetrievalEvaluator(test_queries, ground_truth)
metrics = evaluator.evaluate_retrieval(dense_retriever)

print("æ£€ç´¢æ€§èƒ½è¯„ä¼°:")
for metric, value in metrics.items():
    print(f"{metric.upper()}: {value:.3f}")
```

---

## âš ï¸ å¸¸è§é—®é¢˜ä¸è§£å†³

### é—®é¢˜1ï¼šæ£€ç´¢ç»“æœä¸ç›¸å…³

**ç°è±¡**ï¼šè¿”å›çš„æ–‡æ¡£ä¸æŸ¥è¯¢è¯­ä¹‰ä¸åŒ¹é…  
**åŸå› åˆ†æ**ï¼š
- Embeddingæ¨¡å‹ä¸é€‚é…
- æŸ¥è¯¢è¡¨è¾¾ä¸å‡†ç¡®
- æ–‡æ¡£åˆ‡åˆ†ç²’åº¦ä¸å½“

**è§£å†³æ–¹æ¡ˆ**ï¼š
```python
# 1. æŸ¥è¯¢é¢„å¤„ç†
def preprocess_query(query):
    """æŸ¥è¯¢é¢„å¤„ç†"""
    # å»é™¤åœç”¨è¯
    query = remove_stopwords(query)
    # æ·»åŠ ä¸Šä¸‹æ–‡ä¿¡æ¯
    if len(query.split()) < 3:
        query = f"è¯·è¯¦ç»†ä»‹ç»{query}"
    return query

# 2. ç»“æœåå¤„ç†
def postprocess_results(results, query, threshold=0.6):
    """ç»“æœåå¤„ç†"""
    filtered = []
    for result in results:
        # è¯­ä¹‰ç›¸å…³æ€§äºŒæ¬¡éªŒè¯
        if semantic_similarity(query, result['text']) > threshold:
            filtered.append(result)
    return filtered
```

### é—®é¢˜2ï¼šæ£€ç´¢é€Ÿåº¦æ…¢

**ç°è±¡**ï¼šæ£€ç´¢å“åº”æ—¶é—´è¿‡é•¿  
**ä¼˜åŒ–ç­–ç•¥**ï¼š

```python
# 1. å‘é‡ç¼“å­˜
from functools import lru_cache

@lru_cache(maxsize=1000)
def cached_embedding(text):
    return embedding_model.encode(text)

# 2. æ‰¹é‡æ£€ç´¢ä¼˜åŒ–
class BatchRetriever:
    def __init__(self, retriever, batch_size=32):
        self.retriever = retriever
        self.batch_size = batch_size
    
    def batch_retrieve(self, queries):
        results = {}
        for i in range(0, len(queries), self.batch_size):
            batch = queries[i:i + self.batch_size]
            # æ‰¹é‡å‘é‡åŒ–
            vectors = embedding_model.encode(batch)
            # æ‰¹é‡æ£€ç´¢
            for query, vector in zip(batch, vectors):
                results[query] = self.retriever.search_by_vector(vector)
        return results
```

---

## ç›¸å…³é˜…è¯»

- [RAGèŒƒå¼æ¼”è¿›](/llms/rag/paradigms) - äº†è§£RAGæŠ€æœ¯å‘å±•è„‰ç»œ
- [æ–‡æ¡£åˆ‡åˆ†ç­–ç•¥](/llms/rag/chunking) - å½±å“æ£€ç´¢ç²’åº¦çš„åˆ‡åˆ†æŠ€æœ¯
- [EmbeddingæŠ€æœ¯](/llms/rag/embedding) - ç¨ å¯†æ£€ç´¢çš„åŸºç¡€
- [å‘é‡æ•°æ®åº“](/llms/rag/vector-db) - æ£€ç´¢çš„åº•å±‚å­˜å‚¨
- [é‡æ’åºä¼˜åŒ–](/llms/rag/rerank) - æ£€ç´¢åçš„ç²¾æ’æŠ€æœ¯

> **ç›¸å…³æ–‡ç« **ï¼š
> - [é«˜çº§RAGæŠ€æœ¯å…¨æ™¯ï¼šä»åŸç†åˆ°å®æˆ˜](https://dd-ff.blog.csdn.net/article/details/149396526)
> - [ä»â€œæ‹†æ–‡æ¡£â€åˆ°â€œé€šè¯­ä¹‰â€ï¼šRAG+çŸ¥è¯†å›¾è°±å¦‚ä½•ç ´è§£å¤§æ¨¡å‹â€œå¤±å¿†+å¹»è§‰â€éš¾é¢˜ï¼Ÿ](https://dd-ff.blog.csdn.net/article/details/149354855)
> - [ä»â€œå¤±å¿†â€åˆ°â€œè¿‡ç›®ä¸å¿˜â€ï¼šRAGæŠ€æœ¯å¦‚ä½•ç»™LLMè£…ä¸Šâ€œå¤–æŒ‚å¤§è„‘â€ï¼Ÿ](https://dd-ff.blog.csdn.net/article/details/149348018)

> **å¤–éƒ¨èµ„æº**ï¼š
> - [LlamaIndexæ£€ç´¢æŒ‡å—](https://docs.llamaindex.ai/en/stable/module_guides/querying/retriever/) - æ£€ç´¢å™¨è¯¦ç»†æ–‡æ¡£
> - [LangChain Retrievers](https://python.langchain.com/docs/modules/data_connection/retrievers/) - å¤šç§æ£€ç´¢å™¨å®ç°

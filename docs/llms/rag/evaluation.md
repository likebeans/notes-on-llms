---
title: RAG è¯„ä¼°æ–¹æ³•è¯¦è§£
description: RAG ç³»ç»Ÿè¯„ä¼°æŒ‡æ ‡ã€æ¡†æ¶ä¸å®æˆ˜æ–¹æ³•
---

# RAG è¯„ä¼°æ–¹æ³•è¯¦è§£

> ç§‘å­¦è¯„ä¼°RAGç³»ç»Ÿæ•ˆæœï¼Œä»æŒ‡æ ‡è®¾è®¡åˆ°æ¡†æ¶åº”ç”¨çš„å®Œæ•´æŒ‡å—

## ğŸ¯ æ ¸å¿ƒæ¦‚å¿µ

### ä¸ºä»€ä¹ˆéœ€è¦RAGè¯„ä¼°ï¼Ÿ

RAGç³»ç»Ÿçš„å¤æ‚æ€§è¦æ±‚æˆ‘ä»¬å»ºç«‹**ç§‘å­¦ã€å…¨é¢çš„è¯„ä¼°ä½“ç³»**æ¥è¡¡é‡å…¶æ•ˆæœï¼š

- **å¤šç»„ä»¶ç³»ç»Ÿ**ï¼šæ£€ç´¢å™¨+ç”Ÿæˆå™¨çš„è”åˆä¼˜åŒ–éœ€è¦åˆ†åˆ«å’Œæ•´ä½“è¯„ä¼°
- **è´¨é‡æ§åˆ¶**ï¼šç¡®ä¿ç³»ç»Ÿåœ¨ç”Ÿäº§ç¯å¢ƒä¸­çš„ç¨³å®šæ€§å’Œå¯é æ€§  
- **æŒç»­æ”¹è¿›**ï¼šé€šè¿‡é‡åŒ–æŒ‡æ ‡æŒ‡å¯¼ç³»ç»Ÿä¼˜åŒ–æ–¹å‘
- **ä¸šåŠ¡ä»·å€¼**ï¼šå°†æŠ€æœ¯æŒ‡æ ‡ä¸ä¸šåŠ¡ç›®æ ‡å¯¹é½

### è¯„ä¼°çš„æ ¸å¿ƒæŒ‘æˆ˜

::: warning å…³é”®éš¾ç‚¹
**ä¸»è§‚æ€§å¼º**ï¼šæ–‡æœ¬è´¨é‡è¯„ä¼°å¾€å¾€å¸¦æœ‰ä¸»è§‚è‰²å½©  
**å¤šç»´åº¦æƒè¡¡**ï¼šå‡†ç¡®æ€§ã€ç›¸å…³æ€§ã€æµç•…æ€§éœ€è¦ç»¼åˆè€ƒè™‘  
**æˆæœ¬é«˜æ˜‚**ï¼šäººå·¥æ ‡æ³¨å’Œè¯„ä¼°æˆæœ¬è¾ƒé«˜  
**åŠ¨æ€å˜åŒ–**ï¼šç”¨æˆ·éœ€æ±‚å’Œæ•°æ®åˆ†å¸ƒéšæ—¶é—´å˜åŒ–
:::

### RAGç³»ç»Ÿ12ä¸ªå¸¸è§ç—›ç‚¹åŠè§£å†³æ–¹æ¡ˆ

> æ¥æºï¼š[RAGæŠ€æœ¯çš„5ç§èŒƒå¼](https://hub.baai.ac.cn/view/43613)

| ç—›ç‚¹ | é—®é¢˜æè¿° | è§£å†³æ–¹æ¡ˆ |
|------|----------|----------|
| **1. å†…å®¹ç¼ºå¤±** | çŸ¥è¯†åº“ç¼ºå°‘ä¸Šä¸‹æ–‡æ—¶è¿”å›ä¼¼æ˜¯è€Œéçš„ç­”æ¡ˆ | æ¸…ç†æ•°æ®ã€ç²¾å¿ƒè®¾è®¡æç¤ºè¯ |
| **2. é”™è¿‡é‡è¦æ–‡æ¡£** | å…³é”®æ–‡æ¡£æœªå‡ºç°åœ¨Topç»“æœä¸­ | è°ƒæ•´æ£€ç´¢ç­–ç•¥ã€Embeddingæ¨¡å‹è°ƒä¼˜ |
| **3. ä¸Šä¸‹æ–‡æ•´åˆé™åˆ¶** | æ•´åˆé•¿åº¦è¶…è¿‡LLMçª—å£å¤§å° | è°ƒæ•´æ£€ç´¢ç­–ç•¥ã€ä¸Šä¸‹æ–‡å‹ç¼© |
| **4. ä¿¡æ¯æœªæå–** | æ–‡æ¡£ä¸­çš„å…³é”®ä¿¡æ¯æœªè¢«æå– | æ•°æ®æ¸…æ´—ã€æç¤ºè¯å‹ç¼©ã€é•¿å†…å®¹ä¼˜å…ˆæ’åº |
| **5. æ ¼å¼é”™è¯¯** | è¾“å‡ºæ ¼å¼ä¸é¢„æœŸä¸ç¬¦ | æ”¹è¿›æç¤ºè¯ã€æ ¼å¼åŒ–è¾“å‡ºã€ä½¿ç”¨JSONæ¨¡å¼ |
| **6. ç­”æ¡ˆä¸æ­£ç¡®** | ç¼ºä¹å…·ä½“ç»†èŠ‚å¯¼è‡´é”™è¯¯ | é‡‡ç”¨å…ˆè¿›æ£€ç´¢ç­–ç•¥ã€å¤šè·¯å¬å› |
| **7. å›ç­”ä¸å®Œæ•´** | ç­”æ¡ˆä¸å¤Ÿå…¨é¢ | æŸ¥è¯¢è½¬æ¢ã€é—®é¢˜ç»†åˆ† |
| **8. å¯æ‰©å±•æ€§é—®é¢˜** | æ•°æ®æ‘„å…¥æ€§èƒ½ç“¶é¢ˆ | å¹¶è¡Œå¤„ç†ã€æå‡å¤„ç†é€Ÿåº¦ |
| **9. ç»“æ„åŒ–æ•°æ®QA** | è¡¨æ ¼ç­‰ç»“æ„åŒ–æ•°æ®å¤„ç†å›°éš¾ | é“¾å¼æ€ç»´ã€æ··åˆæŸ¥è¯¢å¼•æ“ |
| **10. å¤æ‚PDFæå–** | å¤æ‚å¸ƒå±€PDFå¤„ç†å›°éš¾ | åµŒå…¥å¼è¡¨æ ¼æ£€ç´¢ã€LayoutLM |
| **11. åå¤‡æ¨¡å‹ç­–ç•¥** | ç¼ºå°‘fallbackæœºåˆ¶ | Neutrinoè·¯ç”±å™¨ã€OpenRouter |
| **12. LLMå®‰å…¨æ€§** | å®‰å…¨é˜²æŠ¤é—®é¢˜ | å†…å®¹å®¡æ ¸ã€è¾“å…¥éªŒè¯ã€è¾“å‡ºè¿‡æ»¤ |

---

## ğŸ“Š RAGè¯„ä¼°ä½“ç³»æ¶æ„

> åŸºäº[ã€Šæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿç»¼åˆè¯„ä¼°ï¼šä»æ ¸å¿ƒæŒ‡æ ‡åˆ°å‰æ²¿æ¡†æ¶ã€‹](https://dd-ff.blog.csdn.net/article/details/152823514)

### ä¸‰å±‚è¯„ä¼°ç»“æ„

```python
# RAGè¯„ä¼°çš„ä¸‰ä¸ªå±‚æ¬¡
RAGç³»ç»Ÿè¯„ä¼° = {
    "æ£€ç´¢å±‚è¯„ä¼°": "è¯„ä¼°æ£€ç´¢ç»„ä»¶çš„æ•ˆæœ",
    "ç”Ÿæˆå±‚è¯„ä¼°": "è¯„ä¼°ç”Ÿæˆç»„ä»¶çš„è´¨é‡", 
    "ç«¯åˆ°ç«¯è¯„ä¼°": "è¯„ä¼°æ•´ä½“ç³»ç»Ÿæ€§èƒ½"
}
```

| è¯„ä¼°å±‚æ¬¡ | å…³æ³¨ç‚¹ | å…¸å‹æŒ‡æ ‡ | è¯„ä¼°æ–¹æ³• |
|----------|--------|----------|----------|
| **æ£€ç´¢å±‚** | ç›¸å…³æ–‡æ¡£å¬å›è´¨é‡ | Recall@K, MRR, NDCG | ç¦»çº¿è¯„ä¼° |
| **ç”Ÿæˆå±‚** | ç­”æ¡ˆè´¨é‡ä¸å¿ å®åº¦ | Faithfulness, Relevance | LLM-Judge |
| **ç«¯åˆ°ç«¯** | ç”¨æˆ·æ»¡æ„åº¦ | Answer Accuracy, F1 | åœ¨çº¿A/Bæµ‹è¯• |

---

## ğŸ” æ£€ç´¢å±‚è¯„ä¼°

### æ ¸å¿ƒæŒ‡æ ‡è¯¦è§£

#### 1. å¬å›ç‡ï¼ˆRecall@Kï¼‰
```python
def recall_at_k(relevant_docs, retrieved_docs, k):
    """è®¡ç®—Recall@KæŒ‡æ ‡"""
    retrieved_k = retrieved_docs[:k]
    relevant_retrieved = set(retrieved_k) & set(relevant_docs)
    return len(relevant_retrieved) / len(relevant_docs)

# ç¤ºä¾‹
relevant_docs = ['doc1', 'doc3', 'doc5', 'doc7']  # ç›¸å…³æ–‡æ¡£
retrieved_docs = ['doc1', 'doc2', 'doc3', 'doc4', 'doc5']  # æ£€ç´¢ç»“æœ

recall_5 = recall_at_k(relevant_docs, retrieved_docs, k=5)
print(f"Recall@5: {recall_5:.3f}")  # è¾“å‡ºï¼š0.750
```

#### 2. å¹³å‡å€’æ•°æ’åï¼ˆMRRï¼‰
```python
def mean_reciprocal_rank(queries_results):
    """è®¡ç®—å¤šæŸ¥è¯¢çš„å¹³å‡å€’æ•°æ’å"""
    total_rr = 0
    valid_queries = 0
    
    for relevant_docs, retrieved_docs in queries_results:
        rr = 0
        for i, doc in enumerate(retrieved_docs):
            if doc in relevant_docs:
                rr = 1 / (i + 1)  # ç¬¬ä¸€ä¸ªç›¸å…³æ–‡æ¡£çš„å€’æ•°æ’å
                break
        total_rr += rr
        valid_queries += 1
    
    return total_rr / valid_queries if valid_queries > 0 else 0

# ç¤ºä¾‹
queries_data = [
    (['doc1', 'doc3'], ['doc2', 'doc1', 'doc4']),  # ç¬¬ä¸€ä¸ªæŸ¥è¯¢
    (['doc5'], ['doc5', 'doc6', 'doc7']),           # ç¬¬äºŒä¸ªæŸ¥è¯¢
]

mrr = mean_reciprocal_rank(queries_data)
print(f"MRR: {mrr:.3f}")
```

#### 3. å½’ä¸€åŒ–æŠ˜æ‰£ç´¯ç§¯å¢ç›Šï¼ˆNDCGï¼‰
```python
import numpy as np

def dcg_at_k(relevance_scores, k):
    """è®¡ç®—DCG@K"""
    relevance_scores = np.array(relevance_scores[:k])
    if relevance_scores.size:
        return np.sum(relevance_scores / np.log2(np.arange(2, relevance_scores.size + 2)))
    return 0

def ndcg_at_k(relevant_scores, retrieved_scores, k):
    """è®¡ç®—NDCG@K"""
    dcg = dcg_at_k(retrieved_scores, k)
    idcg = dcg_at_k(sorted(relevant_scores, reverse=True), k)
    return dcg / idcg if idcg > 0 else 0

# ç¤ºä¾‹ï¼šç›¸å…³æ€§åˆ†æ•°ï¼ˆ0-3åˆ†ï¼‰
relevant_scores = [3, 2, 3, 1, 2]  # ç†æƒ³æ’åºçš„ç›¸å…³æ€§
retrieved_scores = [3, 1, 2, 3, 0]  # å®é™…æ£€ç´¢çš„ç›¸å…³æ€§

ndcg_5 = ndcg_at_k(relevant_scores, retrieved_scores, k=5)
print(f"NDCG@5: {ndcg_5:.3f}")
```

### å®æˆ˜è¯„ä¼°ä»£ç 

```python
class RetrievalEvaluator:
    def __init__(self, ground_truth_path):
        """
        ground_truth_path: æ ‡å‡†ç­”æ¡ˆæ–‡ä»¶è·¯å¾„
        æ ¼å¼: {
            "query_id": {
                "query": "æŸ¥è¯¢æ–‡æœ¬",
                "relevant_docs": ["doc1", "doc2", ...]
            }
        }
        """
        with open(ground_truth_path, 'r', encoding='utf-8') as f:
            self.ground_truth = json.load(f)
    
    def evaluate_retriever(self, retriever, top_k=10):
        """è¯„ä¼°æ£€ç´¢å™¨æ€§èƒ½"""
        metrics = {
            'recall': [],
            'precision': [],
            'mrr': [],
            'ndcg': []
        }
        
        for query_id, data in self.ground_truth.items():
            query = data['query']
            relevant_docs = data['relevant_docs']
            
            # æ‰§è¡Œæ£€ç´¢
            results = retriever.retrieve(query, top_k)
            retrieved_docs = [r['doc_id'] for r in results]
            
            # è®¡ç®—æŒ‡æ ‡
            recall = self._calculate_recall(relevant_docs, retrieved_docs)
            precision = self._calculate_precision(relevant_docs, retrieved_docs)
            mrr = self._calculate_single_mrr(relevant_docs, retrieved_docs)
            
            metrics['recall'].append(recall)
            metrics['precision'].append(precision)
            metrics['mrr'].append(mrr)
        
        # è®¡ç®—å¹³å‡å€¼
        avg_metrics = {k: np.mean(v) for k, v in metrics.items()}
        return avg_metrics
    
    def _calculate_recall(self, relevant, retrieved):
        if not relevant:
            return 0
        return len(set(relevant) & set(retrieved)) / len(relevant)
    
    def _calculate_precision(self, relevant, retrieved):
        if not retrieved:
            return 0
        return len(set(relevant) & set(retrieved)) / len(retrieved)
    
    def _calculate_single_mrr(self, relevant, retrieved):
        for i, doc in enumerate(retrieved):
            if doc in relevant:
                return 1 / (i + 1)
        return 0

# ä½¿ç”¨ç¤ºä¾‹
evaluator = RetrievalEvaluator('ground_truth.json')
metrics = evaluator.evaluate_retriever(my_retriever)

print("æ£€ç´¢è¯„ä¼°ç»“æœ:")
for metric, value in metrics.items():
    print(f"{metric.upper()}: {value:.3f}")
```

---

## ğŸ“ ç”Ÿæˆå±‚è¯„ä¼°

### å…³é”®æŒ‡æ ‡ä½“ç³»

#### 1. å¿ å®åº¦ï¼ˆFaithfulnessï¼‰
è¯„ä¼°ç”Ÿæˆå†…å®¹æ˜¯å¦å¿ å®äºæ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ï¼š

```python
from openai import OpenAI

class FaithfulnessEvaluator:
    def __init__(self):
        self.client = OpenAI()
    
    def evaluate_faithfulness(self, context: str, generated_answer: str):
        """è¯„ä¼°ç­”æ¡ˆå¯¹ä¸Šä¸‹æ–‡çš„å¿ å®åº¦"""
        prompt = f"""
è¯·è¯„ä¼°ä»¥ä¸‹ç”Ÿæˆçš„ç­”æ¡ˆæ˜¯å¦å¿ å®äºç»™å®šçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚

ä¸Šä¸‹æ–‡ï¼š
{context}

ç”Ÿæˆçš„ç­”æ¡ˆï¼š
{generated_answer}

è¯„ä¼°æ ‡å‡†ï¼š
1. ç­”æ¡ˆä¸­çš„äº‹å®æ˜¯å¦éƒ½èƒ½åœ¨ä¸Šä¸‹æ–‡ä¸­æ‰¾åˆ°æ”¯æ’‘
2. æ˜¯å¦å­˜åœ¨ä¸ä¸Šä¸‹æ–‡çŸ›ç›¾çš„ä¿¡æ¯
3. æ˜¯å¦æ·»åŠ äº†ä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰çš„ä¿¡æ¯

è¯·ç»™å‡º0-1ä¹‹é—´çš„åˆ†æ•°ï¼Œå…¶ä¸­ï¼š
- 1.0ï¼šå®Œå…¨å¿ å®ï¼Œæ‰€æœ‰ä¿¡æ¯éƒ½æ¥è‡ªä¸Šä¸‹æ–‡
- 0.8ï¼šåŸºæœ¬å¿ å®ï¼Œå°‘é‡åˆç†æ¨ç†
- 0.6ï¼šéƒ¨åˆ†å¿ å®ï¼Œæœ‰ä¸€äº›ä¸å‡†ç¡®ä¿¡æ¯
- 0.4ï¼šè¾ƒå¤šä¸å‡†ç¡®ä¿¡æ¯
- 0.2ï¼šå¤§é‡é”™è¯¯ä¿¡æ¯
- 0.0ï¼šå®Œå…¨ä¸å¿ å®æˆ–æ— å…³

åˆ†æ•°ï¼š"""
        
        response = self.client.chat.completions.create(
            model="gpt-4",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.1
        )
        
        # æå–åˆ†æ•°
        import re
        score_text = response.choices[0].message.content
        score_match = re.search(r'åˆ†æ•°[ï¼š:]\s*([0-9.]+)', score_text)
        
        if score_match:
            return float(score_match.group(1))
        return 0.5  # é»˜è®¤å€¼

# ä½¿ç”¨ç¤ºä¾‹
evaluator = FaithfulnessEvaluator()
context = "RAGæŠ€æœ¯ç»“åˆäº†æ£€ç´¢å’Œç”Ÿæˆï¼Œèƒ½å¤Ÿè·å–å®æ—¶ä¿¡æ¯..."
answer = "RAGæ˜¯ä¸€ç§å°†ä¿¡æ¯æ£€ç´¢ä¸æ–‡æœ¬ç”Ÿæˆç›¸ç»“åˆçš„æŠ€æœ¯..."

faithfulness_score = evaluator.evaluate_faithfulness(context, answer)
print(f"å¿ å®åº¦åˆ†æ•°: {faithfulness_score:.2f}")
```

#### 2. ç›¸å…³æ€§ï¼ˆRelevanceï¼‰
è¯„ä¼°ç­”æ¡ˆä¸ç”¨æˆ·æŸ¥è¯¢çš„ç›¸å…³ç¨‹åº¦ï¼š

```python
class RelevanceEvaluator:
    def __init__(self):
        self.client = OpenAI()
    
    def evaluate_relevance(self, query: str, generated_answer: str):
        """è¯„ä¼°ç­”æ¡ˆä¸æŸ¥è¯¢çš„ç›¸å…³æ€§"""
        prompt = f"""
è¯·è¯„ä¼°ç”Ÿæˆçš„ç­”æ¡ˆä¸ç”¨æˆ·æŸ¥è¯¢çš„ç›¸å…³æ€§ã€‚

ç”¨æˆ·æŸ¥è¯¢ï¼š
{query}

ç”Ÿæˆçš„ç­”æ¡ˆï¼š
{generated_answer}

è¯„ä¼°æ ‡å‡†ï¼š
1. ç­”æ¡ˆæ˜¯å¦ç›´æ¥å›åº”äº†ç”¨æˆ·çš„é—®é¢˜
2. ç­”æ¡ˆæ˜¯å¦åŒ…å«ç”¨æˆ·éœ€è¦çš„æ ¸å¿ƒä¿¡æ¯
3. ç­”æ¡ˆçš„è¯¦ç»†ç¨‹åº¦æ˜¯å¦é€‚å½“

è¯·ç»™å‡º0-1ä¹‹é—´çš„åˆ†æ•°ï¼š
- 1.0ï¼šå®Œå…¨ç›¸å…³ï¼Œç›´æ¥å›ç­”é—®é¢˜
- 0.8ï¼šé«˜åº¦ç›¸å…³ï¼ŒåŸºæœ¬å›ç­”é—®é¢˜
- 0.6ï¼šéƒ¨åˆ†ç›¸å…³ï¼Œå›ç­”äº†éƒ¨åˆ†é—®é¢˜
- 0.4ï¼šç›¸å…³æ€§è¾ƒä½
- 0.2ï¼šç›¸å…³æ€§å¾ˆä½
- 0.0ï¼šå®Œå…¨ä¸ç›¸å…³

åˆ†æ•°ï¼š"""
        
        response = self.client.chat.completions.create(
            model="gpt-4",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.1
        )
        
        # æå–åˆ†æ•°çš„é€»è¾‘åŒä¸Š
        # ...
        
# æ‰¹é‡è¯„ä¼°å·¥å…·
class BatchGenerationEvaluator:
    def __init__(self):
        self.faithfulness_evaluator = FaithfulnessEvaluator()
        self.relevance_evaluator = RelevanceEvaluator()
    
    def evaluate_batch(self, test_cases):
        """æ‰¹é‡è¯„ä¼°ç”Ÿæˆè´¨é‡"""
        results = []
        
        for case in test_cases:
            query = case['query']
            context = case['context']
            generated_answer = case['generated_answer']
            
            faithfulness = self.faithfulness_evaluator.evaluate_faithfulness(
                context, generated_answer
            )
            relevance = self.relevance_evaluator.evaluate_relevance(
                query, generated_answer
            )
            
            results.append({
                'query': query,
                'faithfulness': faithfulness,
                'relevance': relevance,
                'overall': (faithfulness + relevance) / 2
            })
        
        return results

# ä½¿ç”¨ç¤ºä¾‹
test_cases = [
    {
        'query': 'ä»€ä¹ˆæ˜¯RAGæŠ€æœ¯ï¼Ÿ',
        'context': 'RAGæŠ€æœ¯æ–‡æ¡£å†…å®¹...',
        'generated_answer': 'RAGæ˜¯æ£€ç´¢å¢å¼ºç”ŸæˆæŠ€æœ¯...'
    }
    # æ›´å¤šæµ‹è¯•ç”¨ä¾‹...
]

batch_evaluator = BatchGenerationEvaluator()
results = batch_evaluator.evaluate_batch(test_cases)

avg_faithfulness = np.mean([r['faithfulness'] for r in results])
avg_relevance = np.mean([r['relevance'] for r in results])
print(f"å¹³å‡å¿ å®åº¦: {avg_faithfulness:.3f}")
print(f"å¹³å‡ç›¸å…³æ€§: {avg_relevance:.3f}")
```

---

## ğŸ”„ ç«¯åˆ°ç«¯è¯„ä¼°

### ç»¼åˆè¯„ä¼°æŒ‡æ ‡

#### 1. ç­”æ¡ˆå‡†ç¡®ç‡ï¼ˆAnswer Accuracyï¼‰
```python
class AnswerAccuracyEvaluator:
    def __init__(self):
        self.client = OpenAI()
    
    def evaluate_accuracy(self, query: str, generated_answer: str, ground_truth: str):
        """è¯„ä¼°ç­”æ¡ˆå‡†ç¡®æ€§"""
        prompt = f"""
è¯·æ¯”è¾ƒç”Ÿæˆç­”æ¡ˆä¸æ ‡å‡†ç­”æ¡ˆçš„å‡†ç¡®æ€§ã€‚

é—®é¢˜ï¼š{query}

ç”Ÿæˆç­”æ¡ˆï¼š{generated_answer}

æ ‡å‡†ç­”æ¡ˆï¼š{ground_truth}

è¯·åˆ¤æ–­ç”Ÿæˆç­”æ¡ˆæ˜¯å¦æ­£ç¡®ï¼Œç»™å‡ºåˆ†æ•°ï¼š
- 1ï¼šå®Œå…¨æ­£ç¡®
- 0.8ï¼šåŸºæœ¬æ­£ç¡®ï¼Œæœ‰ç»†å¾®å·®å¼‚
- 0.6ï¼šéƒ¨åˆ†æ­£ç¡®
- 0.4ï¼šæœ‰è¾ƒå¤šé”™è¯¯
- 0.2ï¼šå¤§éƒ¨åˆ†é”™è¯¯
- 0ï¼šå®Œå…¨é”™è¯¯

åˆ†æ•°ï¼š"""
        
        # LLMè¯„ä¼°é€»è¾‘...
        
    def calculate_accuracy_metrics(self, predictions, ground_truths):
        """è®¡ç®—å‡†ç¡®ç‡ç›¸å…³æŒ‡æ ‡"""
        exact_matches = []
        f1_scores = []
        
        for pred, gt in zip(predictions, ground_truths):
            # ç²¾ç¡®åŒ¹é…
            exact_match = 1 if pred.strip().lower() == gt.strip().lower() else 0
            exact_matches.append(exact_match)
            
            # F1åˆ†æ•°ï¼ˆåŸºäºè¯çº§åˆ«ï¼‰
            f1 = self._calculate_f1(pred, gt)
            f1_scores.append(f1)
        
        return {
            'exact_match': np.mean(exact_matches),
            'f1_score': np.mean(f1_scores)
        }
    
    def _calculate_f1(self, prediction, ground_truth):
        """è®¡ç®—F1åˆ†æ•°"""
        pred_tokens = set(prediction.lower().split())
        gt_tokens = set(ground_truth.lower().split())
        
        if len(pred_tokens) == 0:
            return 0
        
        common_tokens = pred_tokens & gt_tokens
        precision = len(common_tokens) / len(pred_tokens)
        recall = len(common_tokens) / len(gt_tokens) if len(gt_tokens) > 0 else 0
        
        if precision + recall == 0:
            return 0
        
        return 2 * (precision * recall) / (precision + recall)
```

#### 2. ç”¨æˆ·æ»¡æ„åº¦è¯„ä¼°
```python
class UserSatisfactionEvaluator:
    def __init__(self):
        self.satisfaction_history = []
    
    def collect_feedback(self, query: str, answer: str, user_rating: int, 
                        feedback_text: str = ""):
        """æ”¶é›†ç”¨æˆ·åé¦ˆ"""
        feedback = {
            'timestamp': datetime.now(),
            'query': query,
            'answer': answer,
            'rating': user_rating,  # 1-5åˆ†
            'feedback': feedback_text
        }
        self.satisfaction_history.append(feedback)
    
    def calculate_satisfaction_metrics(self, time_window_days=30):
        """è®¡ç®—æ»¡æ„åº¦æŒ‡æ ‡"""
        cutoff_date = datetime.now() - timedelta(days=time_window_days)
        recent_feedback = [
            f for f in self.satisfaction_history 
            if f['timestamp'] > cutoff_date
        ]
        
        if not recent_feedback:
            return None
        
        ratings = [f['rating'] for f in recent_feedback]
        
        return {
            'avg_rating': np.mean(ratings),
            'satisfaction_rate': len([r for r in ratings if r >= 4]) / len(ratings),
            'total_responses': len(recent_feedback),
            'rating_distribution': {
                i: ratings.count(i) for i in range(1, 6)
            }
        }
```

---

## ğŸ› ï¸ è¯„ä¼°æ¡†æ¶å®æˆ˜

### 1. RAGAsæ¡†æ¶ä½¿ç”¨

```python
from ragas import evaluate
from ragas.metrics import (
    faithfulness,
    answer_relevancy,
    context_recall,
    context_precision,
)

class RAGAsEvaluator:
    def __init__(self):
        self.metrics = [
            faithfulness,
            answer_relevancy, 
            context_recall,
            context_precision
        ]
    
    def evaluate_with_ragas(self, dataset):
        """ä½¿ç”¨RAGAsè¿›è¡Œè¯„ä¼°"""
        # datasetæ ¼å¼ï¼š
        # {
        #     'question': [...],
        #     'contexts': [...],  # æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡åˆ—è¡¨
        #     'answer': [...],    # ç”Ÿæˆçš„ç­”æ¡ˆ
        #     'ground_truths': [...] # æ ‡å‡†ç­”æ¡ˆ
        # }
        
        results = evaluate(
            dataset=dataset,
            metrics=self.metrics
        )
        
        return results.to_pandas()

# ä½¿ç”¨ç¤ºä¾‹
evaluator = RAGAsEvaluator()

# å‡†å¤‡æ•°æ®é›†
eval_dataset = {
    'question': ['ä»€ä¹ˆæ˜¯RAGæŠ€æœ¯ï¼Ÿ'],
    'contexts': [['RAGæ˜¯æ£€ç´¢å¢å¼ºç”ŸæˆæŠ€æœ¯ï¼Œç»“åˆäº†æ£€ç´¢å’Œç”Ÿæˆ...']],
    'answer': ['RAGæŠ€æœ¯æ˜¯ä¸€ç§ç»“åˆæ£€ç´¢å’Œç”Ÿæˆçš„AIæŠ€æœ¯...'],
    'ground_truths': [['RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰æ˜¯ä¸€ç§AIæŠ€æœ¯...']]
}

results_df = evaluator.evaluate_with_ragas(eval_dataset)
print("RAGAsè¯„ä¼°ç»“æœï¼š")
print(results_df.describe())
```

### 2. è‡ªå®šä¹‰è¯„ä¼°æµæ°´çº¿

```python
class ComprehensiveRAGEvaluator:
    def __init__(self, config):
        self.retrieval_evaluator = RetrievalEvaluator(config['ground_truth_path'])
        self.generation_evaluator = BatchGenerationEvaluator()
        self.answer_evaluator = AnswerAccuracyEvaluator()
        
    def full_evaluation(self, rag_system, test_queries):
        """å®Œæ•´RAGç³»ç»Ÿè¯„ä¼°"""
        results = {
            'retrieval_metrics': {},
            'generation_metrics': {},
            'end_to_end_metrics': {},
            'detailed_results': []
        }
        
        for query_data in test_queries:
            query = query_data['query']
            expected_docs = query_data.get('relevant_docs', [])
            ground_truth_answer = query_data.get('ground_truth', '')
            
            # 1. æ‰§è¡ŒRAGæµç¨‹
            retrieved_docs = rag_system.retrieve(query)
            generated_answer = rag_system.generate(query, retrieved_docs)
            
            # 2. æ£€ç´¢è¯„ä¼°
            if expected_docs:
                retrieval_recall = self._calculate_recall(
                    expected_docs, [d['id'] for d in retrieved_docs]
                )
            
            # 3. ç”Ÿæˆè¯„ä¼°
            context = ' '.join([doc['text'] for doc in retrieved_docs])
            faithfulness = self.generation_evaluator.faithfulness_evaluator.evaluate_faithfulness(
                context, generated_answer
            )
            relevance = self.generation_evaluator.relevance_evaluator.evaluate_relevance(
                query, generated_answer  
            )
            
            # 4. ç«¯åˆ°ç«¯è¯„ä¼°
            if ground_truth_answer:
                accuracy = self.answer_evaluator.evaluate_accuracy(
                    query, generated_answer, ground_truth_answer
                )
            
            # è®°å½•è¯¦ç»†ç»“æœ
            results['detailed_results'].append({
                'query': query,
                'retrieval_recall': retrieval_recall if expected_docs else None,
                'faithfulness': faithfulness,
                'relevance': relevance,
                'accuracy': accuracy if ground_truth_answer else None,
                'generated_answer': generated_answer
            })
        
        # è®¡ç®—æ±‡æ€»æŒ‡æ ‡
        results['retrieval_metrics'] = self._summarize_retrieval_metrics(results['detailed_results'])
        results['generation_metrics'] = self._summarize_generation_metrics(results['detailed_results'])
        results['end_to_end_metrics'] = self._summarize_e2e_metrics(results['detailed_results'])
        
        return results
    
    def _summarize_retrieval_metrics(self, detailed_results):
        recalls = [r['retrieval_recall'] for r in detailed_results if r['retrieval_recall'] is not None]
        return {'avg_recall': np.mean(recalls)} if recalls else {}
    
    def _summarize_generation_metrics(self, detailed_results):
        faithfulness_scores = [r['faithfulness'] for r in detailed_results]
        relevance_scores = [r['relevance'] for r in detailed_results]
        return {
            'avg_faithfulness': np.mean(faithfulness_scores),
            'avg_relevance': np.mean(relevance_scores)
        }
    
    def _summarize_e2e_metrics(self, detailed_results):
        accuracy_scores = [r['accuracy'] for r in detailed_results if r['accuracy'] is not None]
        return {'avg_accuracy': np.mean(accuracy_scores)} if accuracy_scores else {}

# ä½¿ç”¨ç¤ºä¾‹
config = {'ground_truth_path': 'test_data.json'}
evaluator = ComprehensiveRAGEvaluator(config)

test_queries = [
    {
        'query': 'ä»€ä¹ˆæ˜¯RAGæŠ€æœ¯ï¼Ÿ',
        'relevant_docs': ['doc1', 'doc3'],
        'ground_truth': 'RAGæ˜¯æ£€ç´¢å¢å¼ºç”ŸæˆæŠ€æœ¯...'
    }
    # æ›´å¤šæµ‹è¯•æŸ¥è¯¢...
]

evaluation_results = evaluator.full_evaluation(my_rag_system, test_queries)

print("å®Œæ•´è¯„ä¼°ç»“æœ:")
print(f"æ£€ç´¢æŒ‡æ ‡: {evaluation_results['retrieval_metrics']}")
print(f"ç”ŸæˆæŒ‡æ ‡: {evaluation_results['generation_metrics']}")
print(f"ç«¯åˆ°ç«¯æŒ‡æ ‡: {evaluation_results['end_to_end_metrics']}")
```

---

## ğŸ“ˆ æŒç»­è¯„ä¼°ä¸ç›‘æ§

### 1. åœ¨çº¿è¯„ä¼°ç³»ç»Ÿ

```python
class OnlineRAGMonitor:
    def __init__(self, rag_system):
        self.rag_system = rag_system
        self.metrics_buffer = []
        
    def log_interaction(self, query: str, answer: str, user_feedback: dict):
        """è®°å½•ç”¨æˆ·äº¤äº’"""
        interaction = {
            'timestamp': datetime.now(),
            'query': query,
            'answer': answer,
            'feedback': user_feedback,
            'response_time': user_feedback.get('response_time', 0)
        }
        self.metrics_buffer.append(interaction)
        
        # å®šæœŸåˆ†æ
        if len(self.metrics_buffer) >= 100:
            self._analyze_recent_performance()
    
    def _analyze_recent_performance(self):
        """åˆ†ææœ€è¿‘æ€§èƒ½"""
        recent_interactions = self.metrics_buffer[-100:]
        
        # è®¡ç®—å…³é”®æŒ‡æ ‡
        avg_rating = np.mean([i['feedback'].get('rating', 0) for i in recent_interactions])
        avg_response_time = np.mean([i['response_time'] for i in recent_interactions])
        
        # æ£€æµ‹å¼‚å¸¸
        if avg_rating < 3.5:
            self._alert_low_satisfaction()
        if avg_response_time > 5.0:
            self._alert_slow_response()
    
    def _alert_low_satisfaction(self):
        """ä½æ»¡æ„åº¦å‘Šè­¦"""
        print("âš ï¸ è­¦å‘Šï¼šç”¨æˆ·æ»¡æ„åº¦ä¸‹é™")
    
    def _alert_slow_response(self):
        """å“åº”æ…¢å‘Šè­¦"""
        print("âš ï¸ è­¦å‘Šï¼šç³»ç»Ÿå“åº”æ—¶é—´è¿‡é•¿")

# 2. A/Bæµ‹è¯•æ¡†æ¶
class RAGABTester:
    def __init__(self, system_a, system_b):
        self.system_a = system_a
        self.system_b = system_b
        self.results = {'A': [], 'B': []}
    
    def run_test(self, queries, traffic_split=0.5):
        """è¿è¡ŒA/Bæµ‹è¯•"""
        for query in queries:
            # éšæœºåˆ†é…æµé‡
            if random.random() < traffic_split:
                result = self._test_system('A', self.system_a, query)
                self.results['A'].append(result)
            else:
                result = self._test_system('B', self.system_b, query)
                self.results['B'].append(result)
    
    def _test_system(self, version, system, query):
        start_time = time.time()
        answer = system.generate_answer(query)
        response_time = time.time() - start_time
        
        return {
            'query': query,
            'answer': answer,
            'response_time': response_time,
            'version': version
        }
    
    def analyze_results(self):
        """åˆ†æA/Bæµ‹è¯•ç»“æœ"""
        metrics_a = self._calculate_metrics(self.results['A'])
        metrics_b = self._calculate_metrics(self.results['B'])
        
        # ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ
        from scipy.stats import ttest_ind
        
        times_a = [r['response_time'] for r in self.results['A']]
        times_b = [r['response_time'] for r in self.results['B']]
        
        t_stat, p_value = ttest_ind(times_a, times_b)
        
        return {
            'system_a_metrics': metrics_a,
            'system_b_metrics': metrics_b,
            'significance_test': {
                't_statistic': t_stat,
                'p_value': p_value,
                'significant': p_value < 0.05
            }
        }
```

---

## ğŸ”— ç›¸å…³é˜…è¯»

- [RAGèŒƒå¼æ¼”è¿›](/llms/rag/paradigms) - äº†è§£RAGæŠ€æœ¯å‘å±•è„‰ç»œ
- [æ£€ç´¢ç­–ç•¥ä¼˜åŒ–](/llms/rag/retrieval) - æ£€ç´¢ç»„ä»¶çš„ä¼˜åŒ–æ–¹æ³•
- [é‡æ’åºæŠ€æœ¯](/llms/rag/rerank) - æå‡æ£€ç´¢ç²¾åº¦çš„æŠ€æœ¯
- [ç”Ÿäº§å®è·µæŒ‡å—](/llms/rag/production) - è¯„ä¼°åœ¨ç”Ÿäº§ç¯å¢ƒä¸­çš„åº”ç”¨

> **ç›¸å…³æ–‡ç« **ï¼š
> - [æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿç»¼åˆè¯„ä¼°ï¼šä»æ ¸å¿ƒæŒ‡æ ‡åˆ°å‰æ²¿æ¡†æ¶](https://dd-ff.blog.csdn.net/article/details/152823514)
> - [åˆ«å†å·äº†ï¼ä½ å¼•ä»¥ä¸ºå‚²çš„ RAGï¼Œæ­£åœ¨æ€æ­»ä½ çš„ AI åˆ›ä¸šå…¬å¸](https://dd-ff.blog.csdn.net/article/details/150944979)
> - [LLM ä¸Šä¸‹æ–‡é€€åŒ–ï¼šå½“è¶Šé•¿çš„è¾“å…¥è®©AIå˜å¾—è¶Š"ç¬¨"](https://dd-ff.blog.csdn.net/article/details/149531324)

> **å¤–éƒ¨èµ„æº**ï¼š
> - [RAGAså®˜æ–¹æ–‡æ¡£](https://docs.ragas.io/) - RAGè¯„ä¼°æ¡†æ¶
> - [TruLensæ–‡æ¡£](https://www.trulens.org/trulens_eval/getting_started/) - LLMåº”ç”¨è¯„ä¼°å·¥å…·
> - [ARES GitHub](https://github.com/stanford-futuredata/ARES) - æ–¯å¦ç¦RAGè¯„ä¼°æ¡†æ¶

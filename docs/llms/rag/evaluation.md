---
title: RAG è¯„ä¼°æ–¹æ³•è¯¦è§£
description: RAG ç³»ç»Ÿè¯„ä¼°æŒ‡æ ‡ã€æ¡†æ¶ä¸å®æˆ˜æ–¹æ³•
---

# RAG è¯„ä¼°æ–¹æ³•è¯¦è§£

> ç§‘å­¦è¯„ä¼°RAGç³»ç»Ÿæ•ˆæœï¼Œä»æŒ‡æ ‡è®¾è®¡åˆ°æ¡†æ¶åº”ç”¨çš„å®Œæ•´æŒ‡å—

## ğŸ¯ æ ¸å¿ƒæ¦‚å¿µ

### ä¸ºä»€ä¹ˆéœ€è¦RAGè¯„ä¼°ï¼Ÿ

RAGç³»ç»Ÿçš„å¤æ‚æ€§è¦æ±‚æˆ‘ä»¬å»ºç«‹**ç§‘å­¦ã€å…¨é¢çš„è¯„ä¼°ä½“ç³»**æ¥è¡¡é‡å…¶æ•ˆæœï¼š

- **å¤šç»„ä»¶ç³»ç»Ÿ**ï¼šæ£€ç´¢å™¨+ç”Ÿæˆå™¨çš„è”åˆä¼˜åŒ–éœ€è¦åˆ†åˆ«å’Œæ•´ä½“è¯„ä¼°
- **è´¨é‡æ§åˆ¶**ï¼šç¡®ä¿ç³»ç»Ÿåœ¨ç”Ÿäº§ç¯å¢ƒä¸­çš„ç¨³å®šæ€§å’Œå¯é æ€§  
- **æŒç»­æ”¹è¿›**ï¼šé€šè¿‡é‡åŒ–æŒ‡æ ‡æŒ‡å¯¼ç³»ç»Ÿä¼˜åŒ–æ–¹å‘
- **ä¸šåŠ¡ä»·å€¼**ï¼šå°†æŠ€æœ¯æŒ‡æ ‡ä¸ä¸šåŠ¡ç›®æ ‡å¯¹é½

### è¯„ä¼°çš„æ ¸å¿ƒæŒ‘æˆ˜

::: warning å…³é”®éš¾ç‚¹
**ä¸»è§‚æ€§å¼º**ï¼šæ–‡æœ¬è´¨é‡è¯„ä¼°å¾€å¾€å¸¦æœ‰ä¸»è§‚è‰²å½©  
**å¤šç»´åº¦æƒè¡¡**ï¼šå‡†ç¡®æ€§ã€ç›¸å…³æ€§ã€æµç•…æ€§éœ€è¦ç»¼åˆè€ƒè™‘  
**æˆæœ¬é«˜æ˜‚**ï¼šäººå·¥æ ‡æ³¨å’Œè¯„ä¼°æˆæœ¬è¾ƒé«˜  
**åŠ¨æ€å˜åŒ–**ï¼šç”¨æˆ·éœ€æ±‚å’Œæ•°æ®åˆ†å¸ƒéšæ—¶é—´å˜åŒ–
:::

### RAGç³»ç»Ÿ12ä¸ªå¸¸è§ç—›ç‚¹åŠè§£å†³æ–¹æ¡ˆ

> æ¥æºï¼š[RAGæŠ€æœ¯çš„5ç§èŒƒå¼](https://hub.baai.ac.cn/view/43613)

| ç—›ç‚¹ | é—®é¢˜æè¿° | è§£å†³æ–¹æ¡ˆ |
|------|----------|----------|
| **1. å†…å®¹ç¼ºå¤±** | çŸ¥è¯†åº“ç¼ºå°‘ä¸Šä¸‹æ–‡æ—¶è¿”å›ä¼¼æ˜¯è€Œéçš„ç­”æ¡ˆ | æ¸…ç†æ•°æ®ã€ç²¾å¿ƒè®¾è®¡æç¤ºè¯ |
| **2. é”™è¿‡é‡è¦æ–‡æ¡£** | å…³é”®æ–‡æ¡£æœªå‡ºç°åœ¨Topç»“æœä¸­ | è°ƒæ•´æ£€ç´¢ç­–ç•¥ã€Embeddingæ¨¡å‹è°ƒä¼˜ |
| **3. ä¸Šä¸‹æ–‡æ•´åˆé™åˆ¶** | æ•´åˆé•¿åº¦è¶…è¿‡LLMçª—å£å¤§å° | è°ƒæ•´æ£€ç´¢ç­–ç•¥ã€ä¸Šä¸‹æ–‡å‹ç¼© |
| **4. ä¿¡æ¯æœªæå–** | æ–‡æ¡£ä¸­çš„å…³é”®ä¿¡æ¯æœªè¢«æå– | æ•°æ®æ¸…æ´—ã€æç¤ºè¯å‹ç¼©ã€é•¿å†…å®¹ä¼˜å…ˆæ’åº |
| **5. æ ¼å¼é”™è¯¯** | è¾“å‡ºæ ¼å¼ä¸é¢„æœŸä¸ç¬¦ | æ”¹è¿›æç¤ºè¯ã€æ ¼å¼åŒ–è¾“å‡ºã€ä½¿ç”¨JSONæ¨¡å¼ |
| **6. ç­”æ¡ˆä¸æ­£ç¡®** | ç¼ºä¹å…·ä½“ç»†èŠ‚å¯¼è‡´é”™è¯¯ | é‡‡ç”¨å…ˆè¿›æ£€ç´¢ç­–ç•¥ã€å¤šè·¯å¬å› |
| **7. å›ç­”ä¸å®Œæ•´** | ç­”æ¡ˆä¸å¤Ÿå…¨é¢ | æŸ¥è¯¢è½¬æ¢ã€é—®é¢˜ç»†åˆ† |
| **8. å¯æ‰©å±•æ€§é—®é¢˜** | æ•°æ®æ‘„å…¥æ€§èƒ½ç“¶é¢ˆ | å¹¶è¡Œå¤„ç†ã€æå‡å¤„ç†é€Ÿåº¦ |
| **9. ç»“æ„åŒ–æ•°æ®QA** | è¡¨æ ¼ç­‰ç»“æ„åŒ–æ•°æ®å¤„ç†å›°éš¾ | é“¾å¼æ€ç»´ã€æ··åˆæŸ¥è¯¢å¼•æ“ |
| **10. å¤æ‚PDFæå–** | å¤æ‚å¸ƒå±€PDFå¤„ç†å›°éš¾ | åµŒå…¥å¼è¡¨æ ¼æ£€ç´¢ã€LayoutLM |
| **11. åå¤‡æ¨¡å‹ç­–ç•¥** | ç¼ºå°‘fallbackæœºåˆ¶ | Neutrinoè·¯ç”±å™¨ã€OpenRouter |
| **12. LLMå®‰å…¨æ€§** | å®‰å…¨é˜²æŠ¤é—®é¢˜ | å†…å®¹å®¡æ ¸ã€è¾“å…¥éªŒè¯ã€è¾“å‡ºè¿‡æ»¤ |

---

## ğŸ“Š RAGè¯„ä¼°ä½“ç³»æ¶æ„

> åŸºäº[ã€Šæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿç»¼åˆè¯„ä¼°ï¼šä»æ ¸å¿ƒæŒ‡æ ‡åˆ°å‰æ²¿æ¡†æ¶ã€‹](https://dd-ff.blog.csdn.net/article/details/152823514)

### ä¸‰å±‚è¯„ä¼°ç»“æ„

```python
# RAGè¯„ä¼°çš„ä¸‰ä¸ªå±‚æ¬¡
RAGç³»ç»Ÿè¯„ä¼° = {
    "æ£€ç´¢å±‚è¯„ä¼°": "è¯„ä¼°æ£€ç´¢ç»„ä»¶çš„æ•ˆæœ",
    "ç”Ÿæˆå±‚è¯„ä¼°": "è¯„ä¼°ç”Ÿæˆç»„ä»¶çš„è´¨é‡", 
    "ç«¯åˆ°ç«¯è¯„ä¼°": "è¯„ä¼°æ•´ä½“ç³»ç»Ÿæ€§èƒ½"
}
```

| è¯„ä¼°å±‚æ¬¡ | å…³æ³¨ç‚¹ | å…¸å‹æŒ‡æ ‡ | è¯„ä¼°æ–¹æ³• |
|----------|--------|----------|----------|
| **æ£€ç´¢å±‚** | ç›¸å…³æ–‡æ¡£å¬å›è´¨é‡ | Recall@K, MRR, NDCG | ç¦»çº¿è¯„ä¼° |
| **ç”Ÿæˆå±‚** | ç­”æ¡ˆè´¨é‡ä¸å¿ å®åº¦ | Faithfulness, Relevance | LLM-Judge |
| **ç«¯åˆ°ç«¯** | ç”¨æˆ·æ»¡æ„åº¦ | Answer Accuracy, F1 | åœ¨çº¿A/Bæµ‹è¯• |

### è¯„ä¼°çš„äºŒå…ƒæ€§ï¼šåˆ†ç¦»è¯Šæ–­

RAGç³»ç»Ÿçš„æ€§èƒ½æ˜¯æ£€ç´¢å’Œç”Ÿæˆä¸¤ä¸ªç»„ä»¶ååŒä½œç”¨çš„ç»“æœï¼Œå…¨é¢çš„è¯„ä¼°ç­–ç•¥å¿…é¡»å…·å¤‡**äºŒå…ƒæ€§**ï¼šæ—¢è¦ç‹¬ç«‹è¯„ä¼°æ¯ä¸ªç»„ä»¶çš„æ€§èƒ½ï¼Œä¹Ÿè¦è¯„ä¼°æ•´ä¸ªç®¡é“çš„ç«¯åˆ°ç«¯è¡¨ç°ã€‚

å½“æœ€ç»ˆç­”æ¡ˆè´¨é‡ä¸ä½³æ—¶ï¼Œå¯èƒ½çš„åŸå› åˆ†ä¸ºä¸¤ç±»ï¼š

| å¤±è´¥ç±»å‹ | å®šä¹‰ | è¯Šæ–­æ–¹æ³• |
|----------|------|----------|
| **æ£€ç´¢å¤±è´¥** | æ£€ç´¢å™¨æœªèƒ½ä»çŸ¥è¯†åº“ä¸­æ‰¾åˆ°ä¸æŸ¥è¯¢ç›¸å…³ã€å‡†ç¡®æˆ–è¶³å¤Ÿçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ | æ£€æŸ¥Context Precision/Recall |
| **ç”Ÿæˆå¤±è´¥** | æ£€ç´¢å™¨æˆåŠŸæä¾›é«˜è´¨é‡ä¸Šä¸‹æ–‡ï¼Œä½†ç”Ÿæˆå™¨æœªèƒ½æ­£ç¡®åˆ©ç”¨ï¼ˆå¹»è§‰ã€å¿½ç•¥å…³é”®ä¿¡æ¯ã€ç­”éæ‰€é—®ï¼‰ | æ£€æŸ¥Faithfulness/Answer Relevance |

::: tip æ ¹å› åˆ†æåŸåˆ™
- è‹¥**æ£€ç´¢æŒ‡æ ‡ä½**ï¼ˆå¦‚ä¸Šä¸‹æ–‡ç²¾ç¡®ç‡ä½ï¼‰â†’ ä¼˜åŒ–é‡ç‚¹ï¼šæ•°æ®é¢„å¤„ç†ã€åµŒå…¥æ¨¡å‹ã€æ£€ç´¢ç­–ç•¥
- è‹¥**æ£€ç´¢æŒ‡æ ‡é«˜ä½†ç”ŸæˆæŒ‡æ ‡ä½**ï¼ˆå¦‚å¿ å®åº¦ä½ï¼‰â†’ ä¼˜åŒ–é‡ç‚¹ï¼šLLMé€‰æ‹©ã€æç¤ºå·¥ç¨‹ã€ç”Ÿæˆå‚æ•°
- **æ³¨æ„**ï¼šç´¢å¼•å’Œæ£€ç´¢é˜¶æ®µçš„é”™è¯¯ä¼šå‘ä¸Š"å†’æ³¡"ï¼Œå¹¶åœ¨ç”Ÿæˆé˜¶æ®µè¢«æ”¾å¤§ï¼Œç¡®ä¿æ£€ç´¢è´¨é‡æ˜¯ä¿éšœç³»ç»Ÿæ€§èƒ½çš„å…ˆå†³æ¡ä»¶
:::

### "RAGä¸‰å…ƒç»„"ï¼šæ•´ä½“è¯„ä¼°å“²å­¦

TruLensæ¡†æ¶æå‡ºçš„**"RAGä¸‰å…ƒç»„ï¼ˆRAG Triadï¼‰"**æ¦‚å¿µæ¨¡å‹ï¼Œå°†é«˜è´¨é‡RAGå“åº”åˆ†è§£ä¸ºä¸‰ä¸ªä¸å¯æˆ–ç¼ºçš„æ ¸å¿ƒæ”¯æŸ±ï¼š

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        RAG ä¸‰å…ƒç»„                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   â”‚  ä¸Šä¸‹æ–‡ç›¸å…³æ€§    â”‚   â”‚   å¿ å®åº¦/åŸºç¡€æ€§  â”‚   â”‚   ç­”æ¡ˆç›¸å…³æ€§    â”‚
â”‚   â”‚  Context        â”‚   â”‚   Faithfulness  â”‚   â”‚   Answer        â”‚
â”‚   â”‚  Relevance      â”‚   â”‚   Groundedness  â”‚   â”‚   Relevance     â”‚
â”‚   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   â”‚ è¯„ä¼°å¯¹è±¡:æ£€ç´¢å™¨  â”‚   â”‚ è¯„ä¼°å¯¹è±¡:ç”Ÿæˆå™¨  â”‚   â”‚ è¯„ä¼°å¯¹è±¡:ç”Ÿæˆå™¨  â”‚
â”‚   â”‚                 â”‚   â”‚                 â”‚   â”‚                 â”‚
â”‚   â”‚ æ£€ç´¢çš„ä¸Šä¸‹æ–‡ä¸  â”‚   â”‚ ç­”æ¡ˆæ˜¯å¦å¿ å®äº  â”‚   â”‚ ç­”æ¡ˆæ˜¯å¦ç›´æ¥   â”‚
â”‚   â”‚ æŸ¥è¯¢æ˜¯å¦ç›¸å…³ï¼Ÿ  â”‚   â”‚ æ£€ç´¢çš„ä¸Šä¸‹æ–‡ï¼Ÿ  â”‚   â”‚ å›åº”ç”¨æˆ·æ„å›¾ï¼Ÿ  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚                                                                 â”‚
â”‚   ä½ç›¸å…³æ€§ â†’ RAGç®¡é“ä»æºå¤´åç¦»æ–¹å‘                               â”‚
â”‚   ä½å¿ å®åº¦ â†’ ç³»ç»Ÿä¸å¯é ï¼ˆå³ä½¿ä¸Šä¸‹æ–‡é«˜ç›¸å…³ï¼‰                       â”‚
â”‚   ä½ç­”æ¡ˆç›¸å…³æ€§ â†’ æ— æ³•æ»¡è¶³ç”¨æˆ·éœ€æ±‚ï¼ˆå³ä½¿é«˜å¿ å®åº¦ï¼‰                  â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### ä¸‰å…ƒç»„çš„å†…åœ¨åˆ¶è¡¡å…³ç³»

ä¸‰ä¸ªç»´åº¦å­˜åœ¨å†…åœ¨åˆ¶è¡¡ï¼Œæ•´ä½“è´¨é‡å—é™äº**"æœ€è–„å¼±ç¯èŠ‚"**ï¼š

| åœºæ™¯ | ä¸Šä¸‹æ–‡ç›¸å…³æ€§ | å¿ å®åº¦ | ç­”æ¡ˆç›¸å…³æ€§ | ç»“æœ |
|------|-------------|--------|-----------|------|
| æ£€ç´¢å™¨æä¾›é«˜ç›¸å…³ä¸Šä¸‹æ–‡ï¼Œä½†ç”Ÿæˆå™¨æé€ ä¿¡æ¯ | âœ… é«˜ | âŒ ä½ | - | ç³»ç»Ÿä¸å¯é  |
| æ£€ç´¢å’Œå¿ å®åº¦å‡é«˜ï¼Œä½†ç­”æ¡ˆæœªè§£å†³æ ¸å¿ƒç–‘é—® | âœ… é«˜ | âœ… é«˜ | âŒ ä½ | æ— æ³•æ»¡è¶³ç”¨æˆ·éœ€æ±‚ |
| ä¸‰ä¸ªç»´åº¦åŒæ—¶è¾¾é«˜æ ‡å‡† | âœ… é«˜ | âœ… é«˜ | âœ… é«˜ | ç³»ç»Ÿå¯é æœ‰æ•ˆ |

> **æ ¸å¿ƒæ´å¯Ÿ**ï¼šRAGç³»ç»Ÿçš„ä¼˜åŒ–æœ¬è´¨æ˜¯"è¯†åˆ«å¹¶åŠ å›ºæœ€è–„å¼±ç¯èŠ‚"ï¼Œè€Œéå­¤ç«‹æå‡æŸä¸€ç»„ä»¶ã€‚

---

## ğŸ” æ£€ç´¢å±‚è¯„ä¼°

### æ ¸å¿ƒæŒ‡æ ‡è¯¦è§£

#### 1. å¬å›ç‡ï¼ˆRecall@Kï¼‰
```python
def recall_at_k(relevant_docs, retrieved_docs, k):
    """è®¡ç®—Recall@KæŒ‡æ ‡"""
    retrieved_k = retrieved_docs[:k]
    relevant_retrieved = set(retrieved_k) & set(relevant_docs)
    return len(relevant_retrieved) / len(relevant_docs)

# ç¤ºä¾‹
relevant_docs = ['doc1', 'doc3', 'doc5', 'doc7']  # ç›¸å…³æ–‡æ¡£
retrieved_docs = ['doc1', 'doc2', 'doc3', 'doc4', 'doc5']  # æ£€ç´¢ç»“æœ

recall_5 = recall_at_k(relevant_docs, retrieved_docs, k=5)
print(f"Recall@5: {recall_5:.3f}")  # è¾“å‡ºï¼š0.750
```

#### 2. å¹³å‡å€’æ•°æ’åï¼ˆMRRï¼‰
```python
def mean_reciprocal_rank(queries_results):
    """è®¡ç®—å¤šæŸ¥è¯¢çš„å¹³å‡å€’æ•°æ’å"""
    total_rr = 0
    valid_queries = 0
    
    for relevant_docs, retrieved_docs in queries_results:
        rr = 0
        for i, doc in enumerate(retrieved_docs):
            if doc in relevant_docs:
                rr = 1 / (i + 1)  # ç¬¬ä¸€ä¸ªç›¸å…³æ–‡æ¡£çš„å€’æ•°æ’å
                break
        total_rr += rr
        valid_queries += 1
    
    return total_rr / valid_queries if valid_queries > 0 else 0

# ç¤ºä¾‹
queries_data = [
    (['doc1', 'doc3'], ['doc2', 'doc1', 'doc4']),  # ç¬¬ä¸€ä¸ªæŸ¥è¯¢
    (['doc5'], ['doc5', 'doc6', 'doc7']),           # ç¬¬äºŒä¸ªæŸ¥è¯¢
]

mrr = mean_reciprocal_rank(queries_data)
print(f"MRR: {mrr:.3f}")
```

#### 3. å½’ä¸€åŒ–æŠ˜æ‰£ç´¯ç§¯å¢ç›Šï¼ˆNDCGï¼‰
```python
import numpy as np

def dcg_at_k(relevance_scores, k):
    """è®¡ç®—DCG@K"""
    relevance_scores = np.array(relevance_scores[:k])
    if relevance_scores.size:
        return np.sum(relevance_scores / np.log2(np.arange(2, relevance_scores.size + 2)))
    return 0

def ndcg_at_k(relevant_scores, retrieved_scores, k):
    """è®¡ç®—NDCG@K"""
    dcg = dcg_at_k(retrieved_scores, k)
    idcg = dcg_at_k(sorted(relevant_scores, reverse=True), k)
    return dcg / idcg if idcg > 0 else 0

# ç¤ºä¾‹ï¼šç›¸å…³æ€§åˆ†æ•°ï¼ˆ0-3åˆ†ï¼‰
relevant_scores = [3, 2, 3, 1, 2]  # ç†æƒ³æ’åºçš„ç›¸å…³æ€§
retrieved_scores = [3, 1, 2, 3, 0]  # å®é™…æ£€ç´¢çš„ç›¸å…³æ€§

ndcg_5 = ndcg_at_k(relevant_scores, retrieved_scores, k=5)
print(f"NDCG@5: {ndcg_5:.3f}")
```

### å®æˆ˜è¯„ä¼°ä»£ç 

```python
class RetrievalEvaluator:
    def __init__(self, ground_truth_path):
        """
        ground_truth_path: æ ‡å‡†ç­”æ¡ˆæ–‡ä»¶è·¯å¾„
        æ ¼å¼: {
            "query_id": {
                "query": "æŸ¥è¯¢æ–‡æœ¬",
                "relevant_docs": ["doc1", "doc2", ...]
            }
        }
        """
        with open(ground_truth_path, 'r', encoding='utf-8') as f:
            self.ground_truth = json.load(f)
    
    def evaluate_retriever(self, retriever, top_k=10):
        """è¯„ä¼°æ£€ç´¢å™¨æ€§èƒ½"""
        metrics = {
            'recall': [],
            'precision': [],
            'mrr': [],
            'ndcg': []
        }
        
        for query_id, data in self.ground_truth.items():
            query = data['query']
            relevant_docs = data['relevant_docs']
            
            # æ‰§è¡Œæ£€ç´¢
            results = retriever.retrieve(query, top_k)
            retrieved_docs = [r['doc_id'] for r in results]
            
            # è®¡ç®—æŒ‡æ ‡
            recall = self._calculate_recall(relevant_docs, retrieved_docs)
            precision = self._calculate_precision(relevant_docs, retrieved_docs)
            mrr = self._calculate_single_mrr(relevant_docs, retrieved_docs)
            
            metrics['recall'].append(recall)
            metrics['precision'].append(precision)
            metrics['mrr'].append(mrr)
        
        # è®¡ç®—å¹³å‡å€¼
        avg_metrics = {k: np.mean(v) for k, v in metrics.items()}
        return avg_metrics
    
    def _calculate_recall(self, relevant, retrieved):
        if not relevant:
            return 0
        return len(set(relevant) & set(retrieved)) / len(relevant)
    
    def _calculate_precision(self, relevant, retrieved):
        if not retrieved:
            return 0
        return len(set(relevant) & set(retrieved)) / len(retrieved)
    
    def _calculate_single_mrr(self, relevant, retrieved):
        for i, doc in enumerate(retrieved):
            if doc in relevant:
                return 1 / (i + 1)
        return 0

# ä½¿ç”¨ç¤ºä¾‹
evaluator = RetrievalEvaluator('ground_truth.json')
metrics = evaluator.evaluate_retriever(my_retriever)

print("æ£€ç´¢è¯„ä¼°ç»“æœ:")
for metric, value in metrics.items():
    print(f"{metric.upper()}: {value:.3f}")
```

---

## ğŸ“ ç”Ÿæˆå±‚è¯„ä¼°

### å…³é”®æŒ‡æ ‡ä½“ç³»

#### 1. å¿ å®åº¦ï¼ˆFaithfulnessï¼‰

**å®šä¹‰**ï¼šè¡¡é‡ç”Ÿæˆå™¨è¾“å‡ºä¸æ£€ç´¢ä¸Šä¸‹æ–‡äº‹å®ä¸€è‡´æ€§çš„æ ¸å¿ƒæŒ‡æ ‡ã€‚é«˜å¿ å®åº¦çš„ç­”æ¡ˆï¼Œå…¶æ‰€æœ‰äº‹å®æ€§å£°æ˜éƒ½å¿…é¡»èƒ½ä»æä¾›çš„æºä¸Šä¸‹æ–‡ä¸­å¾—åˆ°ç›´æ¥æ”¯æŒæˆ–åˆç†æ¨æ–­ï¼Œæ— ä»»ä½•"æé€ äº‹å®"ã€‚

::: warning å…³é”®æ³¨æ„äº‹é¡¹
- è‹¥ç”Ÿæˆç­”æ¡ˆåŒ…å«"æ— æ³•ä»ä¸Šä¸‹æ–‡éªŒè¯çš„å£°æ˜"ï¼ˆå³ä½¿è¯¥å£°æ˜åœ¨å®¢è§‚ä¸–ç•Œä¸­ä¸ºçœŸï¼‰ï¼Œä»éœ€åˆ¤å®šä¸º"ä¸å¿ å®"â€”â€”RAGçš„æ ¸å¿ƒé€»è¾‘æ˜¯"åŸºäºæ£€ç´¢åˆ°çš„ä¿¡æ¯ç”Ÿæˆç­”æ¡ˆ"ï¼Œè€Œéä¾èµ–LLMè‡ªèº«çŸ¥è¯†åº“
- éœ€é¿å…LLMè¯„åˆ¤è€…"è¿‡åº¦å®½å®¹"ï¼šå¯¹äº"æ¨¡ç³Šè¡¨è¿°"ï¼ˆå¦‚å°†ä¸Šä¸‹æ–‡çš„"çº¦1000ä¸‡"è¡¨è¿°ä¸º"1000ä¸‡"ï¼‰ï¼Œéœ€æ ¹æ®åœºæ™¯å®šä¹‰æ˜¯å¦åˆ¤å®šä¸º"ä¸å¿ å®"
:::

**è®¡ç®—æ–¹å¼ï¼ˆLLM-as-a-Judgeï¼Œå£°æ˜åˆ†è§£+é€ä¸€éªŒè¯ï¼‰**ï¼š

```
å¿ å®åº¦å¾—åˆ† = è¢«è¯å®çš„å£°æ˜æ•° / æ€»å£°æ˜æ•°
```

1. **å£°æ˜åˆ†è§£**ï¼šå°†ç­”æ¡ˆæ‹†è§£ä¸ºç‹¬ç«‹ã€åŸå­åŒ–çš„äº‹å®å£°æ˜
2. **é€ä¸€éªŒè¯**ï¼šåˆ¤æ–­æ¯ä¸ªå£°æ˜æ˜¯å¦èƒ½ä»ä¸Šä¸‹æ–‡å¾—åˆ°æ”¯æŒæˆ–æ¨æ–­
3. **è®¡ç®—å¾—åˆ†**ï¼šå¾—åˆ†èŒƒå›´0~1ï¼Œè¶Šæ¥è¿‘1è¡¨ç¤ºå¹»è§‰ç¨‹åº¦è¶Šä½

```python
from openai import OpenAI

class FaithfulnessEvaluator:
    def __init__(self):
        self.client = OpenAI()
    
    def evaluate_faithfulness(self, context: str, generated_answer: str):
        """è¯„ä¼°ç­”æ¡ˆå¯¹ä¸Šä¸‹æ–‡çš„å¿ å®åº¦"""
        prompt = f"""
è¯·è¯„ä¼°ä»¥ä¸‹ç”Ÿæˆçš„ç­”æ¡ˆæ˜¯å¦å¿ å®äºç»™å®šçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚

ä¸Šä¸‹æ–‡ï¼š
{context}

ç”Ÿæˆçš„ç­”æ¡ˆï¼š
{generated_answer}

è¯„ä¼°æ ‡å‡†ï¼š
1. ç­”æ¡ˆä¸­çš„äº‹å®æ˜¯å¦éƒ½èƒ½åœ¨ä¸Šä¸‹æ–‡ä¸­æ‰¾åˆ°æ”¯æ’‘
2. æ˜¯å¦å­˜åœ¨ä¸ä¸Šä¸‹æ–‡çŸ›ç›¾çš„ä¿¡æ¯
3. æ˜¯å¦æ·»åŠ äº†ä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰çš„ä¿¡æ¯

è¯·ç»™å‡º0-1ä¹‹é—´çš„åˆ†æ•°ï¼Œå…¶ä¸­ï¼š
- 1.0ï¼šå®Œå…¨å¿ å®ï¼Œæ‰€æœ‰ä¿¡æ¯éƒ½æ¥è‡ªä¸Šä¸‹æ–‡
- 0.8ï¼šåŸºæœ¬å¿ å®ï¼Œå°‘é‡åˆç†æ¨ç†
- 0.6ï¼šéƒ¨åˆ†å¿ å®ï¼Œæœ‰ä¸€äº›ä¸å‡†ç¡®ä¿¡æ¯
- 0.4ï¼šè¾ƒå¤šä¸å‡†ç¡®ä¿¡æ¯
- 0.2ï¼šå¤§é‡é”™è¯¯ä¿¡æ¯
- 0.0ï¼šå®Œå…¨ä¸å¿ å®æˆ–æ— å…³

åˆ†æ•°ï¼š"""
        
        response = self.client.chat.completions.create(
            model="gpt-4",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.1
        )
        
        # æå–åˆ†æ•°
        import re
        score_text = response.choices[0].message.content
        score_match = re.search(r'åˆ†æ•°[ï¼š:]\s*([0-9.]+)', score_text)
        
        if score_match:
            return float(score_match.group(1))
        return 0.5  # é»˜è®¤å€¼

# ä½¿ç”¨ç¤ºä¾‹
evaluator = FaithfulnessEvaluator()
context = "RAGæŠ€æœ¯ç»“åˆäº†æ£€ç´¢å’Œç”Ÿæˆï¼Œèƒ½å¤Ÿè·å–å®æ—¶ä¿¡æ¯..."
answer = "RAGæ˜¯ä¸€ç§å°†ä¿¡æ¯æ£€ç´¢ä¸æ–‡æœ¬ç”Ÿæˆç›¸ç»“åˆçš„æŠ€æœ¯..."

faithfulness_score = evaluator.evaluate_faithfulness(context, answer)
print(f"å¿ å®åº¦åˆ†æ•°: {faithfulness_score:.2f}")
```

#### 2. ç­”æ¡ˆç›¸å…³æ€§ï¼ˆAnswer Relevanceï¼‰

**å®šä¹‰**ï¼šè¯„ä¼°ç”Ÿæˆç­”æ¡ˆä¸ç”¨æˆ·åŸå§‹æŸ¥è¯¢æ„å›¾çš„åŒ¹é…ç¨‹åº¦ã€‚å®ƒè§£å†³äº†"å¿ å®ä½†æ— ç”¨"çš„é—®é¢˜â€”â€”ä¸€ä¸ªç­”æ¡ˆå¯èƒ½å®Œå…¨å¿ å®äºæ£€ç´¢ä¸Šä¸‹æ–‡ï¼ˆé«˜å¿ å®åº¦ï¼‰ï¼Œä½†å¦‚æœç­”éæ‰€é—®ã€ä¿¡æ¯å†—ä½™æˆ–æœªè¦†ç›–æ ¸å¿ƒéœ€æ±‚ï¼Œåˆ™ä»å±äºä½è´¨é‡è¾“å‡ºã€‚

**è®¡ç®—æ–¹å¼ï¼ˆé€†å‘é—®é¢˜ç”Ÿæˆ+è¯­ä¹‰ç›¸ä¼¼åº¦åŒ¹é…ï¼‰**ï¼š

RAGAsç­‰æ¡†æ¶é‡‡ç”¨åˆ›æ–°æ–¹æ³•ï¼Œé¿å…å¯¹"é»„é‡‘æ ‡å‡†ç­”æ¡ˆ"çš„ä¾èµ–ï¼š

1. **é€†å‘é—®é¢˜ç”Ÿæˆ**ï¼šLLMåŸºäºç”Ÿæˆçš„ç­”æ¡ˆï¼Œåå‘ç”Ÿæˆ3~5ä¸ª"å¯èƒ½å¼•å‡ºè¯¥ç­”æ¡ˆçš„æ½œåœ¨é—®é¢˜"
2. **è¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®—**ï¼šå°†æ½œåœ¨é—®é¢˜ä¸ç”¨æˆ·åŸå§‹æŸ¥è¯¢è½¬ä¸ºå‘é‡ï¼Œè®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
3. **è®¡ç®—å¾—åˆ†**ï¼šç­”æ¡ˆç›¸å…³æ€§å¾—åˆ† = æ‰€æœ‰æ½œåœ¨é—®é¢˜ä¸åŸå§‹æŸ¥è¯¢çš„ç›¸ä¼¼åº¦å¹³å‡å€¼

```python
class RelevanceEvaluator:
    def __init__(self):
        self.client = OpenAI()
    
    def evaluate_relevance(self, query: str, generated_answer: str):
        """è¯„ä¼°ç­”æ¡ˆä¸æŸ¥è¯¢çš„ç›¸å…³æ€§"""
        prompt = f"""
è¯·è¯„ä¼°ç”Ÿæˆçš„ç­”æ¡ˆä¸ç”¨æˆ·æŸ¥è¯¢çš„ç›¸å…³æ€§ã€‚

ç”¨æˆ·æŸ¥è¯¢ï¼š
{query}

ç”Ÿæˆçš„ç­”æ¡ˆï¼š
{generated_answer}

è¯„ä¼°æ ‡å‡†ï¼š
1. ç­”æ¡ˆæ˜¯å¦ç›´æ¥å›åº”äº†ç”¨æˆ·çš„é—®é¢˜
2. ç­”æ¡ˆæ˜¯å¦åŒ…å«ç”¨æˆ·éœ€è¦çš„æ ¸å¿ƒä¿¡æ¯
3. ç­”æ¡ˆçš„è¯¦ç»†ç¨‹åº¦æ˜¯å¦é€‚å½“

è¯·ç»™å‡º0-1ä¹‹é—´çš„åˆ†æ•°ï¼š
- 1.0ï¼šå®Œå…¨ç›¸å…³ï¼Œç›´æ¥å›ç­”é—®é¢˜
- 0.8ï¼šé«˜åº¦ç›¸å…³ï¼ŒåŸºæœ¬å›ç­”é—®é¢˜
- 0.6ï¼šéƒ¨åˆ†ç›¸å…³ï¼Œå›ç­”äº†éƒ¨åˆ†é—®é¢˜
- 0.4ï¼šç›¸å…³æ€§è¾ƒä½
- 0.2ï¼šç›¸å…³æ€§å¾ˆä½
- 0.0ï¼šå®Œå…¨ä¸ç›¸å…³

åˆ†æ•°ï¼š"""
        
        response = self.client.chat.completions.create(
            model="gpt-4",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.1
        )
        
        # æå–åˆ†æ•°çš„é€»è¾‘åŒä¸Š
        # ...
        
# æ‰¹é‡è¯„ä¼°å·¥å…·
class BatchGenerationEvaluator:
    def __init__(self):
        self.faithfulness_evaluator = FaithfulnessEvaluator()
        self.relevance_evaluator = RelevanceEvaluator()
    
    def evaluate_batch(self, test_cases):
        """æ‰¹é‡è¯„ä¼°ç”Ÿæˆè´¨é‡"""
        results = []
        
        for case in test_cases:
            query = case['query']
            context = case['context']
            generated_answer = case['generated_answer']
            
            faithfulness = self.faithfulness_evaluator.evaluate_faithfulness(
                context, generated_answer
            )
            relevance = self.relevance_evaluator.evaluate_relevance(
                query, generated_answer
            )
            
            results.append({
                'query': query,
                'faithfulness': faithfulness,
                'relevance': relevance,
                'overall': (faithfulness + relevance) / 2
            })
        
        return results

# ä½¿ç”¨ç¤ºä¾‹
test_cases = [
    {
        'query': 'ä»€ä¹ˆæ˜¯RAGæŠ€æœ¯ï¼Ÿ',
        'context': 'RAGæŠ€æœ¯æ–‡æ¡£å†…å®¹...',
        'generated_answer': 'RAGæ˜¯æ£€ç´¢å¢å¼ºç”ŸæˆæŠ€æœ¯...'
    }
    # æ›´å¤šæµ‹è¯•ç”¨ä¾‹...
]

batch_evaluator = BatchGenerationEvaluator()
results = batch_evaluator.evaluate_batch(test_cases)

avg_faithfulness = np.mean([r['faithfulness'] for r in results])
avg_relevance = np.mean([r['relevance'] for r in results])
print(f"å¹³å‡å¿ å®åº¦: {avg_faithfulness:.3f}")
print(f"å¹³å‡ç›¸å…³æ€§: {avg_relevance:.3f}")
```

---

## ğŸ”„ ç«¯åˆ°ç«¯è¯„ä¼°

### ç»¼åˆè¯„ä¼°æŒ‡æ ‡

#### 1. ç­”æ¡ˆå‡†ç¡®ç‡ï¼ˆAnswer Accuracyï¼‰
```python
class AnswerAccuracyEvaluator:
    def __init__(self):
        self.client = OpenAI()
    
    def evaluate_accuracy(self, query: str, generated_answer: str, ground_truth: str):
        """è¯„ä¼°ç­”æ¡ˆå‡†ç¡®æ€§"""
        prompt = f"""
è¯·æ¯”è¾ƒç”Ÿæˆç­”æ¡ˆä¸æ ‡å‡†ç­”æ¡ˆçš„å‡†ç¡®æ€§ã€‚

é—®é¢˜ï¼š{query}

ç”Ÿæˆç­”æ¡ˆï¼š{generated_answer}

æ ‡å‡†ç­”æ¡ˆï¼š{ground_truth}

è¯·åˆ¤æ–­ç”Ÿæˆç­”æ¡ˆæ˜¯å¦æ­£ç¡®ï¼Œç»™å‡ºåˆ†æ•°ï¼š
- 1ï¼šå®Œå…¨æ­£ç¡®
- 0.8ï¼šåŸºæœ¬æ­£ç¡®ï¼Œæœ‰ç»†å¾®å·®å¼‚
- 0.6ï¼šéƒ¨åˆ†æ­£ç¡®
- 0.4ï¼šæœ‰è¾ƒå¤šé”™è¯¯
- 0.2ï¼šå¤§éƒ¨åˆ†é”™è¯¯
- 0ï¼šå®Œå…¨é”™è¯¯

åˆ†æ•°ï¼š"""
        
        # LLMè¯„ä¼°é€»è¾‘...
        
    def calculate_accuracy_metrics(self, predictions, ground_truths):
        """è®¡ç®—å‡†ç¡®ç‡ç›¸å…³æŒ‡æ ‡"""
        exact_matches = []
        f1_scores = []
        
        for pred, gt in zip(predictions, ground_truths):
            # ç²¾ç¡®åŒ¹é…
            exact_match = 1 if pred.strip().lower() == gt.strip().lower() else 0
            exact_matches.append(exact_match)
            
            # F1åˆ†æ•°ï¼ˆåŸºäºè¯çº§åˆ«ï¼‰
            f1 = self._calculate_f1(pred, gt)
            f1_scores.append(f1)
        
        return {
            'exact_match': np.mean(exact_matches),
            'f1_score': np.mean(f1_scores)
        }
    
    def _calculate_f1(self, prediction, ground_truth):
        """è®¡ç®—F1åˆ†æ•°"""
        pred_tokens = set(prediction.lower().split())
        gt_tokens = set(ground_truth.lower().split())
        
        if len(pred_tokens) == 0:
            return 0
        
        common_tokens = pred_tokens & gt_tokens
        precision = len(common_tokens) / len(pred_tokens)
        recall = len(common_tokens) / len(gt_tokens) if len(gt_tokens) > 0 else 0
        
        if precision + recall == 0:
            return 0
        
        return 2 * (precision * recall) / (precision + recall)
```

#### 2. ç”¨æˆ·æ»¡æ„åº¦è¯„ä¼°
```python
class UserSatisfactionEvaluator:
    def __init__(self):
        self.satisfaction_history = []
    
    def collect_feedback(self, query: str, answer: str, user_rating: int, 
                        feedback_text: str = ""):
        """æ”¶é›†ç”¨æˆ·åé¦ˆ"""
        feedback = {
            'timestamp': datetime.now(),
            'query': query,
            'answer': answer,
            'rating': user_rating,  # 1-5åˆ†
            'feedback': feedback_text
        }
        self.satisfaction_history.append(feedback)
    
    def calculate_satisfaction_metrics(self, time_window_days=30):
        """è®¡ç®—æ»¡æ„åº¦æŒ‡æ ‡"""
        cutoff_date = datetime.now() - timedelta(days=time_window_days)
        recent_feedback = [
            f for f in self.satisfaction_history 
            if f['timestamp'] > cutoff_date
        ]
        
        if not recent_feedback:
            return None
        
        ratings = [f['rating'] for f in recent_feedback]
        
        return {
            'avg_rating': np.mean(ratings),
            'satisfaction_rate': len([r for r in ratings if r >= 4]) / len(ratings),
            'total_responses': len(recent_feedback),
            'rating_distribution': {
                i: ratings.count(i) for i in range(1, 6)
            }
        }
```

---

## ğŸ› ï¸ è¯„ä¼°æ¡†æ¶å®æˆ˜

### ä¸»æµæ¡†æ¶å¯¹æ¯”

| æ¡†æ¶ | æ ¸å¿ƒç‰¹ç‚¹ | ä¼˜åŠ¿ | å±€é™ | é€‚ç”¨åœºæ™¯ |
|------|----------|------|------|----------|
| **RAGAs** | æ— å‚è€ƒè¯„ä¼°ï¼ŒLLM-as-a-Judge | æ— éœ€é»„é‡‘æ ‡å‡†ï¼Œå¿«é€ŸéªŒè¯ | ä¾èµ–LLMåˆ¤æ–­ç¨³å®šæ€§ | å¿«é€ŸåŸå‹éªŒè¯ã€è¿­ä»£ç›‘æ§ |
| **ARES** | åˆæˆæ•°æ®+å¾®è°ƒè¯„åˆ¤è€… | é«˜ç²¾åº¦ã€é¢†åŸŸé€‚é… | è®¾ç½®æˆæœ¬é«˜ | ç”Ÿäº§çº§ä¸¥æ ¼éªŒè¯ |
| **TruLens** | å¼€å‘é›†æˆã€RAGä¸‰å…ƒç»„ | ç«¯åˆ°ç«¯è¿½è¸ªã€å¯è§†åŒ– | é…ç½®å¤æ‚ | å¼€å‘è°ƒè¯•ã€å…¨é“¾è·¯ç›‘æ§ |

### 1. RAGAsæ¡†æ¶è¯¦è§£

RAGAsï¼ˆRetrieval-Augmented Generation Assessmentï¼‰æ˜¯ç”±IBM Researchå¼€æºçš„RAGä¸“ç”¨è¯„ä¼°æ¡†æ¶ï¼Œå…¶æ ¸å¿ƒæ ‡ç­¾æ˜¯**"æ— å‚è€ƒè¯„ä¼°"**â€”â€”æ— éœ€äººå·¥æ ‡æ³¨çš„é»„é‡‘æ ‡å‡†ç­”æ¡ˆï¼Œæå¤§é™ä½äº†è¯„ä¼°é—¨æ§›ã€‚

#### æ ¸å¿ƒè®¾è®¡å“²å­¦

RAGAsé€šè¿‡**"LLMå³è¯„åˆ¤è€…ï¼ˆLLM-as-a-Judgeï¼‰"**èŒƒå¼ï¼Œè®©å¼ºå¤§çš„é€šç”¨LLMï¼ˆå¦‚GPT-4ã€Llama 3ï¼‰æ¨¡æ‹Ÿäººç±»ä¸“å®¶çš„åˆ¤æ–­é€»è¾‘ï¼Œå®ç°è¯„ä¼°æµç¨‹çš„å…¨è‡ªåŠ¨åŒ–ã€‚

#### å››å¤§æ ¸å¿ƒæŒ‡æ ‡

| æŒ‡æ ‡ | è¯„ä¼°å¯¹è±¡ | å«ä¹‰ | è®¡ç®—æ–¹å¼ |
|------|----------|------|----------|
| **ä¸Šä¸‹æ–‡ç²¾ç¡®ç‡** | æ£€ç´¢å™¨ | æ£€ç´¢çš„ä¸Šä¸‹æ–‡ä¸­æœ‰å¤šå°‘æ˜¯å›ç­”é—®é¢˜å¿…éœ€çš„ | å¿…éœ€å¥å­æ•° / æ€»å¥å­æ•° |
| **ä¸Šä¸‹æ–‡å¬å›ç‡** | æ£€ç´¢å™¨ | é»„é‡‘ç­”æ¡ˆä¸­çš„ä¿¡æ¯æœ‰å¤šå°‘èƒ½åœ¨ä¸Šä¸‹æ–‡ä¸­æ‰¾åˆ° | å¯å½’å› å£°æ˜æ•° / æ€»å£°æ˜æ•° |
| **å¿ å®åº¦** | ç”Ÿæˆå™¨ | ç­”æ¡ˆçš„äº‹å®å£°æ˜æ˜¯å¦éƒ½èƒ½ä»ä¸Šä¸‹æ–‡éªŒè¯ | è¢«è¯å®å£°æ˜æ•° / æ€»å£°æ˜æ•° |
| **ç­”æ¡ˆç›¸å…³æ€§** | ç”Ÿæˆå™¨ | ç­”æ¡ˆæ˜¯å¦ç›´æ¥å›åº”ç”¨æˆ·æŸ¥è¯¢æ„å›¾ | é€†å‘é—®é¢˜ä¸åŸå§‹æŸ¥è¯¢çš„è¯­ä¹‰ç›¸ä¼¼åº¦ |

::: tip æç¤ºè¯è®¾è®¡ç»†èŠ‚
RAGAsçš„æç¤ºè¯éµå¾ª"æŒ‡ä»¤+ç¤ºä¾‹"ç»“æ„â€”â€”ä¾‹å¦‚è®¡ç®—ä¸Šä¸‹æ–‡ç²¾ç¡®ç‡æ—¶ï¼Œä¼šå…ˆå‘LLMè¯´æ˜"å¿…éœ€å¥å­"çš„å®šä¹‰ï¼Œå†ç»™å‡º2~3ä¸ªæ­£åç¤ºä¾‹ï¼Œç¡®ä¿LLMç†è§£è¯„åˆ¤æ ‡å‡†ï¼Œå‡å°‘ä¸»è§‚åå·®ã€‚
:::

```python
from ragas import evaluate
from ragas.metrics import (
    faithfulness,
    answer_relevancy,
    context_recall,
    context_precision,
)

class RAGAsEvaluator:
    def __init__(self):
        self.metrics = [
            faithfulness,
            answer_relevancy, 
            context_recall,
            context_precision
        ]
    
    def evaluate_with_ragas(self, dataset):
        """ä½¿ç”¨RAGAsè¿›è¡Œè¯„ä¼°"""
        # datasetæ ¼å¼ï¼š
        # {
        #     'question': [...],
        #     'contexts': [...],  # æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡åˆ—è¡¨
        #     'answer': [...],    # ç”Ÿæˆçš„ç­”æ¡ˆ
        #     'ground_truths': [...] # æ ‡å‡†ç­”æ¡ˆ
        # }
        
        results = evaluate(
            dataset=dataset,
            metrics=self.metrics
        )
        
        return results.to_pandas()

# ä½¿ç”¨ç¤ºä¾‹
evaluator = RAGAsEvaluator()

# å‡†å¤‡æ•°æ®é›†
eval_dataset = {
    'question': ['ä»€ä¹ˆæ˜¯RAGæŠ€æœ¯ï¼Ÿ'],
    'contexts': [['RAGæ˜¯æ£€ç´¢å¢å¼ºç”ŸæˆæŠ€æœ¯ï¼Œç»“åˆäº†æ£€ç´¢å’Œç”Ÿæˆ...']],
    'answer': ['RAGæŠ€æœ¯æ˜¯ä¸€ç§ç»“åˆæ£€ç´¢å’Œç”Ÿæˆçš„AIæŠ€æœ¯...'],
    'ground_truths': [['RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰æ˜¯ä¸€ç§AIæŠ€æœ¯...']]
}

results_df = evaluator.evaluate_with_ragas(eval_dataset)
print("RAGAsè¯„ä¼°ç»“æœï¼š")
print(results_df.describe())
```

### 2. ARESæ¡†æ¶ï¼šé«˜ç²¾åº¦è¯„ä¼°

ARESï¼ˆAutomated RAG Evaluation Systemï¼‰æ˜¯ç”±æ–¯å¦ç¦å¤§å­¦å›¢é˜Ÿæå‡ºçš„é«˜ç²¾åº¦è¯„ä¼°æ¡†æ¶ï¼Œå…¶æ ¸å¿ƒåˆ›æ–°æ˜¯**"ç”¨åˆæˆæ•°æ®å¾®è°ƒä¸“ç”¨è¯„åˆ¤è€…"**ã€‚

#### æ ¸å¿ƒè®¾è®¡å“²å­¦

ARESçš„è®¾è®¡æ€è·¯æ˜¯**"é¢†åŸŸé€‚é…ä¼˜äºé€šç”¨èƒ½åŠ›"**ï¼šé€šç”¨LLMåœ¨ç‰¹å®šé¢†åŸŸï¼ˆå¦‚åŒ»ç–—ã€é‡‘èï¼‰çš„è¯„ä¼°å‡†ç¡®æ€§ä»æœ‰å·®è·ï¼Œå¯èƒ½å› ä¸ç†è§£ä¸“ä¸šæœ¯è¯­å¯¼è‡´è¯¯åˆ¤ã€‚

#### ä¸‰é˜¶æ®µè¯„ä¼°æµç¨‹

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  é˜¶æ®µ1ï¼šåˆæˆæ•°æ® â”‚ --> â”‚ é˜¶æ®µ2ï¼šå¾®è°ƒè¯„åˆ¤è€… â”‚ --> â”‚ é˜¶æ®µ3ï¼šé¢„æµ‹æ¨ç†  â”‚
â”‚  ç”Ÿæˆ            â”‚     â”‚                  â”‚     â”‚                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤     â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤     â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ç”¨GPT-4ä»é¢†åŸŸ   â”‚     â”‚ ç”¨åˆæˆæ•°æ®å¾®è°ƒ   â”‚     â”‚ å¾®è°ƒåçš„è¯„åˆ¤è€…   â”‚
â”‚ æ–‡æ¡£ç”Ÿæˆæ­£/è´Ÿä¾‹  â”‚     â”‚ DeBERTa/RoBERTa â”‚     â”‚ å¯¹çœŸå®æ•°æ®æ‰“åˆ†   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**é˜¶æ®µ1ï¼šåˆæˆæ•°æ®ç”Ÿæˆ**
- **æ­£ä¾‹**ï¼šï¼ˆæŸ¥è¯¢ï¼Œç›¸å…³ä¸Šä¸‹æ–‡ï¼Œå¿ å®ä¸”ç›¸å…³çš„ç­”æ¡ˆï¼‰â€”â€”æ¨¡æ‹Ÿç†æƒ³è¾“å‡º
- **è´Ÿä¾‹-æ£€ç´¢å¤±è´¥**ï¼šï¼ˆæŸ¥è¯¢ï¼Œä¸ç›¸å…³ä¸Šä¸‹æ–‡ï¼Œç­”æ¡ˆï¼‰â€”â€”æ¨¡æ‹Ÿæ£€ç´¢å™¨å¤±æ•ˆ
- **è´Ÿä¾‹-ç”Ÿæˆå¤±è´¥**ï¼šï¼ˆæŸ¥è¯¢ï¼Œç›¸å…³ä¸Šä¸‹æ–‡ï¼Œå¹»è§‰ç­”æ¡ˆï¼‰â€”â€”æ¨¡æ‹Ÿç”Ÿæˆå™¨å¤±æ•ˆ

**é˜¶æ®µ2ï¼šå¾®è°ƒä¸“ç”¨è¯„åˆ¤è€…**
- ä½¿ç”¨DeBERTaã€RoBERTaç­‰è½»é‡çº§æ¨¡å‹
- åœ¨åˆæˆæ•°æ®ä¸Šè¿›è¡ŒäºŒåˆ†ç±»/å›å½’å¾®è°ƒ
- ä½¿å…¶æˆä¸ºè¯¥é¢†åŸŸçš„"ä¸“å®¶è¯„åˆ¤è€…"

**é€‚ç”¨åœºæ™¯**ï¼šå¯¹æ€§èƒ½è¦æ±‚ä¸¥è‹›çš„ç”Ÿäº§çº§åœºæ™¯ï¼ˆåŒ»ç–—ã€é‡‘èã€æ³•å¾‹ï¼‰

### 3. è‡ªå®šä¹‰è¯„ä¼°æµæ°´çº¿

```python
class ComprehensiveRAGEvaluator:
    def __init__(self, config):
        self.retrieval_evaluator = RetrievalEvaluator(config['ground_truth_path'])
        self.generation_evaluator = BatchGenerationEvaluator()
        self.answer_evaluator = AnswerAccuracyEvaluator()
        
    def full_evaluation(self, rag_system, test_queries):
        """å®Œæ•´RAGç³»ç»Ÿè¯„ä¼°"""
        results = {
            'retrieval_metrics': {},
            'generation_metrics': {},
            'end_to_end_metrics': {},
            'detailed_results': []
        }
        
        for query_data in test_queries:
            query = query_data['query']
            expected_docs = query_data.get('relevant_docs', [])
            ground_truth_answer = query_data.get('ground_truth', '')
            
            # 1. æ‰§è¡ŒRAGæµç¨‹
            retrieved_docs = rag_system.retrieve(query)
            generated_answer = rag_system.generate(query, retrieved_docs)
            
            # 2. æ£€ç´¢è¯„ä¼°
            if expected_docs:
                retrieval_recall = self._calculate_recall(
                    expected_docs, [d['id'] for d in retrieved_docs]
                )
            
            # 3. ç”Ÿæˆè¯„ä¼°
            context = ' '.join([doc['text'] for doc in retrieved_docs])
            faithfulness = self.generation_evaluator.faithfulness_evaluator.evaluate_faithfulness(
                context, generated_answer
            )
            relevance = self.generation_evaluator.relevance_evaluator.evaluate_relevance(
                query, generated_answer  
            )
            
            # 4. ç«¯åˆ°ç«¯è¯„ä¼°
            if ground_truth_answer:
                accuracy = self.answer_evaluator.evaluate_accuracy(
                    query, generated_answer, ground_truth_answer
                )
            
            # è®°å½•è¯¦ç»†ç»“æœ
            results['detailed_results'].append({
                'query': query,
                'retrieval_recall': retrieval_recall if expected_docs else None,
                'faithfulness': faithfulness,
                'relevance': relevance,
                'accuracy': accuracy if ground_truth_answer else None,
                'generated_answer': generated_answer
            })
        
        # è®¡ç®—æ±‡æ€»æŒ‡æ ‡
        results['retrieval_metrics'] = self._summarize_retrieval_metrics(results['detailed_results'])
        results['generation_metrics'] = self._summarize_generation_metrics(results['detailed_results'])
        results['end_to_end_metrics'] = self._summarize_e2e_metrics(results['detailed_results'])
        
        return results
    
    def _summarize_retrieval_metrics(self, detailed_results):
        recalls = [r['retrieval_recall'] for r in detailed_results if r['retrieval_recall'] is not None]
        return {'avg_recall': np.mean(recalls)} if recalls else {}
    
    def _summarize_generation_metrics(self, detailed_results):
        faithfulness_scores = [r['faithfulness'] for r in detailed_results]
        relevance_scores = [r['relevance'] for r in detailed_results]
        return {
            'avg_faithfulness': np.mean(faithfulness_scores),
            'avg_relevance': np.mean(relevance_scores)
        }
    
    def _summarize_e2e_metrics(self, detailed_results):
        accuracy_scores = [r['accuracy'] for r in detailed_results if r['accuracy'] is not None]
        return {'avg_accuracy': np.mean(accuracy_scores)} if accuracy_scores else {}

# ä½¿ç”¨ç¤ºä¾‹
config = {'ground_truth_path': 'test_data.json'}
evaluator = ComprehensiveRAGEvaluator(config)

test_queries = [
    {
        'query': 'ä»€ä¹ˆæ˜¯RAGæŠ€æœ¯ï¼Ÿ',
        'relevant_docs': ['doc1', 'doc3'],
        'ground_truth': 'RAGæ˜¯æ£€ç´¢å¢å¼ºç”ŸæˆæŠ€æœ¯...'
    }
    # æ›´å¤šæµ‹è¯•æŸ¥è¯¢...
]

evaluation_results = evaluator.full_evaluation(my_rag_system, test_queries)

print("å®Œæ•´è¯„ä¼°ç»“æœ:")
print(f"æ£€ç´¢æŒ‡æ ‡: {evaluation_results['retrieval_metrics']}")
print(f"ç”ŸæˆæŒ‡æ ‡: {evaluation_results['generation_metrics']}")
print(f"ç«¯åˆ°ç«¯æŒ‡æ ‡: {evaluation_results['end_to_end_metrics']}")
```

---

## âš ï¸ è¯„ä¼°æŒ‘æˆ˜ä¸æœªæ¥å±•æœ›

### å½“å‰æ ¸å¿ƒæŒ‘æˆ˜

å°½ç®¡RAGè¯„ä¼°æŠ€æœ¯å·²ä»"ä¸»è§‚åˆ¤æ–­"å‘å±•åˆ°"æŒ‡æ ‡é©±åŠ¨"ï¼Œä½†åœ¨å®é™…åº”ç”¨ä¸­ä»é¢ä¸´ä¸‰å¤§æ ¹æœ¬æ€§æŒ‘æˆ˜ï¼š

#### 1. ä¸»è§‚æ€§ä¸å¯æ‰©å±•æ€§çš„å›ºæœ‰çŸ›ç›¾

| æ–¹æ¡ˆ | ç‰¹ç‚¹ | å±€é™ |
|------|------|------|
| **äººç±»æ ‡æ³¨é»„é‡‘æ ‡å‡†** | é«˜å®¢è§‚æ€§ | å¯æ‰©å±•æ€§æå·®ï¼ˆ1ä¸‡æ¡éœ€æ•°åäººÂ·å¤©ï¼‰ï¼Œæ— æ³•é€‚åº”çŸ¥è¯†åº“åŠ¨æ€æ›´æ–° |
| **LLM-as-a-Judge** | é«˜å¯æ‰©å±•æ€§ | å­˜åœ¨ä¸»è§‚åå·®ï¼Œè¾“å‡ºå—æç¤ºè¯ã€æ¸©åº¦å‚æ•°å½±å“ï¼ˆç›¸åŒè¾“å…¥å¾—åˆ†æ³¢åŠ¨å¯è¾¾Â±0.15ï¼‰ |

#### 2. åŠ¨æ€çŸ¥è¯†ä¸é™æ€åŸºå‡†çš„è„±èŠ‚

å½“å‰ä¸»æµè¯„ä¼°åŸºå‡†å‡ä¸ºé™æ€æ•°æ®ï¼Œæ— æ³•æ¨¡æ‹ŸçœŸå®åœºæ™¯ä¸­çš„"çŸ¥è¯†å˜åŒ–"ï¼š

- **æ— æ³•è¯„ä¼°çŸ¥è¯†æ›´æ–°èƒ½åŠ›**ï¼šé™æ€åŸºå‡†æ— æ³•æ£€æµ‹"çŸ¥è¯†åº“æ–°å¢æ–‡æ¡£åæœªèƒ½æ£€ç´¢åˆ°æœ€æ–°ä¿¡æ¯"çš„å¤±æ•ˆ
- **æ— æ³•è¯„ä¼°å†²çªçŸ¥è¯†å¤„ç†èƒ½åŠ›**ï¼šå½“æ–°æ—§çŸ¥è¯†å†²çªæ—¶ï¼ŒRAGç³»ç»Ÿæ˜¯å¦èƒ½ä¼˜å…ˆé€‰æ‹©æ–°ä¿¡æ¯ï¼Ÿ
- **åŸºå‡†è€åŒ–é€Ÿåº¦å¿«**ï¼šæ—¶æ•ˆæ€§å¼ºçš„é¢†åŸŸï¼ˆé‡‘èã€ç§‘æŠ€ï¼‰ï¼Œé™æ€åŸºå‡†æœ‰æ•ˆæœŸé€šå¸¸ä»…1-3ä¸ªæœˆ

#### 3. ä¼¦ç†é£é™©è¯„ä¼°çš„ç¼ºå¤±

å½“å‰æ¡†æ¶ä¸»è¦å…³æ³¨"äº‹å®å‡†ç¡®æ€§"ï¼Œä½†å¿½è§†äº†å¯èƒ½å¯¼è‡´ä¸¥é‡é—®é¢˜çš„ä¼¦ç†é£é™©ï¼š

| é£é™©ç±»å‹ | æè¿° | å½“å‰è¯„ä¼°ç°çŠ¶ |
|----------|------|--------------|
| **åè§ä¸æ­§è§†** | çŸ¥è¯†åº“å­˜åœ¨æ€§åˆ«ã€ç§æ—åè§ï¼ŒRAGå¯èƒ½ç”Ÿæˆå¸¦åè§ç­”æ¡ˆ | æ— é‡åŒ–æŒ‡æ ‡ |
| **æ¯’æ€§ä¸æœ‰å®³å†…å®¹** | çŸ¥è¯†åº“å«æç«¯è§‚ç‚¹ï¼ŒRAGå¯èƒ½"å¤è¿°"æœ‰å®³å†…å®¹ | å½“å‰æŒ‡æ ‡åè€Œå¯èƒ½åˆ¤å®šä¸º"é«˜å¿ å®åº¦" |
| **éšç§æ³„éœ²** | RAGæ£€ç´¢åˆ°åŒ…å«ç”¨æˆ·éšç§çš„ä¸Šä¸‹æ–‡å¹¶åœ¨ç­”æ¡ˆä¸­æ³„éœ² | æ— ç›‘æ§æŒ‡æ ‡ |

### æœªæ¥è¶‹åŠ¿ä¸ç ”ç©¶æ–¹å‘

#### 1. ä»"é€šç”¨æŒ‡æ ‡"åˆ°"ä»»åŠ¡ç‰¹å®šæŒ‡æ ‡"

| åº”ç”¨åœºæ™¯ | ä¸“å±æŒ‡æ ‡ç¤ºä¾‹ |
|----------|--------------|
| **åŒ»ç–—RAG** | é£é™©æç¤ºå®Œæ•´æ€§ã€ç¦å¿Œç—‡è¦†ç›–ç‡ |
| **æ³•å¾‹RAG** | æ³•æ¡å¼•ç”¨å‡†ç¡®æ€§ã€æ—¶æ•ˆæ€§æ£€æŸ¥ |
| **é‡‘èRAG** | æ•°æ®æ—¶æ•ˆæ€§ã€åˆè§„å£°æ˜å®Œæ•´æ€§ |

#### 2. åŠ¨æ€è¯„ä¼°æ¡†æ¶çš„æ„å»º

- **åŠ¨æ€åŸºå‡†ç”ŸæˆæŠ€æœ¯**ï¼šLLMè‡ªåŠ¨ç”Ÿæˆæ—¶æ•ˆæ€§æŸ¥è¯¢ã€åŠ¨æ€ä¸Šä¸‹æ–‡ã€æ–°æ—§çŸ¥è¯†å†²çªåœºæ™¯
- **çŸ¥è¯†æ›´æ–°æ€§èƒ½æŒ‡æ ‡**ï¼š
  - çŸ¥è¯†æ›´æ–°å»¶è¿Ÿï¼šä»"æ–°å¢æ–‡æ¡£"åˆ°"å¯æ£€ç´¢åˆ°è¯¥æ–‡æ¡£"çš„æ—¶é—´å·®
  - æ–°ä¿¡æ¯ä¼˜å…ˆç‡ï¼šæ–°æ—§çŸ¥è¯†å†²çªæ—¶é€‰æ‹©æ–°ä¿¡æ¯çš„æ¯”ä¾‹
  - æ—§ä¿¡æ¯è¿‡æ»¤ç‡ï¼šèƒ½å¦è¯†åˆ«å¹¶è¿‡æ»¤"å·²è¿‡æ—¶çš„æ—§ä¿¡æ¯"
- **åœ¨çº¿è¯„ä¼°ä¸åé¦ˆé—­ç¯**ï¼šé›†æˆåˆ°ç”Ÿäº§ç¯å¢ƒï¼Œè‡ªåŠ¨è§¦å‘åŠ¨æ€åœºæ™¯æµ‹è¯•

#### 3. ä¼¦ç†é£é™©è¯„ä¼°ä½“ç³»çš„å®Œå–„

- åè§æ£€æµ‹æŒ‡æ ‡
- æ¯’æ€§å†…å®¹è¿‡æ»¤è¯„ä¼°
- éšç§æ³„éœ²é£é™©ç›‘æ§

---

## ğŸ“ˆ æŒç»­è¯„ä¼°ä¸ç›‘æ§

### 1. åœ¨çº¿è¯„ä¼°ç³»ç»Ÿ

```python
class OnlineRAGMonitor:
    def __init__(self, rag_system):
        self.rag_system = rag_system
        self.metrics_buffer = []
        
    def log_interaction(self, query: str, answer: str, user_feedback: dict):
        """è®°å½•ç”¨æˆ·äº¤äº’"""
        interaction = {
            'timestamp': datetime.now(),
            'query': query,
            'answer': answer,
            'feedback': user_feedback,
            'response_time': user_feedback.get('response_time', 0)
        }
        self.metrics_buffer.append(interaction)
        
        # å®šæœŸåˆ†æ
        if len(self.metrics_buffer) >= 100:
            self._analyze_recent_performance()
    
    def _analyze_recent_performance(self):
        """åˆ†ææœ€è¿‘æ€§èƒ½"""
        recent_interactions = self.metrics_buffer[-100:]
        
        # è®¡ç®—å…³é”®æŒ‡æ ‡
        avg_rating = np.mean([i['feedback'].get('rating', 0) for i in recent_interactions])
        avg_response_time = np.mean([i['response_time'] for i in recent_interactions])
        
        # æ£€æµ‹å¼‚å¸¸
        if avg_rating < 3.5:
            self._alert_low_satisfaction()
        if avg_response_time > 5.0:
            self._alert_slow_response()
    
    def _alert_low_satisfaction(self):
        """ä½æ»¡æ„åº¦å‘Šè­¦"""
        print("âš ï¸ è­¦å‘Šï¼šç”¨æˆ·æ»¡æ„åº¦ä¸‹é™")
    
    def _alert_slow_response(self):
        """å“åº”æ…¢å‘Šè­¦"""
        print("âš ï¸ è­¦å‘Šï¼šç³»ç»Ÿå“åº”æ—¶é—´è¿‡é•¿")

# 2. A/Bæµ‹è¯•æ¡†æ¶
class RAGABTester:
    def __init__(self, system_a, system_b):
        self.system_a = system_a
        self.system_b = system_b
        self.results = {'A': [], 'B': []}
    
    def run_test(self, queries, traffic_split=0.5):
        """è¿è¡ŒA/Bæµ‹è¯•"""
        for query in queries:
            # éšæœºåˆ†é…æµé‡
            if random.random() < traffic_split:
                result = self._test_system('A', self.system_a, query)
                self.results['A'].append(result)
            else:
                result = self._test_system('B', self.system_b, query)
                self.results['B'].append(result)
    
    def _test_system(self, version, system, query):
        start_time = time.time()
        answer = system.generate_answer(query)
        response_time = time.time() - start_time
        
        return {
            'query': query,
            'answer': answer,
            'response_time': response_time,
            'version': version
        }
    
    def analyze_results(self):
        """åˆ†æA/Bæµ‹è¯•ç»“æœ"""
        metrics_a = self._calculate_metrics(self.results['A'])
        metrics_b = self._calculate_metrics(self.results['B'])
        
        # ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ
        from scipy.stats import ttest_ind
        
        times_a = [r['response_time'] for r in self.results['A']]
        times_b = [r['response_time'] for r in self.results['B']]
        
        t_stat, p_value = ttest_ind(times_a, times_b)
        
        return {
            'system_a_metrics': metrics_a,
            'system_b_metrics': metrics_b,
            'significance_test': {
                't_statistic': t_stat,
                'p_value': p_value,
                'significant': p_value < 0.05
            }
        }
```

---

## ğŸ”— ç›¸å…³é˜…è¯»

- [RAGèŒƒå¼æ¼”è¿›](/llms/rag/paradigms) - äº†è§£RAGæŠ€æœ¯å‘å±•è„‰ç»œ
- [æ£€ç´¢ç­–ç•¥ä¼˜åŒ–](/llms/rag/retrieval) - æ£€ç´¢ç»„ä»¶çš„ä¼˜åŒ–æ–¹æ³•
- [é‡æ’åºæŠ€æœ¯](/llms/rag/rerank) - æå‡æ£€ç´¢ç²¾åº¦çš„æŠ€æœ¯
- [ç”Ÿäº§å®è·µæŒ‡å—](/llms/rag/production) - è¯„ä¼°åœ¨ç”Ÿäº§ç¯å¢ƒä¸­çš„åº”ç”¨

> **ç›¸å…³æ–‡ç« **ï¼š
> - [æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿç»¼åˆè¯„ä¼°ï¼šä»æ ¸å¿ƒæŒ‡æ ‡åˆ°å‰æ²¿æ¡†æ¶](https://dd-ff.blog.csdn.net/article/details/152823514)
> - [åˆ«å†å·äº†ï¼ä½ å¼•ä»¥ä¸ºå‚²çš„ RAGï¼Œæ­£åœ¨æ€æ­»ä½ çš„ AI åˆ›ä¸šå…¬å¸](https://dd-ff.blog.csdn.net/article/details/150944979)
> - [LLM ä¸Šä¸‹æ–‡é€€åŒ–ï¼šå½“è¶Šé•¿çš„è¾“å…¥è®©AIå˜å¾—è¶Š"ç¬¨"](https://dd-ff.blog.csdn.net/article/details/149531324)

> **å¤–éƒ¨èµ„æº**ï¼š
> - [RAGAså®˜æ–¹æ–‡æ¡£](https://docs.ragas.io/) - RAGè¯„ä¼°æ¡†æ¶
> - [TruLensæ–‡æ¡£](https://www.trulens.org/trulens_eval/getting_started/) - LLMåº”ç”¨è¯„ä¼°å·¥å…·
> - [ARES GitHub](https://github.com/stanford-futuredata/ARES) - æ–¯å¦ç¦RAGè¯„ä¼°æ¡†æ¶
